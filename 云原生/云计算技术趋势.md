## **（1）云计算/云原生（cloud native）：云时代的软件架构**

云原生可以简单理解为**面向云环境的软件架构**，即应用原生（与生俱来）为云设计，应用原生被设计为在云上以最佳方式运行，充分发挥云的优势

### **云原生之前**

- **应用职责**：业务代码，开发框架&类库使用、服务能力调用，既关注业务需求，也关注一些非业务需求，比如RPC的网络通信、限流降级容错等
- **底层基础设施职责（硬件、虚拟机/容器、操作系统等）**：向上提供基本运行资源，cpu&内存&网络&磁盘存储

### **云原生**

- **应用层职责**：更多关注业务逻辑，非业务逻辑被解耦并下沉到基础设施层
- **底层基础设施职责**：向上提供更多资源和各种能力（比如调度）

即：云的出现，可以在提供各种资源之外，还提供各种能力，从而帮助应用开发者，使得应用开发可以专注于业务需求的实现

更详细说，应用开发者不用关注用如何部署应用和更新应用版本，应部署到哪些资源上，当出现资源故障的时候如何容错，当出现流量变化的时候如何伸缩，不用关心服务之间进行网络通讯的任何细节（如rpc的序列化&传输协议，服务发现、负载均衡、出现网络错误时候的限流、熔断、降级等）

### **以服务间网络通信为例**

- **最早**：服务在实现业务需求的同时，为了实现服务间 RPC通信，还得实现一堆诸如序列化/反序列化、路由、负载均衡、流量控制（熔断、限流）、容错（超时、重试、降级）等；
- **微服务sdk模式**：在应用层添加一个胖客户端，在这个客户端中实现上述各种服务通信相关功能。问题：语言绑定、学习和维护时间成本（开发时不透明）、应用侵入性（同进程）；
- **微服务service mesh模式**：将SDK客户端的功能剥离出来，放到Sidecar中，Sidecar是独立进程，在Sidecar中实现sdk的各种功能。其实就是把更多业务无关的事情下沉到基础设施中，业务开发者更多关注业务逻辑开发；

---

### **云计算**

![iaas-paas-saas]()

- 云：统一接口 抽象概念 租户自助
- openstack:是当今最具影响力的云计算管理工具——通过命令或者基于 Web 的可视化控制面板来管理 IaaS 云端的资源池（服务器、存储和网络）
- flavor：抽象资源配比，在Openstack中，虚机硬件模板被称为类型模板(flavor)，包括RAM和硬盘大小，CPU核数等
- 虚拟私有云（Virtual Private Cloud，以下简称VPC），是存在于共享或公用云中的私有云（Private Cloud），为弹性云服务器构建隔离的、用户自主配置和管理的虚拟网络环境，提升用户云中资源的安全性，简化用户的网络部署

### **区域与可用区**

1. 区域对应的是云计算厂商在某个地理位置提供的所有云服务的组合
    - 开服：是厂商对外提供云服务的基本单位，开设一个新的区域，包括了云计算服务商在某个地区建立数据中心，安置大量的计算、存储和网络等硬件资源，以及部署虚拟化、服务组件、资源调度等各种复杂软件，最后与外界互联网相连，获得批准对外提供云服务的全过程；
    - 不同区域的间隔，一般在数百公里及以上，这也对应了单个区域能够辐射和服务的范围；
    - 同一个云在不同的区域，能够提供的服务和规模可能是不同的;
    - 区域的流量费用：如果把区域作为一个有界的实体圈起来，流量可以分为入站流量、出站流量和内部流量，一般计费框架下，入站和内部流量是免费或接近免费的，出站流量则单独收费；
    - 多区域架构：**各区域之间建设有互联网专线**，一般称为骨干网，骨干网使得不同区域间的通信，能够有较高的带宽和较低的时延。多区域的私有内网（vpc）能够借助骨干网高速打通。**DNS能够提供就近解析和智能路由，将c端流量路由到最近的数据中心**；

2. 可用区是区域的下级概念，是指一个具备完整而独立的电力供应、冷却系统、网络设施的数据中心单元，**一个区域通常由多个可用区高速互联组成**，区域内的可用区一般位于同一个城市，之间距离往往在一百公里以内
    - 物理上的数据中心或机房的概念，对应到云端，是在可用区这个层面；
    - 多个可用区是为了解决区域高可用的问题，对数据中心级故障进行容灾；
    - 可用区内-》可用区之间-》区域之间，可用性和容灾级别逐渐提升，但网络访问性能逐渐降低；
    - 区域可以通过新建可用区，来扩展自身的容量，所以一个区域里可用区的数量，是衡量一个区域规模的重要指标；

### **云虚拟机**

云端虚拟出来的服务器，这个服务器你可以完全控制它，从底层操作系统到安装上层应用。云虚拟机的体系结构，用一句话来概括，就是全面解耦的**计算存储分离**的设计思想

### **计算存储分离**

传统的虚拟机，往往是对单一物理机器资源的纵向分割，计算、存储、网络等各方面的能力是一台物理宿主机的子集，因此在可伸缩性方面存在较大的局限性。

云虚拟机的组成则有所不同，除了核心的cpu与内存仍属于一台宿主机外（当然也可以选择使用本地磁盘），它的网络、硬盘等其他部分，则可以脱离于宿主机之外，享受云端其他基础设施的能力，比如网络基于云网卡、存储基于云存储，因此一台云虚拟机，**可以同时挂载很多硬盘，还能够插上很多网卡拥有多个不同外部IP**，是由数据中心中的不同部分一起协作，“拼凑”而成的一台机器

## **云IO（云硬盘）**

又叫云盘或者云磁盘，就是云虚拟机上可以挂载和使用的硬盘，既包含了用于承载操作系统的系统盘，也包括了承载数据的数据盘

1. 持久化存储：即写入的数据不会丢失，云厂商对于云盘，不仅仅会保证数据顺利写入，一般还会在存储端同步和保留至少三份数据副本，所以云硬盘的冗余度和可用性非常之高。但是一定不能完全仅依靠它的可靠性，使用方还是得定期为云磁盘创建快照、异地数据备份数据文件等方式，来保护关键数据；
2. 与传统磁盘的差异：传统计算机体系结构中，硬盘是通过本地机器内部的主板的高速总线，与cpu、内存等部件相连，而在云端，**云硬盘并不在宿主机上，而是在专用的磁盘服务阵列中**，两者是通过数据中心内部的特有IO线路进行连接，这也是计算存储分离的一种体现；
3. IOPS来衡量云硬盘的性能：HDD硬盘的IOPS大概在数百左右，成本低是优势。SDD和HDD的混合硬盘的IOPS大概在数千左右，可以同时发挥SDD的性能优势和HDD的容量优势。纯SDD硬盘，IOPS能够上万，可以用来承载IO密集型应用；
4. 云盘可以热挂载，大小灵活调度；

## **网络**

1. **虚拟私有网络（Vitual Private Cloud，简称VPC）**:是云计算网络端最重要的概念之一，指构建在云上的，相互隔离的，用户可以自主控制的私有网络环境，通俗来讲就是属于你自己的内网，内网之间的服务器和设备，可以自由互相通信，与外界默认是隔离的，如果外部互联网或者其他虚拟网络需要连接，则需要额外配置；

2. vpc可以跨可用区，也即可以建设跨同区域不同数据中心的私有网络；

3. 一个vpc包括：
    - 3.1 网段：私有网络内部的IP区段，通常用CIDR形式来表示，如192.168.0.0/24，子网掩码是11111111.11111111.11111111.0，即255.255.255.0,则有256个地址。192.168.0.0/16，子网掩码是11111111.11111111.00000000.00000000，即255.255.0.0,则有256*256=65536个地址。创建vpc的时候需要选择对应的网段；
    - 3.2 子网：私有网络的下级网络结构，一个私有网络可以划分为多个子网，阿里云中把子网称为“交换机”，创建子网的时候需要选择所属vpc网段里的子网段；
    - 3.3 路由表：定义私有网络内流量的路由规则，决定着数据包的“下一跳”去向何处，每个子网都必须有一张关联的路由表；
    - 3.4 网关：是对进出私有网络的流量进行把守和分发的重要节点；
    - 3.5 安全组：私有网络里虚拟机进出流量的通行或拦截规则，可以起到虚拟机网络防火墙的作用；

4. 如果在没有创建vpc的情况下直接创建虚拟机，公有云一般会自动生成vpc，但墙裂建议不要让系统自动建立vpc，而是自行建立vpc，配置好子网和网段等重要参数，然后再创建云虚拟机；

5. 虚拟机如何和vpc进行连接：通过虚拟机的网卡，又称弹性网卡，虚拟机的网卡一方面和虚拟机绑定，一方面嵌入某个vpc的子网，拥有至少一个私网IP。所以，当创建虚拟机的时候选择属于哪个vpc以及哪个子网的时候，实质结果就是新虚拟机自动生成的主网卡，接入了所选vpc的所选子网；一个虚拟机可以绑定多块网卡，有主网卡和辅助网卡之分；一块网卡隶属于一个子网，可以配置同一个子网内的多个私有IP；辅助网卡可以动态解绑，还能够绑定到另一台虚拟机上；

## **云端架构**

1. AWS著名架构原则：Desigh for failure

2. 云上的故障级别：
    - 2.1 **宿主机级别**：将影响位于同一宿主机上的多个虚拟机，为避免这样的影响，就需要创建多台虚拟机组成的集群，这需要我们保证多个虚拟机不在同一台宿主机上，甚至不处于同一个机架上，以免这些虚拟机一起受到宿主机故障影响；
    - 2.2 **数据中心级别**：也即可用区层面，比如火灾、雷击、断电、冷却系统故障等，面对这种故障，就需要多可用区的实例部署，即你的实例需要分散在多个可用区中，这样，可用区之间可以互为主备，也可以同时对外服务分担压力，vpc可以跨可用区，会大大方便我们多可用区架构的搭建；
    - 2.3 **区域级别故障**：比如地震的不可抗力，这个时候就需要靠多区域架构了，典型做法是在DNS层面进行导流，把域名解析到另外一个区域的备用服务上，底层数据则需要日常进行跨区域的实时同步。再进一步的策略，就是考虑多云，即同时选用多家云厂商的公有云，一起来服务业务，但集成多个异构的云会带来额外的成本，但好处是能够最大限度降低服务风险
    综上，无论哪种级别故障，核心思想就是化单点为多点，形成不同层面，不同粒度的冗余，当故障发生的时候，能够迅速发现和切换；

3. 弹性伸缩：配合负载均衡器，将流量按照一定权重或规则，分发到多台虚拟机，当负载增大，虚拟机增多的时候，负载均衡能够自动动态识别，将流量分发到新创建的虚拟机上；

## **PaaS**

PaaS （Platform-as-a-Service），则是指云计算提供的平台类服务，在这些平台的基础上，用户可以直接开发、运行、管理应用程序，而无需构建和维护底层的基础设施，PaaS 在 IaaS 的基础上又做了许多工作，构建了很多关键抽象和可复用的单元，让我们用户能够在更上层进行应用的构建，把更多精力放在业务逻辑上

## **容器服务（Docker 和 k8s）**

Docker 和 Kubernetes 分别是 容器技术 和 容器编排 的 事实标准

### **容器和云的关系**

轻量的容器和**富有弹性**的云计算，互相之间其实是非常契合的。容器对于运行环境的极强适应性和快速启动的能力，配合云上动态扩展的庞大资源规模，让云端的容器应用可以在短时间内拓展到成千上万个实例。所以，云可以说是容器应用的最佳载体，容器应用也非常适合在云上运行和扩展。同时因为云本身是**多租户**的，需要运行环境的隔离性，这个也非常适合Docker。

Kubernetes将一切皆视为资源，不同资源之间依靠层级关系相互组合、协作的这个思想是贯穿Kubernetes整个系统的两大核心设计理念之一，不仅在容器、Pod、主机、集群等计算资源上是这样，如下图所示，在工作负载、持久存储、网络策略、身份权限等其他领域中也都有一致的体现

![k8s-resources]()

* 容器（Container）：延续了自Docker以来一个容器封装一个应用进程的理念，是镜像管理的最小单位；

* 生产任务（Pod）：补充了容器化后缺失的与进程组对应的“容器组”的概念，Pod中的容器共享UTS、IPC、网络等名称空间，是资源调度的最小单位；

* 节点（Node）：对应于集群中的单台机器，这里的机器既可以是生产环境中的物理机，也可以是云计算环境中的虚拟节点，节点是处理器和内存等资源的资源池，是硬件单元的最小单位；

* 集群（Cluster）：对应于整个集群，Kubernetes提倡面向集群来管理应用。当你要部署应用的时候，只需要通过声明式API将你的意图写成一份元数据（Manifest，Yaml），将它提交给集群即可，而无须关心它具体分配到哪个节点（尽管通过标签选择器完全可以控制它分配到哪个节点，但一般不需要这样做）、如何实现Pod间通信、如何保证韧性与弹性，等等，所以集群是处理元数据的最小单位；

* 集群联邦（Federation）：对应于多个集群，通过集群联邦可以统一管理多个Kubernetes集群，它的一种常见应用是能满足跨可用区域多活、跨地域容灾的需求；

Pod是隔离与调度的基本单位，也是我们接触的第一种Kubernetes资源。Pod的两大基本职责：

1. 扮演容器组的角色，满足容器共享名称空间的需求，多个容器位于同一个Pod的特殊关系，它们将默认共享如下内容:
    * UTS名称空间：所有容器都有相同的主机名和域名;
    * 网络名称空间：所有容器都共享一样的网卡、网络栈、IP地址等。因此，同一个Pod中不同容器占用的端口不能冲突;
    * IPC名称空间：所有容器都可以通过信号量或者POSIX共享内存等方式通信;
    * 时间名称空间：所有容器都共享相同的系统时间;

    同一个Pod的容器，只有PID名称空间和文件名称空间默认是隔离的。

    PID的隔离令每个容器都有独立的进程ID编号，它们封装的应用进程就是PID为1的进程，可以通过Pod元数据定义中spec.shareProcessNamespace来改变这点。一旦要求共享PID名称空间，容器封装的应用进程就不再具有PID为1的特征了，这有可能导致部分依赖该特征的应用出现异常。

    在文件名称空间方面，容器要求文件名称空间的隔离是很理所当然的需求，因为容器需要相互独立的文件系统以避免冲突，但容器间可以共享存储卷，这是通过Kubernetes的Volume来实现的。

2. 实现原子性调度：如果容器编排不跨越集群节点，是否具有原子性都无关紧要。但是在集群环境中，在容器可能跨机器调度时，这个特性就变得非常重要。如果以容器为单位来调度，不同的容器就有可能被分配到不同的机器上。两台机器之间本来就是物理隔离，依靠网络连接的，这时候谈什么名称空间共享、cgroups配额共享都将毫无意义；如果我们在容器编排中仍然坚持将容器视为调度的最小粒度，那对容器运行所需资源的需求声明就只能设定在容器上，这样集群每个节点剩余资源越紧张，单个节点无法容纳全部协同容器的概率就越大，协同的容器被分配到不同节点的可能性就越高。但是如果将运行资源的需求声明定义在Pod上，直接以Pod为最小的原子单位来实现调度，由于多个Pod之间必定不存在超亲密的协同关系，只会通过网络非亲密地协作，就没有协同的说法，自然也不需要考虑复杂的调度了；

### **控制器模式与声明式API**

Kubernetes作为云原生时代的基础设施，会尽力帮助程序员以最小的代价来实现容错，为系统健壮运行提供底层支持。**控制器模式**是继资源模型之后另一个Kubernetes核心设计理念，而如何实现具有韧性与弹性的系统是展示Kubernetes控制器设计模式的最好示例。

假设有一个由数十个Node、数百个Pod、近千个Container所组成的分布式系统，要避免系统因为外部流量压力、代码缺陷、软件更新、硬件升级、资源分配等原因而出现中断，作为管理员，你希望编排系统为你提供哪种支持？

作为用户，当然最希望容器编排系统能自动把所有意外因素都消灭掉，让任何一个服务都永远健康，永不出错。但永不出错的服务是不切实际的，所以只能退而求其次，让编排系统在这些服务出现问题或者运行状态不正确的时候，能自动调整成正确的状态。这种需求听起来也是贪心的，却已经具备足够的可行性，相应的解决办法在工业控制系统里已经有非常成熟的应用，叫作控制回路（Control Loop）。Kubernetes官方文档是以房间中空调自动调节温度为例介绍了控制回路的一般工作过程：当你设置好了温度，就是告诉空调你对温度的“期望状态”（Desired State），而传感器测量出的房间的实际温度是“当前状态”（Current State）。根据当前状态与期望状态的差距，由控制器通过控制空调的制冷开关来调节温度，使当前状态逐渐接近期望状态，

将这种控制回路的思想应用到容器编排上，自然会为Kubernetes中的资源附加上期望状态与实际状态两项属性。因此，如果要想使用这些资源来实现某种需求，就不提倡像平常编程那样去调用某个或某一组方法来达成目的，而是要**通过描述清楚这些资源的期望状态，由Kubernetes中对应监视这些资源的控制器来驱动资源的实际状态逐渐向期望状态靠拢**。这种交互风格被称为Kubernetes的**声明式API**。我们日常在元数据文件中定义的spec字段所描述的便是资源的期望状态。

与资源相对应，只要是实际状态有可能发生变化的资源对象，通常都会由对应的控制器进行追踪，每个控制器至少会追踪一种类型的资源对象。为了管理众多资源控制器，Kubernetes设计了统一的控制器管理框架（kubecontrollermanager）来维护这些控制器的正常运作，以及统一的指标监视器（kubeapiserver）来为控制器提供其工作时追踪资源的度量数据。

我们以部署控制器（Deployment Controller）、副本集控制器（ReplicaSet Controller）和自动扩缩控制器（HPA Controller）为例来介绍Kubernetes控制器模式的工作原理。

问题场景：通过服务编排，对任何分布式系统自动实现以下三种通用的能力

1. Pod出现故障时，能够自动恢复，不中断服务
2. Pod更新程序时，能够滚动更新，不中断服务
3. Pod遇到压力时，能够水平扩展，不中断服务

针对1：做法是通过副本集（ReplicaSet）来创建Pod。ReplicaSet也是一种资源，属于工作负荷类，代表一个或多个Pod副本的集合。你可以在ReplicaSet资源的元数据中描述你期望的Pod副本的数量（即spec.replicas的值）。当ReplicaSet成功创建之后，副本集控制器就会持续跟踪该资源，如果一旦有Pod发生崩溃退出，或者状态异常（默认是靠进程返回值，你还可以在Pod中设置探针，以自定义的方式告诉Kubernetes出现何种情况时Pod才算状态异常），ReplicaSet都会自动创建新的Pod来替代异常的Pod；如果异常出现了额外数量的Pod，也会被ReplicaSet自动回收，总之就是确保在任何时候集群中的这个Pod副本的数量都向期望状态靠拢

针对2：做法是由Deployment来创建ReplicaSet，再由ReplicaSet来创建Pod，当你更新Deployment中的信息（譬如更新了镜像的版本）后，部署控制器就会跟踪到新的期望状态，自动创建新的ReplicaSet，并逐渐缩减旧的ReplicaSet的数量，直至升级完成后彻底删除掉旧的ReplicaSet，如下图所示

![k8s-rolling-update]()

如上也是所谓的滚动更新，滚动更新（Rolling Update）是指先停止少量旧副本，维持大量旧副本继续提供服务，当停止的旧副本更新成功，新副本可以提供服务以后，再重复以上操作，直至所有的副本都更新成功

针对3：做法是由autoscaling资源和自动扩缩控制器来实现，从而自动根据度量指标，如处理器、内存占用率、用户自定义的度量值等，来设置Deployment（或者ReplicaSet）的期望状态，实现当度量指标出现变化时，系统自动按照Autoscaling→Deployment→ReplicaSet→Pod”这样的顺序层层变更，最终实现根据度量指标自动扩容/缩容

### **资源与调度**

从编排系统的角度来看，Node是资源的提供者，Pod是资源的使用者，调度是对两者进行恰当的撮合。

Node通常能够提供三方面资源：

* 计算资源（如处理器、图形处理器、内存）
* 存储资源（如磁盘容量、不同类型的介质）
* 网络资源（如带宽、网络地址）

其中与调度关系最密切的是处理器和内存，虽然它们同属于计算资源，但两者在调度时又有一些微妙的差别。

处理器这样的资源被称作可压缩资源（Compressible Resource），特点是当可压缩资源不足时，Pod只会处于“饥饿状态”，运行变慢，但不会被系统杀死，即容器不会被直接终止，或被要求限时退出。

而像内存这样的资源，则被称作不可压缩资源（Incompressible Resource），特点是当不可压缩资源不足，或者超过了容器自己声明的最大限度时，Pod就会因为内存溢出（OutOfMemory，OOM）而被系统直接杀掉

Kubernetes是如何撮合Pod与Node的，这其实也是最难的一个问题。调度是为新创建出来的Pod寻找一个最恰当的宿主机节点去运行它，这句话里就包含“运行”和“恰当”两个调度中关键过程，具体分析如下。

* 运行：从集群所有节点中找出一批剩余资源可以满足该Pod运行的节点。为此，Kubernetes调度器设计了一组名为Predicate的筛选算法；

* 恰当：从符合运行要求的节点中找出一个最适合的节点完成调度。为此，Kubernetes调度器设计了一组名为Priority的评价算法。

想象一下，有一个由数千节点组成的集群，每次创建Pod都必须依据各节点的实时资源状态来确定调度的目标节点，然而各节点的资源是随着程序运行无时无刻不在变动的，资源状况只有节点本身最清楚，如果每次调度都要发生数千次的远程访问来获取这些信息的话，那压力与耗时都很难降下来，结果不仅会令调度器成为集群管理的性能瓶颈，还会出现因耗时过长，某些节点上资源状况已发生变化，调度器的资源信息过时而导致调度结果不准确等问题。由于调度器的工作负载与集群规模大致成正比，随着集群和它们的工作负载不断增长，调度器很有可能成为扩展性瓶颈所在

### **不可变基础设施**

1. **传统的可变服务器基础设施**：服务器会不断更新和修改。使用这类基础设施的工程师和管理员可以SSH到他们的服务器，手动升级或降级软件包版本，逐个服务器调整配置文件，并直接将新代码部署到现有服务器上。换句话说，这些服务器是可变的；它们可以在创建后进行更改。由可变服务器组成的基础设施本身可以称为可变的、传统的或手工的；

2. **不可变基础设施**：这是另一种基础设施模式，其中服务器在部署后永远不会被修改。如果需要以任何方式更新、修复或修改某些内容，就先对公共镜像进行修改，然后用镜像构建新服务器来替换旧服务器。经过验证后，新服务器投入使用，旧的服务器就会下掉。

不可变基础设施的好处是在基础设施中有更多的一致性和可靠性，以及更简单、更可预测的部署过程。它可以缓解或完全防止可变基础设施中常见的问题，如 配置漂移（configuration drift）和 雪花服务器（snowflake servers）。然而，想要高效地使用不可变基础设施，通常需要包括全面的自动化部署、云计算环境中的快速服务器配置，以及处理有状态数据或临时数据(如日志)的解决方案；

### **声明式API（与控制器模式）**

声明式 API就是当用户向 Kubernetes 提交了一个 API 对象（资源）的描述之后，Kubernetes 会负责为你保证整个集群里各项资源的状态，都与你的 API 对象描述的需求相一致。更重要的是，这个保证是一项“无条件的”、“没有期限”的承诺：对于每个保存在 etcd 里的 API 对象，Kubernetes 都通过启动一种叫做“控制器模式”（Controller Pattern）的无限循环，不断检查，然后调谐，最后确保整个集群的状态与这个 API 对象的描述一致

## **K8s提供的语言无关的分布式服务基础设施（包括服务注册发现、负载均衡、容错等）**

使用应用层解决方案的问题（比如spring cloud）

1. 作为开发者，需要关注技术选型问题，比如决定是使用一个AP系统（consul, eureka等）还是CP系统（zookeeper, etcd等）；
2. 需要弄清楚如何监控和管理这些系统，有学习和运维成本；
3. 需要为不同语言提供不同版本的客户端，以方便与服务注册发布中心通信，因为容器中可能运行的是不同语言的应用（Java/Node.js/Go）；

### **k8s在基础设施层提供的解决方案**

关于服务注册发现
用到的技术：
（1）DNS
DNS是每个操作系统都有的基础服务，利用TCP/UDP协议，在私有云，公有云，容器，Windows，Solaris等都有。客户端只需要指向域名就可以了，基础服务可以路由到服务上，可以采用多个轮转DNS配置来实现均衡负载。好处是客户端都不需要知道服务发现服务器，而使用TCP客户端就可以了。而且也不用管理DNS集群，网络路由器支持负载均衡特性，而且这些都很简单容易被理解

但DNS不适合做弹性的，动态的服务集群，当服务加入到集群或者移除时，服务的IP地址可能还在DNS服务器或者路由器（甚至有些不是你能掌控的）或者你自己的IP堆栈上进行了缓存。另外，如果你的应用侦听的是非80端口，而要DNS保存非标准的端口信息，需要使用DNS SRV记录，而这样你又需要使用特定的应用层客户端来发现这些记录

所以，需要用到下面的几个技术来解决如上问题

（2）Pods：代表服务实例，被打上标签
（3）Labels Label Selectors：通过同样一组标签，选择属于一个Services的服务实例，且是实时生效的（比如pod的加入或退出）
（4）Services：使用label selectors选择一组pod，并生成一个vip，然后就可以在DNS里配置服务的访问域名映射到这个vip

Pod很简单，就是Linux容器。Label也很简单，它们是键-值字符串，用于标记 Pod。比如 Pod A可以标记为app=cassandra, tier=backend, version=1.0, language=java，这些标记可以表示任何你的意图。

Services是一个固定的群集IP地址。该IP地址是一个虚拟IP地址，可用于发现/调用在Pod/容器中的实际端点地址。Services使用了label selector来选取你定义过的标签Pod。举例，使用选择器“app=cassandra AND tier=backend”，就得到一个虚拟的IP地址，访问所有具备上述标记的Pod，这个选择器是即时生效的，所以任何离开集群的pod或者加入到集群中的都可以自动被启动并参与到服务发现中

Kubernetes服务不是一个“东西”，一个设施或者docker容器等，它就是一个虚拟表示，所以没有单独故障点，是一个IP地址，由kubernetes来路由消息

另一个使用kubernetes服务的好处是，智能的选取Pod来加入到服务中，根据它们的存活和健康信息。Kubernetes使用内建的存活和健康检查方法，来确定一个Pod是否包含在一个特定服务之中，如果不满足条件，Pod会被驱逐出去

使用上述方案后的架构如下

![k8s-service-discovery]()

在这个方案中，我们不需要额外的配置，我们并不需要担心的DNS缓存/SRV记录，自定义库的客户端和管理额外的服务发现的基础设施。Pod可以被加入到集群中，标签选择器积极的选取符合标记的Pod，应用可以是Java, Python, Node.js, Perl, Go, .NET, Ruby, C++, Scala, Groovy等任何语言开发的。服务发现机制不关心特定的客户端而只是使用它

### **关于负载均衡**

使用Kubernetes的服务，我们完成了适度的负载均衡（没有服务注册，定制的客户端，DNS缺陷等的开销）。当我们通过DNS和一个Kubernetes服务进行交互时，将使用集群中的Pod进行负载均衡操作（使用标签选择器）。如果你不希望在负载均衡处有额外的跳转，请不要担心，虚拟IP直接路由到Pod，并不涉及到物理的网络

如果需要根据业务决策来决定使用具体集群中哪一个后端服务。一般情况下，根据具体的应用程序使用一些特定的算法，而比像“轮询”，“随机”，“会话粘滞”要复杂的自定义算法，这时使用客户端的均衡负载。在这种模式下，依然可以使用kubernetes的服务发现来找到是在哪个Pod集群，然后用客户端代码直接调用其中的Pod。fabric8.io社区的kubeflix项目，就使用Robbon作为发现插件，来使用REST API获得服务对应的所有Pod列表，然后客户端的代码决定调用哪一个pod。其他语言使用kubernetes的REST API也可以做类似的工作。可以投入做一些客户端发现库来简化这个操作。更确切的说，是把这样的业务逻辑，从应用程序中分离出来作为独立的中间件。通过使用kubernetes，你可以部署这样的模块，作为应用的独立部分，并把自定义的均衡负载逻辑定义在哪儿，如下所示：

![k8s-loadbalance]()

### **关于容错**

kubernetes有自愈的能力，如果一个pod或者pod中的一个容器停止了，可以将它重新启动，并保持ReplicaSet保持不变（比如，如果需要有10个“foo” pods，那么kubernetes总是保持有10个，如果有停掉的，会再次启动pod）

熔断降级如何做？可以使用Netflix Turbine项目来聚集和可视化集群中所有的运行中的断路器。Hystrix可以用SSE暴露信息送到Turbine中。然而Turbine如何发现哪些Pod中包含了Hystrix？可以使用kubernetes的标签。如果我们标记所有使用hystrix的pod，"hystrix.enable=true"，那么Kubeflix Turbine引擎自动发现每个断路器，获得SSE流并且显示到Turbine页面上，如下所示：

![k8s-robust]()

### **关于配置**

在Kubernetes我们使用三个结构来注入基于环境的配置：

1. Environment Variables 环境变量
2. GitRepo Volume git仓库作为文件卷
3. ConfigMap

通常我们可以设置环境变量，注入配置数据到Linux容器中，大多数语言都可以轻松读取到这些信息。 我们可以存储这些配置到git中，然后绑定配置仓库到我们的pod上（作为文件系统的文件），这样可以用任何框架来获取配置文件信息了。 最后，我们可以使用Kubernetes的ConfigMap来存储配置版本信息，作为文件系统mount到Pod上，同样的，可以用任何语言和框架来处理配置文件

Kubernetes也可以做到在运行时动态更改配置。您可以更改配置文件，通过ConfigMap这些变化动态地传播到了mount的pod。在这种情况下，你需要有一个客户端库，能够检测这些配置的变化并通知你的应用程序。Netflix的Archais有一个客户端可以做到这一点。Spring Cloud Kubernetes的Java项目使这个更容易（使用ConfigMaps）

## **Serverless**

2014年，亚马逊发布了名为Lambda的商业化无服务计算平台

所谓“无服务器”就是想让用户感觉不到服务器的存在，开发者可以完全专注于业务逻辑的编写，而不再关心任何基础设施，完全屏蔽了计算资源，是在真正地引导你不再去关心底层环境，你只要遵循标准方式来直接编写业务代码就可以了，你甚至可以把每一个具有独立功能的函数，来作为一个单独的服务进行部署和运行。这也是为什么，在有些云计算的分类方法下，无服务器计算能够单独“开宗立派”，被称为函数即服务（Function-as-a-Service，FaaS）的原因。

“无服务器”计算，它会根据我们的负载情况，依托云端庞大的规模自动地进行支撑和扩展。你不需要为云函数事先划定资源池，但对于单个函数执行单元的计算资源，还是能够进行一定控制的。最常见的是可以根据云函数的需要来选择运行环境的内存大小

主要包括两块内容：后端设施（Backend）和 函数（Function）

* **后端设施**：是指数据库、消息队列、日志、存储等这类用于支撑业务逻辑运行，但本身无业务含义的技术组件，这些后端设施都运行在云中，在无服务中将它们称为“后端即服务”（Backend as a Service，Baas）;
* **函数**：指业务逻辑代码，这里函数的概念与粒度都已经很接近程序编码角度的函数了，其区别是无服务中的函数运行在云端，不必考虑算力问题，也不必考虑容量规划（从技术角度可以不考虑，从计费角度还是需要掂量一下），在无服务中将其称为“函数即服务”（Function as a Servcie）

无服务的愿景是让开发者只需要**纯粹地关注业务**:

- 不需要考虑技术组件，后端的技术组件是现成的，可以直接取用，没有采购、版权和选型的烦恼；
- 不需要考虑如何部署，部署过程完全托管到云端，由云端自动完成；
- 不需要考虑算力，有整个数据中心支撑，算力可以认为是无限的；
- 不需要操心运维，维护系统持续平稳运行是云计算服务商的责任而不再是开发者的责任;

事件模型是无服务器的核心编程模型和运行逻辑，所以它非常适合相当广泛的事件驱动开发场景，事件的起始，要依靠触发器。云上 Serverless服务一般都配套提供了多种多样的触发器，包括 API 触发器、对象存储触发器、队列触发器等等。

如果说触发器是无服务器计算的上游的话，那么各种各样的外部交互方式，也让无服务器计算能够对外访问，并向下游输出。云端的 Serverless 环境中，一般都能够提供一系列重要类库和 SDK，让你能够在函数内访问其他云服务，尤其是像数据库、消息队列这样的外部存储.
补充：无服务器计算本身是无状态的，所有的持久化需求都要借助外部存储来实现，所以经常需要和数据库、对象存储等服务配合，这既是常用手法，也是必然选择

无服务器函数们，还可以用另一种方式联合起来，发挥出它更大的威力，这也是现在无服务器业界发展的又一个热点：即允许你按照业务逻辑的控制处理流程，以工作流的方式，进行云函数等事件处理单元的组合和编排。AWS 的 Step Functions 和 Azure 的 Logic Apps，以及阿里云的函数工作流，都是这种类型的云服务的代表。它们能够让你用配置文件或图形化的方式，来设置表达一个复杂的事件处理步骤和逻辑，这是架构在云函数之上的更高层调度框架。你可以不用把 if/else、顺序执行、并发等调度控制逻辑写在一个臃肿的函数中，而是可以分开解耦，通过工作流进行组装。这时，每一个 Serverless 函数，作为处理流程的一个环节，可以只专注做一件事情

弊端：
1. Serverless 服务的限制：

本质是受限的环境，冷启动的延时、内存的限制、云函数的运行时长、并发数上限、应用的可迁移性等，因此擅长短连接、服务无状态、适合事件驱动的场景，不适合具备复杂业务逻辑、依赖服务端状态、响应速度要求高、需要长链接等特征的场景

无服务天生“无限算力”的假设，决定了它必须按照使用量（函数运算时间和占用的内存）计费以控制消耗的算力的规模，因而**函数不会一直以活动状态常驻服务器，请求到了才会开始运行**，这就导致了函数不便依赖服务端状态、也导致了函数会有冷启动时间，响应性能可能不太好。目前无服务的冷启动过程大概是在数十到数百毫秒级别，对于java这类启动性能差的应用，甚至是接近秒的级别

2. 应用的可迁移性

腾讯云的云函数、阿里云的函数计算和 AWS 的 Lambda，它们的编写形式虽然大体相同，但在接口定义、参数结构、SDK 设计等各方面，还是会有不少的细节差异。所以除非你对于厂商绑定不敏感，否则代码的复用性也会是你不得不考虑的一个因素，为了解决厂商绑定问题，业界也涌现出了像 Serverless Framework 这样的厂商中立的第三方技术框架。通过和多个主流云厂商合作和集成，实现“一套代码，多处运行”

与单体架构、微服务架构不同，无服务架构天生的一些特点，比如冷启动、无状态、运行时间有限制等等，决定了它不是一种具有普适性的架构模式。除非是有重大变革，否则它也很难具备普适性。

一方面，对一些适合的应用来说，使用无服务架构确实能够降低开发和运维环节的成本，比如不需要交互的离线大规模计算，又比如多数 Web 资讯类网站、小程序、公共 API 服务、移动应用服务端等，都跟无服务架构擅长的短链接、无状态、适合事件驱动的交互形式很契合。

但另一方面，对于那些信息管理系统、网络游戏等应用来说，又或者说对所有具有业务逻辑复杂、依赖服务端状态、响应速度要求较高、需要长连接等特征的应用来说，无服务架构至少在目前来看并不是最合适的。这是因为，无服务天生“无限算力”的假设，就决定了它必须要按使用量（函数运算的时间和内存）来计费，以控制消耗算力的规模，所以函数不会一直以活动状态常驻服务器，只有请求到了才会开始运行。这导致了函数不便于依赖服务端状态，也导致了函数会有冷启动时间，响应的性能不可能会太好（目前，无服务的云函数冷启动过程大概是在百毫秒级别，对于 Java 这类启动性能差的应用，甚至能到秒级）

Servless最大的卖点就是简单，只涉及了后端设施（Backend）和函数（Function）两块内容。
后端设施是指数据库、消息队列、日志、存储等这一类用于支撑业务逻辑运行，但本身无业务含义的技术组件。这些后端设施都运行在云中，也就是无服务中的“后端即服务”（Backend as a Service，BaaS）。

函数指的就是业务逻辑代码。这里函数的概念与粒度，都已经和程序编码角度的函数非常接近了，区别就在于，无服务中的函数运行在云端，不必考虑算力问题和容量规划（从技术角度可以不考虑，但从计费的角度来看，你还是要掂量一下自己的钱包够不够用），也就是无服务中的“函数即服务”（Function as a Service，FaaS）

## **aPaas**

可以把aPaaS理解为PaaS的一种子形式。aPaaS的全称是application Platform as a Service，即应用程序平台即服务。Gartner对其所下的定义是：“这是基于PaaS（平台即服务）的一种解决方案，支持应用程序在云端的开发、部署和运行，提供软件开发中的基础工具给用户，包括数据对象、权限管理、用户界面等

aPaaS（应用程序平台即服务）有以下2个特征：

1. 提供快速开发的环境，用户在几个小时内就能完成应用的开发、测试、部署，并能够随时调整或更新；
2. 低代码（low code）或 零代码（no code），非技术人员就能完成应用开发；

### **aPaaS和PaaS的区别是什么？**

aPaaS和PaaS都可以完成软件的开发和部署，都支持云端访问，而两者的差异主要体现在用户人群和使用环境不一样：

1. PaaS包含所有平台级别的服务，需要技术人员在本地完成应用程序的开发和数据提供，然后部署到PaaS平台上，再分发给用户使用；
2. aPaaS是PaaS的一种子形式，在aPaaS模式下，非技术人员可以直接在云端完成应用程序的搭建、部署、使用、更新和管理；

### **低代码（low code）或 零代码（no code）**

低代码开发平台（LCDP）本身也是一种软件，它为开发者提供了一个创建应用软件的开发环境；与传统编写代码的 IDE 不同，低代码开发平台提供更易用的可视化 IDE。

简单来讲，低代码（Low Code）就是一种可视化搭建系统，从字面意思来讲，一是可视化；二是少写代码。无代码（No Code）同样从字面上来理解，一是可视化，二是不写代码。

No Code 和 Low Code 这两种的区别是，No Code 是完全不需要写代码，而 Low Code 是需要写部分代码，整体通过拖拽的方式生成

## **云原生时代的编程语言**

### **Java**

Java与其他语言竞争，底气从来不在于语法、类库有多么先进好用，而是来自它庞大的用户群和极其成熟的软件生态.然而随着云原生时代的来临，目前已经有了可预见的、足以威胁动摇其根基的潜在可能性

### **Java 在云原生时代的弱势**

1. 将程序连同它的运行环境一起封装到稳定的镜像里，现已是一种主流的应用程序分发方式。Docker同样提出过“一次构建，到处运行”（Build Once, Run Anywhere）的口号，尽管它只能提供环境兼容性和有局限的平台无关性（指系统内核功能以上的ABI兼容），且完全不可能支撑架构中立性，所以将“一次构建，到处运行”与“一次编写，到处运行”对立起来并不严谨恰当，但是无可否认，今天Java技术“一次编译，到处运行”的优势，已经被容器大幅度地削弱，不再是大多数服务端开发者技术选型的主要考虑因素了;

2. 哪怕再小的Java程序也要带着完整的虚拟机和标准类库，使得镜像拉取和容器创建效率降低，进而使整个容器生命周期拉长。基于Java虚拟机的执行机制，使得任何Java的程序都会有固定的基础内存开销，以及固定的启动时间，而且Java生态中广泛采用的依赖注入进一步将启动时间拉长，使得容器的冷启动时间很难缩短;

对于原生语言的挑战，最有力最彻底的反击手段无疑是将字节码直接编译成可以脱离Java虚拟机的原生代码。如果真的能够生成脱离Java虚拟机运行的原生程序，将意味着启动时间长的问题能够彻底解决，因为此时已经不存在初始化虚拟机和类加载的过程；也意味着程序马上就能达到最佳的性能，因为此时已经不存在即时编译器运行时编译，所有代码都是在编译期编译和优化好的；没有了Java虚拟机、即时编译器这些额外的部件，也就意味着能够省去它们原本消耗的那部分内存资源与镜像体积

Java支持提前编译最大的困难在于它是一门动态链接的语言，它假设程序的代码空间是开放的（Open World），允许在程序的任何时候通过类加载器去加载新的类，作为程序的一部分运行。要进行提前编译，就必须放弃这部分动态性，假设程序的代码空间是封闭的（Closed World），所有要运行的代码都必须在编译期全部可知。这一点不仅仅影响到了类加载器的正常运作，除了无法再动态加载外，反射（通过反射可以调用在编译期不可知的方法）、动态代理、字节码生成库（如CGLib）等一切会运行时产生新代码的功能都不再可用，如果将这些基础能力直接抽离掉，Helloworld还是能跑起来，但Spring肯定跑不起来，Hibernate也跑不起来，大部分的生产力工具都跑不起来，整个Java生态中绝大多数上层建筑都会轰然崩塌

---

## **Netflix服务治理体系**

![netflix-service-govern]()

## **Service Mesh(服务网格)：下一代的微服务架构**

### **定义**

A service mesh is a dedicated infrastructure layer for handling service-to-service communication. It’s responsible for the reliable delivery of requests through the complex topology of services that comprise a modern, cloud native application. In practice, the service mesh is typically implemented as an array of lightweight network proxies that are deployed alongside application code, without the application needing to be aware. (But there are variations to this idea, as we’ll see.)

翻译：Service Mesh 是一个基础设施层，用于处理服务间通信。云原生应用有着复杂的服务拓扑，Service Mesh 保证请求可以在这些拓扑中可靠地穿梭。在实际应用当中，Service Mesh 通常是由一系列轻量级的网络代理组成的，它们与应用程序部署在一起，但应用程序不需要知道它们的存在。

关于这个定义有以下两个值得我们关注的核心点：

1. “dedicated infrastructure layer”：Service Mesh 不是用来解决业务领域问题的，而是一层专门的基础设施（中间件）;
2. “service-to-service communication”：Service Mesh 的定位很简单也很清晰，就是用来处理服务与服务之间的通讯；
3. “reliable delivery of requests”：服务间通讯为什么需要特殊处理？因为网络是不可靠的，Service Mesh 的愿景就是让服务间的请求传递变得可靠；
4. “cloud native application”：Service Mesh 从一开始就是为现代化的云原生应用而生，瞄准了未来的技术发展趋势；
5. “network proxies”：具体 Service Mesh 应该怎么实现？典型方式都是通过一组轻量级的网络代理，在应用无感知的情况下偷偷就把这事给干了；
6. “deployed alongside application code”：这些网络代理一定是跟应用部署在一起，一对一近距离贴心服务（比房产中介专一得多）；否则，如果应用与代理之间也还是远程不靠谱通讯，这事儿就没完了；

Service Mesh 这个基础设施层的职能边界：

- 服务治理
- 请求可靠传输

The concept of the service mesh as a separate layer is tied to the rise of the cloud native application. In the cloud native model, a single application might consist of hundreds of services; each service might have thousands of instances; and each of those instances might be in a constantly-changing state as they are dynamically scheduled by an orchestrator like Kubernetes. Not only is service communication in this world incredibly complex, it’s a pervasive and fundamental part of runtime behavior. Managing it is vital to ensuring end-to-end performance and reliability

翻译：随着云原生应用的崛起，Service Mesh 逐渐成为一个独立的基础设施层。在云原生模型里，一个应用可以由数百个服务组成，每个服务可能有数千个实例，而每个实例可能会持续地发生变化。服务间通信不仅异常复杂，而且也是运行时行为的基础。管理好服务间通信对于保证端到端的性能和可靠性来说是非常重要的

服务架构历史发展历程：
原始分布式 ——》 单体架构（Monolithic） ——》模块化（进程内函数调用，逻辑隔离，物理部署在一起）——》SOA（面向服务架构）——》微服务（跨进程跨网络，服务数爆炸，服务治理成为必须）——》service mesh（解耦业务逻辑和服务间网络通讯）——》 Serverless（无服务）

康威定律：设计系统的架构受制于产生这些设计的组织沟通的结构

银弹理论：没有任何一种技术或管理上的进步，能够极大地提升生产效率。引申到软件开发领域，即没有任何一种技术，可以完美地解决软件开发中的问题

微服务架构面临的最大痛点：服务间网络通讯问题

why是网络通讯？根据分布式计算中的8个谬论

1. 网络是可靠的
2. 网络延迟是0
3. 网络带宽是无限的
4. 网络是安全的
5. 网络拓扑从不改变
6. 只有一个管理员
7. 传输成本是0
8. 网络是同构的

Service Mesh 又称为第二代微服务架构，其解决了第一代微服务架构什么问题？

1. 语言绑定（组件库一般都是基于某个语言）
2. 应用侵入性高（同一个进程）

### **微服务的演进过程**

1. 业务逻辑和网络控制逻辑耦合在一起 ，问题：不同的地方需要重复实现网络控制逻辑；
2. 引入公共库来负责网络控制逻辑，和业务逻辑解耦（比如spring cloud提供的组件） 好处：解耦，消除重复；问题：人力和时间成本（需要花时间学习以及维护）、语言绑定（组件库一般都是基于某个语言），应用侵入性高（同一个进程）；
3. 边车模式（sidecar）：应用旁边单独部署的网络代理，负责处理网络相关的逻辑；

“边车”的意思是，微服务基础设施会由系统自动地在服务的资源容器（指 Kubernetes 的 Pod）中注入一个通讯代理服务器（相当于那个挎斗），用类似网络安全里中间人攻击的方式进行流量劫持，在应用毫无感知的情况下，悄悄接管掉应用的所有对外通讯

### **Service Mesh  ——》 Service Mesh 2**

在data plane（所有的sidecar组合）基础上增加了control plane

- data plane(数据平面)：负责接收或转发系统中每一个数据包或请求，提供服务发现、健康检查、路由、负载均衡、流量控制、安全、监控等一些列跟服务间数据通信有关的功能；

- control plane（控制平面）：为在网格中运行的数据平面提供策略和配置，不接收系统中任何数据包或请求;

### **Service Mesh和k8s的关系**

k8s主要是为了解决容器的编排和调度问题，本质上是管理应用生命周期（调度器），对service mesh的实现提供支持和帮助，pod是k8s最小的调度单位，pod天生支持多容器的部署，因此为植入sidecar提供了便利，为service mesh的部署提供了非常大的支持

Service Mesh主要是为了解决服务间网络通信的问题，本质上是管理服务通信（代理）

Service Mesh和API网关的区别：
API网关是部署在应用边界的，并没有侵入到应用内部，主要功能是对内部API进行一个聚合和抽象，以方便外部进行调用
功能有重叠，但角色不一样，Service Mesh在应用内，API网关在应用之上（边界）

### **Service Mesh的技术标准**

1. UDPA（Universal data plane API）:统一的数据平面API，主要是为不同的数据平面提供一个统一的API，方便应用无缝的接入，不同的数据平面有很多比如Envoy、Linkerd等，他们的接入标准是不一样的；
2. SMI（Service Mesh Interface）
