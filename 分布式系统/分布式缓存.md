## **分布式缓存**

什么是缓存：可以进行**高速数据交换**的存储器，是一种利**用空间换时间**的设计

- 将数据写入读取速度更快的存储（设备）；
- 将数据缓存到离应用更近的位置；
- 将数据缓存到离用户更近的位置；

1. 从开发角度来说，引入缓存会提高系统复杂度，因为你要考虑缓存的失效、更新、一致性等问题;

2. 从运维角度来说，缓存会掩盖一些缺陷，让问题在更久的时间以后，出现在距离发生现场更远的位置上;

3. 从安全角度来说，缓存可能会泄漏某些保密数据，也是容易受到攻击的薄弱点;

冒着上述种种风险，仍能说服你引入缓存的理由，总结起来无外乎以下两种。

- **为缓解CPU压力而引入缓存**：譬如把方法运行结果存储起来、把原本要实时计算的内容提前算好、对一些公用的数据进行复用，这可以节省CPU算力，顺带提升响应性能;

- **为缓解I/O压力而引入缓存**：譬如把原本对网络、磁盘等较慢介质的读写访问变为对内存等较快介质的访问，将原本对单点部件（如数据库）的读写访问变为对可扩缩部件（如缓存中间件）的访问，顺带提升响应性能;

因此，缓存虽然是典型以空间换时间来提升性能的手段，但它的出发点是缓解CPU和I/O资源在峰值流量下的压力，“顺带”而非“专门”地提升响应性能。这里的言外之意是**如果可以通过增强CPU、I/O本身的性能（譬如扩展服务器的数量）来满足需要的话，那升级硬件往往是更好的解决方案**，即使需要一些额外的投入成本，也通常要优于引入缓存后可能带来的风险

## **缓存关注的特征**

1. 命中率：正确返回结果次数/请求缓存次数，命中率越高，表明缓存的使用率越高；
2. 最大元素（空间）：缓存中可以存放的最大元素数量，一旦元素数量超过这个值，就会触发清空策略；
3. 清空（淘汰）策略：缓存容量有限，当缓存容量不够的时候的淘汰策略
    - FIFO：先进先出策略，最先进入缓存的数据在缓存空间不够的情况下（超出最大元素限制）会被优先被清除掉，优先保障最新数据可用，FIFO的实现十分简单，但一般来说它并不是优秀的淘汰策略，越是频繁被用到的数据，往往会越早存入缓存之中。如果采用这种淘汰策略，很可能会大幅降低缓存的命中率；
    - LFU：最少频繁使用策略，根据元素的被使用次数判断，清除使用次数较少的元素释放空间,策略算法主要比较元素的hitCount（命中次数）,适合保证高频数据有效性场景；
    - LRU：最近最少使用策略，根据元素最后一次被使用的时间戳，清除最远使用时间戳的元素释放空间。策略算法主要比较元素最近一次被get使用时间,在热点数据场景下较适用;

根据缓存与应用的耦合度，分为local cache（本地缓存）和 remote cache（分布式缓存）

- local cache（本地缓存）:应用和cache是在同一个进程内部，请求缓存非常快速，没有过多的网络开销等，在单体应用不需要集群支持或者集群情况下各节点无需互相通知的场景下使用本地缓存较合适；同时，它的缺点也是因为缓存跟应用程序耦合，容量受限于进程内存大小，而且多个应用程序无法直接共享缓存，可能存在数据不一致的问题，各应用或集群的各节点都需要维护自己的单独缓存，对内存是一种浪费;
- remote cache（分布式缓存）：与应用分离的缓存组件或服务，其最大的优点是自身就是一个独立的应用，与本地应用隔离，多个应用可直接的共享缓存;

分布式缓存的两种形式

- **复制式缓存**：复制式缓存可以看作“能够支持分布式的进程内缓存”，它的工作原理与Session复制类似。缓存中所有数据在分布式集群的每个节点里面都有一份副本，读取数据时无须网络访问，直接从当前节点的进程内存中返回，理论上可以做到与进程内缓存一样高的读取性能；但当数据发生变化时，就必须遵循复制协议，将变更同步到集群的每个节点中，复制性能随着节点的增加呈现平方级下降，变更数据的代价十分高昂。复制式缓存的代表是JBossCache，这是JBoss针对企业级集群设计的缓存方案，支持JTA事务，依靠JGroup进行集群节点间的数据同步。以JBossCache为代表的复制式缓存曾有一段短暂的兴盛期，但今天基本上已经很难再见到使用这种缓存形式的大型信息系统了。JBossCache被淘汰的主要原因是写入性能太差，它在小规模集群中同步数据尚算差强人意，但在大规模集群下，很容易因网络同步的速度跟不上写入速度，进而导致在内存中累计大量待重发对象，最终引发OutOfMemory崩溃；

- **集中式缓存**：集中式缓存是目前分布式缓存的主流形式，它的读写都需要网络访问，好处是不会随着集群节点数量的增加而产生额外的负担，坏处是读写都不再可能达到进程内缓存那样的高性能。集中式缓存还有一个必须提到的关键特点，它与使用缓存的应用分处在独立的进程空间中。其好处是能够为异构语言提供服务，譬如用C语言编写的Memcached完全可以毫无障碍地为Java语言编写的应用提供缓存服务；但坏处是如果要缓存对象等复杂类型的话，基本上只能靠序列化来支撑具体语言的类型系统（支持Hash类型的缓存，可以部分模拟对象类型），不仅有序列化的成本，还很容易导致传输成本的显著增加。举个例子，假设某个有100个字段的大对象的其中1个字段的值发生变更，通常缓存不得不把整个对象所有内容重新序列化传输出去才能实现更新，因此，一般集中式缓存更提倡直接缓存原始数据类型而不是对象。相比之下，JBossCache通过它的字节码自审（Introspection）功能和树状存储结构（TreeCache），做到了自动跟踪、处理对象的部分变动，当用户修改了对象中某些字段的数据时，缓存只会同步对象中真正变更的那部分数据。从数据一致性角度来说，缓存本身也有集群部署的需求，理论上你应该认真考虑一下是否能接受不同节点取到的缓存数据可能存在差异的情况。譬如刚刚放入缓存中的数据，另外一个节点马上访问却发现未能读到；刚刚更新缓存中的数据，另外一个节点在短时间内读取到的仍是旧的数据，等等。根据分布式缓存集群能否保证数据一致性，可以将它分为AP和CP两种类型。此处又一次出现了“理论上”，是因为我们在实际开发中通常不会把追求强一致性的数据使用缓存来处理，可以这样做，但没必要（可类比MESI等缓存一致性协议）。譬如，Redis集群就是典型的AP式，有着高性能、高可用等特点，却并不保证强一致性。而对于能够保证强一致性的ZooKeeper、Doozerd、etcd等分布式协调框架，通常不会有人将它们当作“缓存框架”来使用，这些分布式协调框架的吞吐量相对Redis来说是非常有限的。不过ZooKeeper、Doozerd、etcd倒是常与Redis及其他分布式缓存搭配工作，用来实现通知、协调、队列、分布式锁等功能；

本地缓存与进程内缓存各有所长，也各有局限，它们是互补而非互斥的关系，如有需要，完全可以将两者搭配使用，构成透明多级缓存（TransparentMultilevelCache，TMC），如下图所示

![TMC](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/resources/TMC.png)

先不考虑“透明”的话，多级缓存是很好理解的，使用进程内缓存做一级缓存，分布式缓存做二级缓存，如果能在一级缓存中查询到结果就直接返回，否则便到二级缓存中去查询，再将二级缓存中的结果回填到一级缓存，以后再访问该数据就没有网络请求了。如果二级缓存也查询不到，就发起对最终数据源的查询，将结果回填到一、二级缓存中去

尽管多级缓存结合了进程内缓存和分布式缓存的优点，但它的代码侵入性较大，需要由开发者承担多次查询、多次回填的工作，也不便于管理，如超时、刷新等策略都要设置多遍，数据更新更是麻烦，很容易出现各个节点的一级缓存以及二级缓存中数据不一致的问题。所以，必须“透明”地解决以上问题，才能使多级缓存具有实用的价值。

一种常见的设计原则是变更以分布式缓存中的数据为准，访问以进程内缓存的数据优先。大致做法是当数据发生变动时，在集群内发送推送通知（简单点的话可采用Redis的PUB/SUB，求严谨的话可引入ZooKeeper或etcd来处理），让各个节点的一级缓存中的相应数据自动失效。当访问缓存时，提供统一封装好的一、二级缓存联合查询接口，接口外部是只查询一次，接口内部自动实现优先查询一级缓存，未获取到数据再自动查询二级缓存的逻辑。

TMC示例：

![TMC-example](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/resources/TMC-example.png)

模块划分
TMC 本地缓存整体结构分为如下模块：

- Jedis-Client：Java 应用与缓存服务端交互的直接入口，接口定义与原生 Jedis-Client 无异；
- Hermes-SDK：自研“热点发现+本地缓存”功能的 SDK 封装，Jedis-Client 通过与它交互来集成相应能力；
- Hermes 服务端集群：接收 Hermes-SDK 上报的缓存访问数据，进行热点探测，将热点 key 推送给 Hermes-SDK 做本地缓存；
- 缓存集群：由代理层和存储层组成，为应用客户端提供统一的分布式缓存服务入口；
- 基础组件：etcd 集群、Apollo 配置中心，为 TMC 提供“集群推送”和“统一配置”能力；

基本流程

1. key 值获取

    - Java 应用调用 Jedis-Client 接口获取 key 的缓存值时，Jedis-Client 会询问 Hermes-SDK 该 key 当前是否是热点key；
    - 对于 热点key ，直接从 Hermes-SDK 的 热点模块 获取热点 key 在本地缓存的 value 值，不去访问 缓存集群 ，从而将访问请求前置在应用层；
    - 对于非 热点key ，Hermes-SDK 会通过 Callable回调 Jedis-Client 的原生接口，从 缓存集群 拿到 value 值；
    - 对于 Jedis-Client 的每次 key 值访问请求，Hermes-SDK 都会通过其 通信模块 将 key 访问事件异步上报给 Hermes 服务端集群 ，以便其根据上报数据进行“热点探测”；

2. key 值过期

    - Java 应用调用 Jedis-Client 的 set() del() expire()接口时会导致对应 key 值失效，Jedis-Client 会同步调用 Hermes-SDK 的 invalid()方法告知其“key 值失效”事件；
    - 对于 热点 key ，Hermes-SDK 的 热点模块 会先将 key 在本地缓存的 value 值失效，以达到本地数据强一致。同时 通信模块 会异步将“key 值失效”事件通过 etcd 集群 推送给 Java 应用集群中其他 Hermes-SDK 节点；
    - 其他 Hermes-SDK 节点的 通信模块 收到 “key 值失效”事件后，会调用 热点模块 将 key 在本地缓存的 value 值失效，以达到集群数据最终一致；

3. 热点发现

    - Hermes 服务端集群 不断收集 Hermes-SDK上报的 key 访问事件，对不同业务应用集群的缓存访问数据进行周期性（3s 一次）分析计算，以探测业务应用集群中的热点 key列表；
    - 对于探测到的热点 key列表，Hermes 服务端集群 将其通过 etcd 集群 推送给不同业务应用集群的 Hermes-SDK 通信模块，通知其对热点 key列表进行本地缓存；

4. 配置读取

    - Hermes-SDK 在启动及运行过程中，会从 Apollo 配置中心 读取其关心的配置信息（如：启动关闭配置、黑白名单配置、etcd 地址...
    - Hermes 服务端集群 在启动及运行过程中，会从 Apollo 配置中心 读取其关心的配置信息（如：业务应用列表、热点阈值配置、etcd 地址...）；

根据应用整体架构来看，还可分为客户端缓存 和 服务端缓存

客户端缓存可以包括：页面缓存、浏览器缓存（http 缓存）和 APP缓存

H5页面的缓存：sessionStorage、localStorage、本地数据库、离线存储

- sessionStorage：将数据保存在session对象中，所谓session，指用户浏览某个网站时，从进入网站到浏览器关闭的这段时间，也就是用户浏览这个网站所花费的时间，只在当前的窗口有效，打开一个新的同源窗口，或者说重启浏览器都失效。数据大小：可以保存5MB甚至更多;

- localStorage：将数据保存在客户端本地的硬件设备(通常是指硬盘，但也可以是其他硬件设备)，即使浏览器被关闭了，该数据依然存在，下次打开浏览器访问网站时仍然可以继续使用。但是，数据保存是按不同的浏览器分别进行的，也就是说，如果打开别的浏览器，是读取不到在这个浏览器中保存的数据的。数据一直保存在硬盘中。持久性保存(但是不同的浏览器保存的数据，是不能通用的)。数据大小：可以保存5MB甚至更多

cookie 与 sessionStorage 及 localStorage的区别

共同点：
都是在客户端存储数据，且是同源的

区别：

1. 存储大小不一样：cookie存储数据最大只能为4kb，而sessionStorage与localStorage可以保存5MB甚至更多数据;
2. 存储位置不一样：Cookie数据始终在同源的http请求中携带，即cookie在浏览器与服务器之间来回传递，而sessionStorage与localStorage不会自动发给服务端，仅在本地保存;
3. 数据有效期不同：
sessionStorage仅在当前浏览器窗口未关闭之前有效(同源的新窗口不生效)
localStorage仅在当前浏览器下永久生效(不同的浏览器不能共享数据)，不管关闭还是重新打开都还是生效的
Cookie只在设置的cookie过期时间之前一直有效，即使窗口或者浏览器关闭，或者打开新的同源窗口
4. 作用域不同：
sessionStorage只在同一个浏览器的同一个窗口中共享，同一个浏览器的不同窗口无法共享，即使是同一个页面
localStorage在所有的同源窗口（子域之间不能共享数据，不支持改domain，可以用postMassage）中都是共享的(仅在同一个浏览器中)
cookie在所有的同源窗口（子域之间可以把各自的domain改成和主域一样来实现共享数据）都是共享的(仅在同一个浏览器中)

- 本地数据库

indexedDB（以前叫 web sql），Web Storage（sessionStorage和localStorage）使用简单字符串键值对在本地存储数据，方便灵活，但是对于大量结构化数据存储力不从心，IndexedDB是为了能够在客户端存储大量的结构化数据，并且使用索引高效检索的API

H5离线存储 vs http 缓存 的区别
    1. 配置与 web server 解耦：静态资源做 http 缓存是要修改 web server 配置的，而 h5离线存储只需要配置 manifest
    2. 完全脱机：h5离线存储一旦缓存成功，完全脱机化，直到 manifest 发生变化

- APP 缓存

  - Android：轻量级缓存框架如ASimpleCache，可以缓存 json、Bitmap、序列化的 java 对象和二进制对象等
  - iOS：常用缓存框架如SDWebImage（图片缓存）、NsCache 等

- 浏览器缓存（http 缓存）
分为 强制缓存 和 协商缓存

  - **强制缓存**：在第一次访问服务器获取到数据之后，在过期时间之内不会再去服务器重复请求数据，实现这个流程的核心就是知道当前时间是否超过了过期时间。过期时间一般是在第一次访问服务器时返回的响应头获取。http1.0协议中是 expires 字段，http1.1协议中试 cache-control:max-age 字段。读取缓存资源的时候不发出任何请求，资源的状态码是200（from cache）。强制缓存在浏览器的地址输入、页面链接跳转、新开窗口、前进和后退中均可生效，但在用户主动刷新页面时应当自动失效；

  - **协商缓存**：与强制缓存不同的是，每次读取数据的时候都需要跟服务器通信，但每次通信不是获取数据，而是判断服务器资源是否发生更新。协商缓存有两种变动检查机制，分别是根据资源的修改时间进行检查，以及根据资源唯一标识是否发生变化进行检查，它们都是靠一组成对出现的请求、响应Header来实现的，包括 lasted_modified_time + If_Modified_Since 和 Etag + If_None_Match。如果资源没有变化，则返回状态码304。根据约定，协商缓存不仅在浏览器的地址输入、页面链接跳转、新开窗口、前进、后退中生效，而且在用户主动刷新页面（F5）时同样是生效的，只有用户强制刷新（Ctrl+F5）或者明确禁用缓存（譬如在DevTools中设定）时才会失效，此时客户端向服务端发出的请求会自动带有“CacheControl:nocache”；

HTTP中的协商缓存与强制缓存并没有互斥性，这两套机制是并行工作的。譬如，当强制缓存存在时，直接从强制缓存中返回资源，无须进行变动检查；而当强制缓存超过时效，或者被禁止（nocache/mustrevalidate）时，协商缓存仍可以正常工作。

常见本地缓存框架
Guava Cache、EHCache、Spring Cache

## **缓存雪崩、缓存击穿和缓存穿透问题及解决思路**

核心解决思路：不让请求穿透缓存积压到数据库，或者减少积压到数据库的请求量

## **缓存击穿**

我们都知道缓存的基本工作原理是首次从真实数据源加载数据，完成加载后回填入缓存，以后其他相同的请求就从缓存中获取数据，以缓解数据源的压力。如果缓存中某些热点数据忽然因某种原因失效了，譬如由于超期而失效，此时又有多个针对该数据的请求同时发送过来，这些请求将全部未能命中缓存，到达真实数据源中，导致其压力剧增，这种现象被称为缓存击穿

解决思路

1. 不给热点数据设置过期时间；
2. 热点数据由代码来手动管理：缓存击穿是仅针对热点数据自动失效才引发的问题，对于这类数据，可以直接由开发者通过代码来有计划地完成更新、失效，避免由缓存的策略自动管理；
3. 业界比较常用的做法是使用锁。以请求该数据的key值为锁，使得只有第一个请求可以流入真实的数据源中，对其他线程则采取阻塞或重试策略。如果是进程内缓存出现问题，施加普通互斥锁即可，如果是分布式缓存中出现问题，就加分布式锁，这样数据源就不会同时收到大量针对同一个数据的请求了。

简单地来说，就是在缓存失效的时候（判断拿出来的值为空），不是立即去load db，而是先使用缓存工具的某些带成功操作返回值的操作（比如Redis的SETNX或者Memcache的ADD）去set一个mutex key，当操作返回成功时，再进行load db的操作并回设缓存；否则，就重试整个get缓存的方法——也即保证只有一个线程去DB取数据并重新缓存，限制了对DB的并发请求，其他线程则等待重试

实例代码：

```Java
public String get(key) {
    String value = redis.get(key);
    if (value == null) { //代表缓存值过期
       //设置3min的超时，防止del操作失败的时候，下次缓存过期一直不能load db
       if (redis.setnx(key_mutex, 1, 3 * 60) == 1) {  
            //代表设置成功
            value = db.get(key);
            redis.set(key, value, expire_secs);
            redis.del(key_mutex);
       } else {
            //这个时候代表同时候的其他线程已经load db并回设到缓存了，这时候重试获取缓存值即可
            sleep(50);
            get(key);  //重试
       }
    } else {
        return value;
    }
}
```

### **缓存雪崩**

缓存击穿是针对单个热点数据失效，由大量请求击穿缓存而给真实数据源带来压力。还有一种可能更普遍的情况，即不是针对单个热点数据的大量请求，而是由于大批不同的数据在短时间内一起失效，导致这些数据的请求都击穿缓存到达数据源，同样令数据源在短时间内压力剧增。

出现种情况，往往是因为

1. 系统有专门的缓存预热功能，或者大量公共数据是由某一次冷操作加载的，使得由此载入缓存的大批数据具有相同的过期时间，在同一时刻一起失效；

2. 也可能是因为缓存服务由于某些原因崩溃后重启，造成大量数据同时失效;

解决思路

1. 避免给大量的数据设置相同的过期时间：可以在用 EXPIRE 命令给每个数据设置过期时间时，给这些数据的过期时间增加一个较小的随机数（例如，随机增加 1~3 分钟），这样一来，不同数据的过期时间有所差别；

2. 控制访问数据库的压力：也可以考虑采取单个热点缓存数据失效击穿的方案，采取分布式锁限制对每个key只有一个线程去数据库获取数据，控制对数据库的压力。此方案可能存在的一个问题是，如果请求量非常大，即使每个key仅限制一个线程访问数据库，也可能会造成同时有很多请求访问数据库，造成数据库很大压力，这个时候需要同步考虑熔断/限流+服务降级的策略；

3. 熔断/限流+服务降级的策略：熔断是指发生缓存雪崩时，业务服务实例不发请求给后端的数据库。限流是指一旦缓存不能访问，则可以只通过十分之一的请求量到数据库（因为缓存抗压的能力一般是数据库的十倍以上）。所有不发送到后端的请求，在业务服务实例处进行服务降级处理，根据不同业务情况直接返回默认数据或空值甚至异常；服务降级可以考虑区分核心业务数据or非核心业务数据，针对非核心业务数据，直接返回默认数据或空值甚至异常，总之不访问缓存或数据库。针对核心业务数据，可以继续访问缓存甚至数据库，这样可以大大降低访问数据库的请求量；
具体实现方式可考虑，用时间轮（每3秒一个时间片，十个时间片形成一个时间轮）+滑动窗口的方法，来统计一段时间内访问数据库请求的数量，达到不同阈值，则执行限流或熔断的策略；

### **缓存穿透**

缓存穿透是指要访问的数据既不在 Redis 缓存中，也不在数据库中，导致请求在访问缓存时，发生缓存缺失，再去访问数据库时，发现数据库中也没有要访问的数据。此时，应用无法从数据库中读取数据再写入缓存来服务后续请求，这样一来，缓存也就成了“摆设”，如果应用持续有大量请求访问数据，就会同时给缓存和数据库带来巨大压力

缓存穿透会发生在什么时候呢？

1. 业务层误操作：缓存中的数据和数据库中的数据被误删除了，所以缓存和数据库中都没有数据；
2. 恶意攻击：专门访问数据库中没有的数据；

解决思路

1. 缓存空值或缺省值：一旦发生缓存穿透，我们就可以针对查询的数据，在 Redis 中缓存一个空值或是和业务层协商确定的缺省值（例如，库存的缺省值可以设为 0）。紧接着，应用发送的后续请求再进行查询时，就可以直接从 Redis 中读取空值或缺省值，返回给业务应用了，避免了把大量请求发送给数据库处理，保持了数据库的正常运行；

缓存空对象带来的问题：缓存数据和数据库存储数据会有一段时间的不一致，可能会对业务有一定影响，例如：空值缓存过期时间设置为5分钟，如果此时数据库存储恢复了这个数据，那这段时间内就会出现数据不一致，因此需要在数据库存储数据恢复的同时，让缓存中的空值数据失效，这样下次再获取缓存数据的时候就可以从数据库中获取正确的最新数据缓存。

2. 布隆过滤器：

使用布隆过滤器解决缓存穿透

布隆过滤器由一个初值都为0的bit数组 和 N个哈希函数组成，可以用来快速判断某个数据是否存在。

当我们想标记某个数据存在时（例如，数据已被写入数据库），布隆过滤器会通过三个操作完成标记：

1. 使用 N 个哈希函数，分别计算这个数据的哈希值，得到 N 个哈希值；
2. 然后，我们把这 N 个哈希值对 bit 数组的长度取模，得到每个哈希值在数组中的对应位置；
3. 最后，我们把对应位置的 bit 位设置为 1，这就完成了在布隆过滤器中标记数据的操作；

如果数据不存在（例如，数据库里没有写入数据），我们也就没有用布隆过滤器标记过数据，那么，bit 数组对应 bit 位的值仍然为 0。当需要查询某个数据时，我们就执行刚刚说的计算过程，先得到这个数据在 bit 数组中对应的 N 个位置。紧接着，我们查看 bit 数组中这 N 个位置上的 bit 值。只要这 N 个 bit 值有一个不为 1，这就表明布隆过滤器没有对该数据做过标记，所以，查询的数据一定没有在数据库中保存，如下图

![bloom-filter](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/resources/bloom-filter.png)

正是基于布隆过滤器的快速检测特性，我们可以在把数据写入数据库时，使用布隆过滤器做个标记。当缓存缺失后，应用查询数据库时，可以通过查询布隆过滤器快速判断数据是否存在。如果不存在，就不用再去数据库中查询了。这样一来，即使发生缓存穿透了，大量请求只会查询 Redis 和布隆过滤器，而不会积压到数据库，也就不会影响数据库的正常运行。布隆过滤器可以使用 Redis 实现，本身就能承担较大的并发访问压力

不过任何事物都有两面性，布隆过滤器也不例外，它主要有两个缺陷：

1. 它在判断元素是否在集合中时是有一定错误几率的，比如它会把不是集合中的元素判断为处在集合中（PS：选择多个 Hash 函数计算多个 Hash 值，可以减少误判的几率）；
2. 不支持删除元素;

关于第一个缺陷，主要是 Hash 算法的问题。因为布隆过滤器是由一个二进制数组和一个 Hash 算法组成的，Hash 算法存在着一定的碰撞几率。Hash 碰撞的含义是不同的输入值经过 Hash 运算后得到了相同的 Hash 结果

Hash 的含义是不同的输入依据不同的算法映射成独一无二的固定长度的值，也就是我输入字符串“1”，根据 CRC32 算法，值是 2212294583。但是现实中 Hash 算法的输入值是无限的，输出值的值空间却是固定的，比如 16 位的 Hash 值的值空间是 65535，那么它的碰撞几率就是 1/65535，即如果输入值的个数超过 65535 就一定会发生碰撞

为什么不映射成更长的 Hash 值呢？

更长的 Hash 值会带来更高的存储成本和计算成本。即使使用 32 位的 Hash 算法，它的值空间长度是 2 的 32 次幂减一，约等于 42 亿，用来映射 20 亿的用户数据，碰撞几率依然有接近 50%

布隆过滤器的误判有一个特点，就是它只会出现“false positive”的情况。这是什么意思呢？当布隆过滤器判断元素在集合中时，这个元素可能不在集合中。但是一旦布隆过滤器判断这个元素不在集合中时，它一定不在集合中。这一点非常适合解决缓存穿透的问题

### **缓存污染**

缓存污染是指缓存中的数据与真实数据源中的数据不一致的现象。尽管缓存通常不追求强一致性，但这显然不能等同于不要求缓存和数据源间的最终一致性。

缓存污染多数是由开发者更新缓存不规范造成的，譬如你从缓存中获得了某个对象，更新了对象的属性，但最后因为某些原因，譬如后续业务发生异常回滚了，最终没有成功写入数据库，导致缓存的数据是新的，数据库中的数据是旧的。

为了尽可能地提高使用缓存时的一致性，目前已经有很多更新缓存时可以遵循的设计模式，譬如CacheAside、Read/WriteThrough、WriteBehindCaching等。
其中最简单、成本最低的CacheAside模式是指：

- 读数据时，先读缓存，如果没有，再读数据源，然后将数据放入缓存，再响应请求；
- 写数据时，先写数据源，然后失效（而不是更新）掉缓存;

### **缓存预热**

缓存预热是指系统上线后，提前将先关的缓存数据加载到缓存系统，避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题，用户直接查询预先被预热的缓存数据

如果不进行预热，那么缓存初始情况数据为空，系统上线初期，对于高并发的流量，都会访问到数据库，对数据库造成较大的压力

## **memcached（C语言）**

memcache是项目名称，memcached是memcache服务端可以执行文件的名称

memcached客户端与服务端的通讯通过构建在TCP协议之上的memcache协议来通信，协议支持两种数据：文本数据 和 非结构化数据（二进制字节流）

文本数据是指客户端发送给服务器的命令以及服务端的响应，而非结构化数据主要用于客户端和服务端数据的传递

memcached的key只能是字符串，但value可以是不同语言的任意类型，比如对于java可以是object、各种数据结构（比如list、map）或实现了序列化接口的自定义类，也即只要能够被序列化成二进制字节流即可

### **memcached的线程模型**

memcached有1条主线程，以及4条woker线程。可以通过启动参数-t来指定worker线程的数量，如果不指定，默认情况下就是4。最好不要设置超过64条线程，线程一旦太多，频繁的切换也需要开销，另外就是memcached大量使用互斥锁，可能会使得没有抢到锁的线程处于等待状态。
简单来说，主线程负责监听请求，分发给worker线程，而worker线程负责接收具体的请求命令并且作出处理，如下图所示

![memcached-concurrent-model](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/resources/memached-concurrent-model.png)

主线程监听到有新的连接之后，会做出一次选择，我们称之为dispatch，其实就是确定此连接后续会由哪个worker线程处理。一旦确定worker线程，接下来主线程会借助管道通知该worker线程。在每条worker线程的内部，都有一个连接队列。worker线程收到通知之后，会从连接队列中取出一个连接，用于后续接受具体的命令，以及做出响应

### **memcached的操作是原子的吗？**

所有被发送到memcached的**单个命令是原子的**，如果针对同一份数据同时发送了一个set命令和一个get命令，它们将被串行化、先后执行，即使在多线程模式，所有的命令都是原子的，but，**命令序列不是原子的**。

针对命令序列的原子问题，memcached在更高版本提供了gets()和cas()命令来解决。gets()操作返回key对应的值和值的版本号，然后用cas()操作对该值进行修改，当key对应的版本号与通过gets取得的版本号相同时，则修改key对应的值，否则会失败，这样可以防止并发修改带来的问题。

memcached已经支持CAS模式的原子操作，可以低成本的解决并发控制问题:

具体过程：有线程试图修改当前 key-value 对的 value 时，先由 gets 方法得到 item 的版本号，操作完成提交、数据时，使用cas方法谨慎变更，如果在本地对item操作过程中这个key-value 对在Memcached server端被其它线程更改过，就放弃此次修改（乐观锁）

memcached本身其实不提供分布式解决方案，也即memcached集群节点之前不会互相通信，memcached的分布式主要是在客户端实现，通过客户端的路由处理来达到分布式解决方案的目的，客户端做路由的原理非常简单，应用服务器在每次存取某key的value时，通过某种算法把key映射到某台memcached服务器nodeA上，因此这个key所有操作都在nodeA上.

一种简单的路由策略是根据缓存的key来进行hash，然后根据后端缓存服务节点的个数（比如N），计算hash(key)%N，这样可以将前端请求均衡地映射到后端的某个缓存服务节点。

但这个方法的缺点是：强依赖后端服务节点的数目，当后端服务节点发生宕机（导致节点数减少）或者要进行流量扩容（增加节点）的时候，因为缓存服务节点数目发生变化，而**路由策略又强依赖节点数**，这样会导致大量的数据需要进行rehash（数据迁移），这会导致大量的缓存未命中从而击穿缓存直接访问后端的数据库，可能导致数据库服务不可用，进而导致应用不可用。

该问题的一个解决办法：

1. 在系统访问量的低峰期（比如深夜），进行扩容；
2. 模拟业务请求预热缓存，使缓存服务器中的数据重新分布；

另外，使用一致性hash（consistent hash）算法能够一定程度上改善上面的问题：

memcached客户端采用一致性hash算法作为路由策略，一致性hash算法除了计算key的hash值外，还会计算每个server对应的hash值，然后将这些hash值映射到一个有限的值域上（比如0~2^32）。通过寻找hash值大于hash(key)的最小server作为存储该key数据的目标server,如果找不到，则直接把具有最小hash值的server作为目标server。

上面的方法的一个问题是：根据server节点映射到的0~2^32上值，可能分布是不均匀的，从而导致数据访问的倾斜，大量的key被映射到同一台服务器上

针对上述一致性hash算法带来的数据倾斜问题的解决思路：

- 解决思路1：增加虚拟节点，即对每一个物理节点计算多个hash值，然后把每个hash值都映射到环上的一个节点位置，该节点就是虚拟节点。而key的映射规则不变，只是多了一步从虚拟节点再映射到真实节点的过程；

- 解决思路2：将值域0~2^32划分为n个等分区间（n可以是初始服务节点的数目），每个区间对应将0~2^32进行n等分后的一段值域区间，然后根据hash(key)%2^32，得到该key映射到哪个区间；

### **历史Hash环（扩容节点）**

针对一致性 hash 的扩容，当增加新节点后，它所对应的 key 在原有节点上还会保留一段时间，因此在扩容的过程中，如果对应的 key 还没有迁移到新节点上去，可以先尝试回原节点获取。也即我们**同时维护扩容前和扩容后两个 Hash 环**，在扩容后的 Hash 环上找不到 key 的时候，先转向扩容前的 Hash环寻找，如果能找到就返回对应的值并将该值写入新的节点，找不到时再透过缓存

### **熔断机制（缩容节点）**

缩容后，剩余各个节点上的访问压力都会增加，此时如果某个节点因为压力过大而宕机，就会引起雪崩反应。因此作为兜底方案，应当建立熔断机制来保护服务的稳定性

### **memcached的内存管理机制:固定空间分配**

slab class -> slab -> page -> chunk(从左至右都是1:n关系)

1. 每个slab下有若干page，每个page默认是1M；
2. 每个page下有一组chunk，chunk是真正存放数据的地方，同一个slab里的chunk大小是固定的；
3. 有相同大小chunk的slab被组织在一起，称为slab_class；
4. Memcache中的value过来存放的地方是由value的大小决定的，value总是会被存放到与chunk大小最接近的一个slab中；
5. 数据要放slab的时候，首先要给slab申请内存，**申请内存是以page为单位申请的**，所以在放入第一个数据的时候，无论数据大小多少，都会有1M大小的page被分配给该slab；
6. Memcached的LRU算法不是针对全局的，是针对slab的；

memcached内存管理采取预分配、分组管理的方式，分组管理就是我们上面提到的slab class，按照chunk的大小slab被分为很多种类（slab 包含 chunk）

向memcached添加一个item时候，memcached首先会根据item的大小，来选择最合适的slab class：例如item的大小为190字节，默认情况下class 4的chunk大小为160字节显然不合适，class 5的chunk大小为200字节，大于190字节，因此该item将放在class 5中（显然这里会有10字节的浪费是不可避免的），计算好所要放入的chunk之后，memcached会去检查该类大小的chunk还有没有空闲的，如果没有，将会申请1M（1个slab）的空间并划分为该种类chunk。

基于memcached内存管理策略需要注意的：

对于key-value信息，最好不要超过1m的大小，如果用户数据大于1m，则memcached会将其切割，放到多个chunk内；同时信息长度最好相对是比较均衡稳定的，这样能够保障最大限度的使用内存;

## **redis（c实现）**

学习redis的两大维度和三大主线

![redis-performence-reliability-scalability](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/resources/redis-performence-reliability-scalability.png)

### **Redis为什么高性能**

Redis接收到一个键值对操作后，能以微秒级别的速度找到数据，并快速完成操作。为啥 Redis 能有这么突出的表现呢？

1. redis对网络请求的数据处理，采用的是基于epoll的IO多路复用，6.0之前是单线程处理，6.0之后优化为多线程处理，非常高效。同时对于value数据的访问也是单线程的，避免了多线程多资源竞争锁的开销和上下文切换成本；

2. value有多种不同的数据结构，都很快吗？这要归功于它的底层数据结构。这是因为，键值对是按一定的数据结构来组织的，操作键值对最终就是对数据结构进行增删改查操作，所以高效的数据结构是 Redis 快速处理数据的基础。

    为什么说高效的数据结构是Redis快速处理数据的基础?

    因为redis的每个数据结构对对应有多个底层数据结构来实现，集合数据元素大小和数量较少的情况下，默认采用内存紧凑排列的方式存储（如数组、压缩列表），同时利用CPU高速缓存不会降低访问速度。当数据元素超过设定阈值后，避免查询时间复杂度太高，转为哈希和跳表数据结构存储，保证查询效率，比如

    - Hash：压缩列表（O（n）） -> 字典 （O（1））
    - Set：整数数组（O（n）） -> 字典（O（1））
    - ZSet：压缩列表（O（n）） -> 跳表（Ologn） + 字典（O（1））
    - List：压缩列表（O（n）） -> 双向链表（（O（n）））

    所以，如上转换后的数据结构，只有List无论是转换前的压缩列表O（n）还是转换后的双向链表O（n）的时间复杂度比较高，所以如果**对双向链表做范围查找或随机查找应该尽量避免**。

    但因为压缩列表和双向链表的数据结构，可以快速查找头节点和尾节点的位置以及元素的个数，所以**对头部或尾部进行操作或获取统计值，会是O（1）的操作非常高效**。

3. redis是内存数据库，所有操作都在内存上完成，内存的访问速度本身就很快;

### **数据结构与对象**

redis数据库里的每个键值对（key-value pair）都是由对象（object）组成的，其中数据库键总是一个字符串对象（string object），而数据库键的值则可以是字符串对象、列表对象（list object）、hash对象（hash object）、集合对象（set object）、有序集合对象（sorted set object）这五种对象中的其中一种。

Redis中的每一个值对象都由一个redisObject结构体表示，该结构体中和保存数据有关的三个属性分别是type、encoding和ptr

1. type属性记录了对象的类型（上述五种对象类型其中之一，比如列表对象类型是REDIS_LIST）;

2. ptr属性指向对象的底层实现数据结构，而这些数据结构由对象的encoding属性决定;

3. encoding属性记录了对象所使用的编码，也即这个对象使用了什么数据结构作为对象的底层实现，每种类型的对象都至少使用了两种不同的编码，比如：

    - 字符串对象可以有三种不同的数据结构实现：整数值、embstr编码的简单动态字符串、简单动态字符串。如果一个字符串对象保存的是整数值，并且这个整数值可以用Long类型来表示，那么字符串对象会将整数值保存在字符串对象结构的ptr属性里，并将字符串对象的encoding设置为int。如果字符串对象保存的是一个字符串值，并且这个字符串的长度大于32字节，那么字符串对象将使用一个简单动态字符串（sds）来保存这个字符串值，并将字符串对象的encoding设置为raw。如果字符串对象保存的是一个字符串值，并且这个字符串值的长度小于等于32字节，那么字符串对象将使用embstr编码的方式来保存这个字符串，embstr好处：embstr编码是专门用于保存短字符串的一种优化编码方式，跟正常的字符编码相比，字符编码会调用两次内存分配函数来分别创建redisObject和sdshdr结构，而embstr编码则通过调用一次内存分配函数来分配一块连续的空间，空间中一次包含redisObject和sdshdr两个结构

    - 列表对象可以有两种不同的数据结构实现：压缩列表（ziplist）、双向链表(linkedlist)。如果列表对象保存的所有字符串元素的长度都小于64字节且元素数量小于512个，则使用ziplist
    否则，使用linkedlist

    - hash对象可以使用两种不同的数据结构实现：压缩列表（ziplist）、字典。如果hash对象保存的所有键值对的键和值的字符串长度都小于64字节且键值对数量小于512个，则使用压缩列表，否则使用字典

    - 集合对象可以使用两种不同的数据结构实现：整数数组、字典。集合对象保存的所有元素都是整数值而且保存的元素数量不超过512个，则使用整数集合，否则使用字典

    - 有序集合对象可以使用两种不同的数据结构实现：压缩列表（ziplist）、跳表 + 字典。如果有序集合保存的元素数量小于128个并且有序集合保存的所有元素成员的长度都小于64字节，则使用压缩列表，否则使用跳表+字典

PS：引起redis对象使用的底层数据结构转换的触发条件，都可通过配置文件修改

![redis-bottom-data-struct](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/resources/redis-bottom-data-struct.png)

因此，从上面可以看出，redis底层使用的数据结构包括：简单动态字符串（sds：simple dynamic string）、双向链表、字典（dict）、跳表（skiplist）、整数集合（intset）和压缩列表（ziplist）

Redis在设计时，集合数据元素大小和数量较少的情况下，默认采用内存紧凑排列的方式存储（如数组、压缩列表），同时利用CPU高速缓存不会降低访问速度。当数据元素超过设定阈值后，避免查询时间复杂度太高，转为哈希和跳表数据结构存储，保证查询效率。

底层数据结构实现：

1. 简单动态字符串

    为什么redis中的字符串使用简单动态字符串而不是c字符串？简单动态字符串相比c字符串的好处：
    1. 常数复杂度获取字符串长度（因为数据结构中有len属性可以直接获取长度）
    2. 避免了缓冲区溢出（SDS API会检查SDS的空间是否满足字符串修改后的空间需求）
    3. 减少修改字符串长度时所需的内存重分配次数（因为sds有未分配空间）
    4. 二进制安全（所有sds api都会以处理二进制的方式来处理SDS存放在buf数组里的数据，因此buf被称为字节数组，因此sds除了保存字符串数据，还可以保存二进制数据，比如图像、视频、音频等）；
    5. 兼容部分c字符串函数（因为遵循c字符串以空字符结尾的惯例）

2. 双向链表

3. 字典

4. 压缩列表：是一种为节约内存而开发，是由一系列特殊编码的连续内存块组成的顺序型数据结构，一个压缩列表可以包含任意多个节点（entry），每个节点可以保存一个字节数组或者一个整数值,组成部分：
    - Zlbytes：4字节，记录整个压缩列表占用的内存字节数;
    - Zltail：4字节，记录压缩列表尾节点距离压缩列表的内存起始地址的偏移量（多少字节），通过这个偏移量，程序无需遍历整个压缩列表就可以确定尾节点的位置;
    - Zllen：2字节，记录了压缩列表包含的节点数量（entry数量），当这个部分的值小于65535时，这个值就是真是的节点数，但当这部分的值等于65535的时候，真实的节点数需要遍历整个压缩列表才能计算出;
    - entryX：压缩列表包含的各个节点，节点长度由节点保存的内容决定（不固定）
    - zlend：特殊值0XFF，用于标记压缩列表的末端;

    entryX 组成：
    - previous_entry_length:1字节或5字节，记录了前一个节点的长度，因此程序可以通过指针运算，根据当前节点的起始地址来计算前一个节点的起始地址，通过该原理**可以实现从表尾向表头遍历**;
    - encoding:1字节、2字节或5字节，记录了 content 属性所保存数据的类型以及长度;
    - content:负责保存节点的值，节点值可以是一个字节数组或整数，值的类型和长度由 encoding 属性决定;

    ![ziplist-search](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/resources/ziplist-search.png)

    在压缩列表中，如果我们要查找定位第一个元素和最后一个元素，可以通过表头三个字段的长度直接定位，复杂度是 O(1)。而查找其他元素时，就没有这么高效了，只能逐个查找，此时的复杂度就是 O(N) 了

    压缩列表的问题：

    添加或删除节点可能导致连续多次内存空间重分配操作，称之为『连锁更新』
    发生场景：一个压缩列表中，有多个连续的，长度介于250字节到253字节之间的节点 e1到 eN，因为e1到 eN的所有节点长度都小于254字节，所以记录这些节点的长度只需要1个字节长的 previous_entry_length，但这个时候如果将一个长度大于等于254字节的新节点设置为压缩列表的头节点，那么 e1的 previous_entry_length 将扩展到5字节，程序需要通过对压缩列表执行空间重分配操作。但 e1原先的长度介于250到253字节之间，一旦 previous_entry_length 新增四个字节空间后，e1的长度也变成了介于254字节至257字节之间，会导e2也需要执行空间重分配，这样就会一直连续不断地执行空间重分配操作。

5. 跳表
有序链表只能逐一查找元素，导致操作起来非常缓慢，于是就出现了跳表。具体来说，跳表在链表的基础上，增加了多级索引，通过索引位置的几个跳转，实现数据的快速定位

![skiplist-search](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/resources/skiplist-search.png)

如果我们要在链表中查找 33 这个元素，只能从头开始遍历链表，查找 6 次，直到找到 33 为止。此时，复杂度是 O(N)，查找效率很低。

为了提高查找速度，我们来增加一级索引：从第一个元素开始，每两个元素选一个出来作为索引。这些索引再通过指针指向原始的链表。例如，从前两个元素中抽取元素 1 作为一级索引，从第三、四个元素中抽取元素 11 作为一级索引。此时，我们只需要 4 次查找就能定位到元素 33 了。

如果我们还想再快，可以再增加二级索引：从一级索引中，再抽取部分元素作为二级索引。例如，从一级索引中抽取 1、27、100 作为二级索引，二级索引指向一级索引。这样，我们只需要 3 次查找，就能定位到元素 33 了。

可以看到，这个查找过程就是在多级索引上跳来跳去，最后定位到元素。这也正好符合“跳”表的叫法。当数据量很大时，跳表的查找复杂度就是 O(logN)

为什么有序集合底层数据结构要同时使用跳表+字典，而不是使用其中一个？

理论上，有序集合可以单独使用字典或者跳表的其中一种数据结构来实现，但无论单独使用字典还是跳表，在性能上对比同时使用都有所降低。

举例来说，如果只使用字典来实现有序集合，虽然以O(1)复杂度查找成员分值这一特性得以保留，但因为字典是以无序的方式来保存集合元素，所以每次执行范围型操作（比如ZRank、ZRange等）命令时，程序都需要对字典保存的所有元素进行排序，完成这种排序至少需要O（NlogN）的时间复杂度，以及额外的O(N)内存空间（因为要创建一个数组来保存排序后的元素）。

另一方面，如果我们只使用跳表来实现有序集合，那么跳表执行范围型操作的性能优点得以保留，但因为没有了字典，所以根据成员查分值这个操作的复杂度从O(1)上升为O(logN)

所以，字典用来存储元素值和对应的分值，跳表用来维护元素对象（也包括了元素值和分值）的顺序。

The elements are added to a hash table mapping Redis objects to scores. At the same time the elements are added to a skip list mapping scores to Redis objects.
zset有个ZSCORE的操作，用于返回单个集合member的分数，它的操作复杂度是O(1)，这就是收益于这个hash table。这个hash table保存了集合元素和相应的分数，所以做ZSCORE操作时，直接查这个表就可以，复杂度就降为O(1)了。而跳表主要服务范围操作，提供O(logN)的复杂度

### **键和值用什么数据结构组织**

为了实现从键到值的快速访问，Redis 使用了一个哈希表来保存所有键值对。一个哈希表，其实就是一个数组，数组的每个元素称为一个哈希桶。所以，我们常说，一个哈希表是由多个哈希桶组成的，每个哈希桶中保存了键值对数据。哈希桶中的元素保存的并不是值本身，而是指向具体值的指针。这也就是说，不管值是 String，还是集合类型，哈希桶中的元素都是指向它们的指针

![global-hash-table](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/resources/global-hash-table.png)

因为这个哈希表保存了所有的键值对，所以，我也把它称为**全局哈希表**。哈希表的最大好处很明显，就是让我们可以用 O(1) 的时间复杂度来快速查找到键值对——我们只需要计算键的哈希值，就可以知道它所对应的哈希桶位置，然后就可以访问相应的 entry 元素

但是，当你往 Redis 中写入大量数据后，就可能发现操作有时候会突然变慢了。这其实是因为你忽略了一个潜在的风险点，那就是哈希表的冲突问题和 rehash 可能带来的操作阻塞

#### **哈希冲突问题**

当你往哈希表中写入更多数据时，哈希冲突是不可避免的问题。这里的哈希冲突，也就是指，两个 key 的哈希值和哈希桶计算对应关系时，正好落在了同一个哈希桶中。毕竟，哈希桶的个数通常要少于 key 的数量，这也就是说，难免会有一些 key 的哈希值对应到了同一个哈希桶中。Redis 解决哈希冲突的方式，就是链式哈希。链式哈希也很容易理解，就是指同一个哈希桶中的多个元素用一个链表来保存，它们之间依次用指针连接

哈希冲突链上的元素只能通过指针逐一查找再操作。如果哈希表里写入的数据越来越多，哈希冲突可能也会越来越多，这就会导致某些哈希冲突链过长，进而导致这个链上的元素查找耗时长，效率降低。对于追求“快”的 Redis 来说，这是不太能接受的。所以，Redis 会对哈希表做 rehash 操作。rehash 也就是增加现有的哈希桶数量，让逐渐增多的 entry 元素能在更多的桶之间分散保存，减少单个桶中的元素数量，从而减少单个桶中的冲突。

为了使 rehash 操作更高效，Redis 默认使用了两个全局哈希表：哈希表 1 和哈希表 2。一开始，当你刚插入数据时，默认使用哈希表 1，此时的哈希表 2 并没有被分配空间。随着数据逐步增多，Redis 开始执行 rehash，这个过程分为三步：

1. 给哈希表 2 分配更大的空间，例如是当前哈希表 1 大小的两倍；
2. 把哈希表 1 中的数据重新映射并拷贝到哈希表 2 中；
3. 释放哈希表 1 的空间。
到此，我们就可以从哈希表 1 切换到哈希表 2，用增大的哈希表 2 保存更多数据，而原来的哈希表 1 留作下一次 rehash 扩容备用。

但是第二步涉及大量的数据拷贝，如果一次性把哈希表 1 中的数据都迁移完，会造成 Redis 线程阻塞，无法服务其他请求。此时，Redis 就无法快速访问数据了。为了避免这个问题，Redis 采用了渐进式 rehash

在第二步拷贝数据时，Redis 仍然正常处理客户端请求，渐进式 rehash包括如下两种方式:

1. 每处理一个请求时，从哈希表 1 中的第一个索引位置开始，顺带着将这个索引位置上的所有 entries 拷贝到哈希表 2 中；等处理下一个请求时，再顺带拷贝哈希表 1 中的下一个索引位置的 entries；

2. 除了根据键值对的操作来进行数据迁移，Redis本身还会有一个定时任务在执行rehash，如果没有键值对操作时，这个定时任务会周期性地（例如每100ms一次）搬移一些数据到新的哈希表中，这样可以缩短整个rehash的过程；

如下图所示：

![rehash-process](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/resources/rehash-process.png)

因为在进行渐进式 rehash 的过程中， 字典会同时使用 ht[0] 和 ht[1] 两个哈希表， 所以在渐进式 rehash 进行期间

1. 字典的删除（delete）、查找（find）、更新（update）等操作会在两个哈希表上进行： 比如说， 要在字典里面查找一个键的话， 程序会先在 ht[0] 里面进行查找， 如果没找到的话， 就会继续到 ht[1] 里面进行查找，诸如此类；
2. 在渐进式 rehash 执行期间， 新添加到字典的键值对一律会被保存到 ht[1] 里面， 而 ht[0] 则不再进行任何添加操作：这一措施保证了 ht[0] 包含的键值对数量会只减不增， 并随着 rehash 操作的执行而最终变成空表；

不同底层数据结构的查找时间复杂度

![bottom-data-struct-time-complexity](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/resources/bottom-data-struct-time-complexity.png)

不同操作的时间复杂度

1. 单元素操作：是指每一种集合类型对单个数据实现的增删改查操作。例如，Hash 类型的 HGET、HSET 和 HDEL，Set 类型的 SADD、SREM、SRANDMEMBER 等。这些操作的复杂度由集合采用的数据结构决定，例如，HGET、HSET 和 HDEL 是对哈希表做操作，所以它们的复杂度都是 O(1)；Set 类型用哈希表作为底层数据结构时，它的 SADD、SREM、SRANDMEMBER 复杂度也是 O(1)。这里，有个地方你需要注意一下，集合类型支持同时对多个元素进行增删改查，例如 Hash 类型的 HMGET 和 HMSET，Set 类型的 SADD 也支持同时增加多个元素。此时，这些操作的复杂度，就是由单个元素操作复杂度和元素个数决定的。例如，HMSET 增加 M 个元素时，复杂度就从 O(1) 变成 O(M) 了;

2. 范围操作：是指集合类型中的遍历操作，可以返回集合中的所有数据，比如 Hash 类型的 HGETALL 和 Set 类型的 SMEMBERS，或者返回一个范围内的部分数据，比如 List 类型的 LRANGE 和 ZSet 类型的 ZRANGE。这类操作的复杂度一般是 O(N)，比较耗时，我们应该尽量避免。不过，Redis 从 2.8 版本开始提供了 SCAN 系列操作（包括 HSCAN，SSCAN 和 ZSCAN），这类操作实现了渐进式遍历，每次只返回有限数量的数据。这样一来，相比于 HGETALL、SMEMBERS 这类操作来说，就避免了一次性返回所有元素而导致的 Redis 阻塞；

3. 统计操作：是指集合类型对集合中所有元素个数的记录，例如 LLEN 和 SCARD。这类操作复杂度只有 O(1)，这是因为当集合类型采用压缩列表、双向链表、整数数组这些数据结构时，这些结构中专门记录了元素的个数统计，因此可以高效地完成相关操作；

4. 例外情况：是指某些数据结构的特殊记录，例如压缩列表和双向链表都会记录表头和表尾的偏移量。这样一来，对于 List 类型的 LPOP、RPOP、LPUSH、RPUSH 这四个操作来说，它们是在列表的头尾增删元素，这就可以通过偏移量直接定位，所以它们的复杂度也只有 O(1)，可以实现快速操作；

Redis 之所以能快速操作键值对，一方面是因为 O(1) 复杂度的哈希表被广泛使用，包括 String、Hash 和 Set，它们的操作复杂度基本由哈希表决定，另一方面，Sorted Set 也采用了 O(logN) 复杂度的跳表。不过，集合类型的范围操作，因为要遍历底层数据结构，复杂度通常是 O(N)
当然，我们不能忘了复杂度较高的 List 类型，它的两种底层实现结构：双向链表和压缩列表的操作复杂度都是 O(N)。因此，我的建议是：因地制宜地使用 List 类型。例如，既然它的 POP/PUSH 效率很高，那么就将它主要用于 FIFO 队列场景，而不是作为一个可以随机读写的集合

### **Redis线程模型**

Redis 是单线程，主要是指 Redis 的网络 IO 和键值对读写是由一个线程来完成的，这也是 Redis 对外提供键值存储服务的主要流程。但 Redis 的其他功能，比如持久化（RDB的bgsave、AOF的everysec策略）、异步删除、集群数据同步等，其实是由额外的线程执行的。

Redis 却能使用单线程模型达到每秒数十万级别的处理能力，一方面，Redis 的大部分操作在内存上完成，再加上它采用了高效的数据结构，例如哈希表和跳表，这是它实现高性能的一个重要原因。另一方面，就是 Redis 采用了IO多路复用机制，使其在网络 IO 操作中能并发处理大量的客户端请求，实现高吞吐率

#### **Redis6.0之前版本的单线程模型**

Redis在处理客户端的请求时，包括获取 (socket 读)、解析、执行、内容返回 (socket 写) 等都由一个顺序串行的主线程处理，这就是所谓的“单线程”

Redis6.0之前版本的单线程模型为什么快？

1. 纯内存操作，时间复杂度低;
2. 单线程操作避免了频繁的线程上下文切换；
3. IO多路复用机制（通过 select、poll 、epoll 等技术实现）

#### **Redis6.0版本之后的多线程模型**

在 redis 多线程模型下，包括主线程和IO线程，所谓多线程是指 IO线程并发地处理网络数据的读写和协议解析（读socke 和 回写 socket），主线程在 IO 线程读写 socket 的时候是阻塞的，在 IO线程完成 socket 读后，仍然是顺序串行的执行命令。因此，Redis6.0的多线程模型下，也不会出现线程安全问题。在多线程模型下：

1. IO线程要么同时在读 socket，要么同时在写，不会同时读或写；
2. IO线程只负责读写socket和解析命令，不负责命令执行；

![redis-6-thread-model](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/resources/redis-6-thread-model.png)

Redis6.0引入的多线程IO特性对性能的提升至少是一倍以上

#### **Redis6.0之前为什么一直不使用多线程？**

使用Redis时，几乎不存在CPU成为瓶颈的情况， Redis瓶颈主要受限于内存和网络。如果应用程序主要使用O(N)或O(log(N))的命令，它几乎不会占用太多CPU。使用了单线程可维护性高。多线程模型虽然在某些方面表现优异，但是它却引入了程序执行顺序的不确定性，带来了并发读写的一系列问题，增加了系统复杂度、同时可能存在线程上下文切换、甚至加锁解锁、死锁造成的性能损耗

Redis单线程模型下的性能瓶颈主要包括2个方面：

1. 任意一个请求在server中一旦发生耗时，都会影响整个server的性能，也就是说后面的请求都要等前面这个耗时请求处理完成，自己才能被处理到。耗时的操作包括以下几种：
    - 操作bigkey：写入一个bigkey在分配内存时需要消耗更多的时间，同样，删除bigkey释放内存同样会产生耗时；如果下面两种情况，我就会认为它是bigkey。
        - 字符串类型：它的big体现在单个value值很大，一般认为超过10KB就是bigkey
        - 非字符串类型：哈希、列表、集合、有序集合，它们的big体现在元素个数太多
    - 使用复杂度过高的命令：例如SORT/SUNION/ZUNIONSTORE，或者O(N)命令，但是N很大，例如lrange key 0 -1一次查询全量数据；
    - 大量key集中过期：Redis的过期机制也是在主线程中执行的，大量key集中过期会导致处理一个请求时，耗时都在删除过期key，耗时变长；
    - 淘汰策略：淘汰策略也是在主线程执行的，当内存超过Redis内存上限后，每次写入都需要淘汰一些key，也会造成耗时变长；
    - AOF刷盘开启always机制：每次写入都需要把这个操作刷到磁盘，写磁盘的速度远比写内存慢，会拖慢Redis的性能；
    - 主从全量同步生成RDB：虽然采用fork子进程生成数据快照，但fork这一瞬间也是会阻塞整个线程的，实例越大，阻塞时间越久；
2. 并发量非常大时，单线程读写客户端IO数据存在性能瓶颈，虽然采用IO多路复用机制，但是读写客户端数据依旧是同步IO，只能单线程依次读取客户端的数据，无法利用到CPU多核;

针对问题1：一方面需要业务人员去规避，一方面Redis在4.0推出了lazy-free机制，把bigkey释放内存的耗时操作放在了异步线程中执行，降低对主线程的影响

针对问题2：Redis在6.0推出了多线程，可以在高并发场景下利用CPU多核多线程读写客户端数据，进一步提升server性能，当然，只是针对客户端的读写是并行的，每个命令的真正操作依旧是单线程的

### **单机数据库**

redis服务端组成：redisServer-》redisdb（多个）-》dict(键空间)
PS：redis集群的节点也是一个redis服务器，但集群节点和单机服务器在数据库方面的一个区别是，集群节点只能使用0号数据库，而单机redis服务器则没有这个限制

### **Redis持久化**

Redis 的持久化主要有两大机制，即 AOF（Append Only File）日志和 RDB 快照，作用用于重启后的数据恢复，Redis 4.0 之后新增了混合持久化的方式

1. RDB（Redis Database）：定时快照（snapshot）,redis默认的持久化方式，默认保存的文件名为dump.rdb，而在Redis服务器启动时，会重新加载dump.rdb文件的数据到内存当中恢复数据。在Redis内部一个定时器事件，每隔固定时间去检查当前数据发生的改变次数与时间是否满足配置的持久化触发的条件，如果满足则通过操作系统fork调用来创建出一个子进程，这个子进程默认会与父进程共享相同的地址空间，这时就可以通过子进程来遍历整个内存来进行存储操作，而主进程则仍然可以提供服务，当有写入时由操作系统按照内存页（page）为单位来进行copy-on-write保证父子进程之间不会互相影响。它的缺点是快照只是代表一段时间内的内存映像，所以系统重启会丢失上次快照与重启之间所有的数据;

    RDB持久化有三种方式：

    1. save命令：是一个同步操作，当客户端向服务器发送save命令请求进行持久化时，服务器会阻塞save命令之后的其他客户端的请求，直到数据同步完成。如果数据量太大，同步数据会执行很久，而这期间Redis服务器也无法接收其他请求，所以，最好不要在生产环境使用save命令；

    2. bgsave命令：与save命令不同，bgsave命令是一个异步操作，当客户端发服务发出bgsave命令时，Redis服务器主进程会fork一个子进程来数据同步问题，在将数据保存到rdb文件之后，子进程会退出。所以，与save命令相比，Redis服务器在处理bgsave采用子线程进行IO写入，而主进程仍然可以接收其他请求，但fork子进程这个操作是同步的，所以fork子进程时，一样不能接收其他请求，这意味着，如果fork一个子进程花费的时间太久(一般是很快的)，bgsave命令仍然有阻塞其他客户的请求的情况发生；

    3. 自动触发：除了通过客户端发送命令外，还有一种方式，就是在Redis配置文件中的save指定到达触发RDB持久化的条件，比如【多少秒内至少达到多少写操作】就开启RDB数据同步，Redis 支持用户通过设置服务器配置save 选项（在 redis.conf 中） ，让服务器每间隔一段时间自动执行一次 BGSAVE命令；

    RDB的几个优点

    - 与AOF方式相比，通过rdb文件恢复数据比较快；
    - rdb文件非常紧凑，适合于数据备份；
    - 通过RDB进行数据备，由于使用子进程生成，所以对Redis服务器性能影响较小；

    RDB的几个缺点

    - 如果服务器宕机的话，采用RDB的方式会造成某个时段内数据的丢失，比如我们设置10分钟同步一次或5分钟达到1000次写入就同步一次，那么如果还没达到触发条件服务器就死机了，那么这个时间段的数据会丢失；

    - 使用save命令会造成服务器阻塞，直接数据同步完成才能接收后续请求；

    - 使用bgsave命令在fork子进程时，如果数据量太大，fork的过程也会发生阻塞，另外，fork子进程会耗费内存；

2. AOF（Append Only File）：基于语句追加文件的方式，类似MySQl的基于语句的binlog方式，即每条会使Redis内存数据发生改变的命令都会追加到一个log文件中，也就是说这个log文件就是Redis的持久化数据.主要缺点是追加log文件可能导致体积过大，当系统重启恢复数据时如果是aof的方式则加载数据会非常慢，几十G的数据可能需要几小时才能加载完，当然这个耗时并不是因为磁盘文件读取速度慢，而是由于读取的所有命令都要在内存中执行一遍。另外由于每条命令都要写log，所以使用aof的方式，Redis的读写性能也会有所下降;

    AOF日志写入是在Redis成功执行命令之后才进行的，所谓的写后日志，为什么这样做有两个原因：
    1. 如果我们不小心输错了Redis指令，然后Redis紧接着将该指令保存到了AOF文件中，等到Redis进行数据恢复的时候就可能导致错误。因此这种写后日志的形式可以避免对指令进行语法检查，避免出现记录错误指令的情况；
    2. 先执行命令后保存日志，不会阻塞当前的写操作；

    但是，AOF写后日志也有两个风险：
    1. 第一个风险，假如Redis写操作成功之后突然宕机，此时AOF日志还未来得及写入，则该条指令和相关参数就有丢失的风险；
    2. 第二个风险，AOF虽然避免了对当前操作的阻塞，但是有可能阻塞下一个操作。因为保存AOF日志的部分工作也是由主线程完成的，Redis的内存操作速度和文件写入速度简直是云泥之别，如果主线程在文件保存的过程中花费太长的时间必然会阻塞后续的操作；

    Redis中提供了3种AOF同步策略
    1. 每秒同步(everysec)：默认策略，每个写命令执行完，只是先把日志写到 AOF文件的内核缓冲区，理论上每隔1秒把缓冲区中的内容同步到磁盘，且同步操作有单独的子线程进行，因此不会阻塞主线程，存在的问题是一旦系统出现宕机现象，那么这一秒钟之内修改的数据将会丢失；
    2. 每修改同步（always）：每修改同步，我们可以将其视为同步持久化，每个写命令执行完，立刻同步地将日志写回磁盘。此模式下同步操作是由 Redis主线程执行的，所以在同步执行期间，主线程会被阻塞，不能接受命令请求；
    3. 不同步：同步时机由内核决定；

    Everysec在性能方面要优于Always ，并且在通常情况下，这种模式最多丢失不多于2秒的数据， 所以它的安全性要高于No ，这是一种兼顾性能和安全性的保存方案。

    Always的安全性是最高的，但性能也是最差的，因为Redis主线程必须阻塞直到命令信息被写入并同步到磁盘之后才能继续处理请求

    AOF和RDB可以同时开启，在这种情况下当redis重启的时候优先加载AOF文件来恢复原始数据，因为在通常情况下AOF文件保存的数据要比RDB文件保存的数据集要完整。RDB的数据不实时，同时使用两者时服务器重启也只会找AOF文件。

    那要不要只使用AOF呢？建议不要，因为RDB更适合备分数据库(AOF在不断变换不好备分)快速重启，而且不会有AOF可能潜在的Bug，留着做一个万一的手段

3. 混合持久化：在开启混合持久化的情况下，AOF 重写时会把 Redis 的持久化数据，以 RDB 的格式写入到 AOF 文件的开头，之后的数据再以 AOF 的格式化追加的文件的末尾。混合持久化的数据存储结构如下图所示：

![mix-persistence-data-struct](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/resources/mix-persistence-data-struct.png)

    混合持久化优点：

    - 混合持久化结合了 RDB 和 AOF 持久化的优点，开头为 RDB 的格式，使得 Redis 可以更快的启动，同时结合 AOF 的优点，又减低了大量数据丢失的风险；
    混合持久化缺点：
    - AOF 文件中添加了 RDB 格式的内容，使得 AOF 文件的可读性变得很差；
    - 兼容性差，如果开启混合持久化，那么此混合持久化 AOF 文件，就不能用在 Redis 4.0 之前版本了；

如何选择合适的持久化方式

1. 如果数据不是那么敏感，可以从其他地方重新生成，那么可以考虑关闭持久化；
2. 如果数据比较重要，且可以承受数分钟的数据丢失，那么可以只使用RDB；
3. 如果数据比较重要，且不可接受数据丢失，建议是RDB和AOF都开启，RDB更适合做数据的备份，AOF可以尽量保证数据的不丢失；

### **Redis主从复制**

在redis中，用户可以通过执行SLAVEOF命令或者设置slafeof选项，让一个服务器去复制（replicate）另一个服务器，被复制的服务器为主服务器（master），而对主服务器进行复制的服务器则被称为从服务器（slave）

redis的复制功能分为同步（sync）和命令传播（command propagate）两个操作

1. 同步操作（sync）用于将从服务器的数据库状态更新至主服务器所处的数据库状态（类似全量同步）；

2. 命令传播操作则用于在主服务器的数据库状态被修改，导致主从不一致时，让主从服务器的数据库重新回到一致状态；

SYNC命令执行过程：

1. 从服务器向主服务器发送SYNC命令；
2. 收到SYNC命令的主服务器执行BGSAVE命令，在后台生成一个RDB文件，并使用一个缓冲区开始记录从现在开始执行的所有写命令；
3. 主服务器将生成好的RDB文件发送给从服务器，从服务器接受并载入这个RDB文件，将自己的数据库状态更新至主服务器执行BGSAVE命令时的状态；
4. 主服务器将记录在缓冲区里的所有写命令发送给从服务器，从服务器执行这些写命令，将自己的状态更新至主服务器当前最新的状态，如果从节点加载RDB花费时间过长，将导致缓冲区溢出，会导致全量同步失败；

在redis中，从服务器（slave）对主服务器（master）的复制可以分为以下两种情况：

1. 初次复制：从服务器没有复制过任何主服务器，或者从服务器当前要复制的主服务器和上一次复制的主服务器不同；
2. 断线后重复制：处于命令传播阶段的主从服务器因为网络原因而中断了复制，但从服务器通过自动重连接重新连接上了主服务器，并继续复制主服务器；

对于断线后复制的情况，redis的旧版复制功能仍然使用SYNC命令完全重新同步数据，造成性能比较低效，新版的复制功能改为PSYNC进行部分重同步，只同步断线期间执行的写命令

### **异步复制**

主节点不但负责数据读写，还负责把写命令同步给从节点，写命令的发送过程是异步完成，也就是说主节点处理完写命令后立即返回客户端，并不等待从节点复制完成。

异步复制的步骤很简单，如下：

![redis-async-replication](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/resources/redis-async-replication.png)

主节点接受处理命令。主节点处理完后返回响应结果 。对于修改命令，异步发送给从节点，从节点在主线程中执行复制的命令

redis的主从数据复制会导致在某一个中间时刻主从服务器的数据出于不一致的状态（异步复制，不是强一致性）

### **Redis的过期策略和内存淘汰策略**

过期策略通常有以下三种：

- 定时过期：每个设置过期时间的key都需要创建一个定时器，到过期时间就会立即清除。该策略可以立即清除过期的数据，对内存很友好；但是会占用大量的CPU资源去处理过期的数据，从而影响缓存的响应时间和吞吐量;

- 惰性过期：只有当访问一个key时，才会判断该key是否已过期，过期则清除。该策略可以最大化地节省CPU资源，却对内存非常不友好。极端情况可能出现大量的过期key没有再次被访问，从而不会被清除，占用大量内存;

- 定期过期：每隔一定的时间，会随机扫描一定数量的数据库的expires字典中一定数量的key，并清除其中已过期的key。该策略是前两者的一个折中方案。通过调整定时扫描的时间间隔和每次扫描的限定耗时，可以在不同情况下使得CPU和内存资源达到最优的平衡效果;

expires字典会保存所有设置了过期时间的key的过期时间数据，其中，key是指向键空间中的某个键的指针，value是该键的毫秒精度的UNIX时间戳表示的过期时间。键空间是指该Redis集群中保存的所有键。

Redis中同时使用了惰性过期和定期过期两种过期策略

Redis的内存淘汰策略是指在Redis的用于缓存的内存不足时，怎么处理需要新写入且需要申请额外空间的数据

内存淘汰策略有如下几种:

1. noeviction：添加数据时，如果redis判断该操作会导致占用内存大小超过内存限制，就返回error，然后啥也不干
2. allkeys-lru：添加数据时，如果redis判断该操作会导致占用内存大小超过内存限制，就会扫描所有的key，淘汰一些最近未使用的key
3. volatile-lru：添加数据时，如果redis判断该操作会导致占用内存大小超过内存限制，扫描那些设置了过期时间的key，淘汰一些最近未使用的key
4. allkeys-random：添加数据时，如果redis判断该操作会导致占用内存大小超过内存限制，就会扫描所有的key，随机淘汰一些key
5. volatile-random：添加数据时，如果redis判断该操作会导致占用内存大小超过内存限制，扫描那些设置了过期时间的key，随机淘汰一些key
6. volatile-ttl：添加数据时，如果redis判断该操作会导致占用内存大小超过内存限制，扫描那些设置了过期时间的key，淘汰一些即将过期的key
7. volatile-lfu:添加数据时，如果redis判断该操作会导致占用内存大小超过内存限制，就会淘汰一些设置了过期时间的，并且最少使用的key
8. allkeys-lfu：添加数据时，如果redis判断该操作会导致占用内存大小超过内存限制，就会扫描所有的key，淘汰一些最少使用的key

我们在选择使用淘汰策略的时候可以根据访问key的方式来选择不同的淘汰策略

1. 当我们redis中的key基本上都有用到，也就是说每个key都有周期性访问到，那就可以选择使用random策略
2. 当我们redis中的key部分是我们经常访问的，部分是非经常访问的就可以考虑使用LRU和LFU策略
3. 当我们想根据时间长久淘汰超时数据时，就选用ttl
4. 我们根据我们的需要是否有要长久保存的key来选择volatile或者是all，如果有需要长久保存的key，则使用volatile，否则可以使用all全表扫描

Redis的内存淘汰策略的选取并不会影响过期的key的处理。内存淘汰策略用于处理内存不足时的需要申请额外空间的数据，过期策略用于处理过期的缓存数据

### **redis的lru和lfu算法的实现原理**

#### **Redis实现近似LRU算法**

若严格按LRU实现，假设Redis保存的数据较多，需要在代码中实现：

- 为Redis使用最大内存时，可容纳的所有数据维护一个链表，需额外内存空间来保存链表数据；

- 每当有新数据插入或现有数据被再次访问，需执行多次链表操作，在访问数据的过程中，让Redis受到数据移动和链表操作的开销影响，将导致降低Redis访问性能

因此，无论是为节省内存还是保持Redis高性能的考虑，Redis并未严格按LRU基本原理实现，而是提供了一个近似LRU算法实现，其策略是：

1. 并不需要一个完全准确的LRU算法，就算移除了一个最近访问过的Key，其实影响也不太大；
2. 最初Redis的实现方式：随机选三个Key，把idle time最大的那个Key移除。后来，把3改成可配置的一个参数，默认为N=5：maxmemory-samples=5；
3. 上述方法的缺点是：每次随机选择的时候，并没有利用历史信息。在每一轮移除(evict)一个Key时，随机从N个里面选一个Key，移除idle time最大的那个Key；下一轮又是随机从N个里面选一个Key，但是在上一轮移除Key的过程中，其实是知道了N个Key的idle time的情况的，这样在下一轮移除Key时，可以利用好上一轮知晓的一些信息；
4. Redis又做出了改进：采用缓冲池(pooling，EvictionPoolLRU)。当每一轮移除Key时，拿到了这个N个Key的idle time，如果它的idle time比 pool 里面的 Key的idle time还要大，就把它添加到pool里面去。这样一来，每次移除的Key并不仅仅是随机选择的N个Key里面最大的，而且还是pool里面idle time最大的，并且pool 里面的Key是经过多轮比较筛选的，它的idle time 在概率上比随机获取的Key的idle time要大，可以这么理解：pool 里面的Key 保留了"历史经验信息”；
5. 采用"pool"，把一个全局排序问题转化成为了局部的比较问题。要想知道idle time 最大的key，精确的LRU需要对全局的key的idle time排序，然后就能找出idle time最大的key了。但是可以采用一种近似的思想，即随机采样(samping)若干个key，这若干个key就代表着全局的key，把samping得到的key放到pool里面，每次采样之后更新pool，使得pool里面总是保存着随机选择过的key的idle time最大的那些key。需要evict key时，直接从pool里面取出idle time最大的key，将之evict掉；

根据LRU算法的基本原理，如果严格按基本原理实现LRU算法，则需要额外内存空间保存LRU链表，系统运行时也会受到LRU链表操作的开销影响。而Redis的内存资源和性能都很重要，所以Redis实现近似LRU算法：

- 首先是设置了全局LRU时钟，并在KV对创建时获取全局LRU时钟值作为访问时间戳，及在每次访问时获取全局LRU时钟值，更新访问时间戳
- 然后，当Redis每处理一个命令，都调用performEvictions判断是否需释放内存。若已使用内存超出maxmemory，则随机选择一些KV对，组成待淘汰候选集合，并根据它们的访问时间戳，选出最旧数据淘汰
近似LRU算法并未使用耗时且耗空间的链表，而使用固定大小的待淘汰数据集合，每次随机选择一些Key加入待淘汰数据集合。最后，按待淘汰集合中K的空闲时间长度，删除空闲时间最长的Key

#### **Redis实现LFU算法**

LFU一般会有这样一些问题：

1. 短时热点数据很难被淘汰问题：比如微博热点数据一般只是几天内有较高的访问频次，过了这段时间就没那么大意义去缓存了。但是因为在热点期间他的频次被刷上去了，之后很长一段时间内很难被淘汰；
2. 新缓存数据容易被淘汰问题：如果采用只记录缓存中的数据的访问信息，新加入的高频访问数据在刚加入的时候由于没有累积优势，很容易被淘汰掉；
3. 内存资源占用问题：如果记录全部出现过的数据的访问信息，会占用更多的内存空间；

Redis记录访问次数使用了一种近似计数算法——Morris算法。Morris算法利用随机算法来增加计数，在Morris算法中，计数不是真实的计数，它代表的是实际计数的量级

Redis中实现LFU算法的时候，有这个两个重要的可配置参数：

- server.lfu_log_factor : 能够影响计数的量级范围，即下表中的factor参数；
- server.lfu_decay_time: 控制LFU计数衰减的参数

**访问计数**

Redis的LFU实现使用了近似计数算法，这个算法的特点是能够用一个较小的数表示一个很大的量级，所以对于Redis来说统计频次不需要太多空间和内容，只需要一个不那么大的数就行（这个特性解决了前面说的LFU的常见问题3）。Redis的LRU算法实现里，用了一个24位的redisObject->lru字段，拿到LFU中正好合用。Redis没有全部用掉这24位，只拿了其中8位用来做计数，剩下的16位另作别用：

- 16 bits      8 bits
- +----------------+--------+

- | Last decr time | LOG_C  |

- +----------------+--------+

8个bit位最大为255，从Redis文档中贴出来的数据（如下表）可以看到，不同的factor的值能够控制计数代表的量级的范围，当factor为100时，能够最大代表10M，也就是千万级别的命中数：

![morris](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/resources/morris.png)

第一列的factor，就是前头说的配置项server.lfu_log_factor，那么这个配置的作用就比较明显了，就是用来控制概率衰减的速率

Redis中在创建新对象的时候，它的LFU计数值不是从0开始的，这个在createObject（object.c文件中）方法中能看到。当使用LFU的时候，它的lru值是这样初始化的：

```c
o->lru = (LFUGetTimeInMinutes()<<8) | LFU_INIT_VAL;
```

初始计数值会直接就是LFU_INIT_VAL，也就是5，这点就是为了解决前头说到的LFU算法的第常见问题2：新增的缓存可能还没开始累积优势就被淘汰了。给新增缓存一个5的计数值，那么至少能保证新增缓存比真正冷（计数值低于5）的数据要晚淘汰。

可是初始计数值就是从5开始的，为什么会有出现计数值低于5的数据呢？这是Redis为了解决LFU解决前头说的常见问题1引入的手段 – 计数衰减。
计数衰减

在缓存被访问时，会更新数据的访问计数，更新的步骤是：先在现有数据的计数上进行计数衰减，再对完成衰减后的计数进行增加。

Redis是使用原来用于LRU的24位的redisObject->lru中的8位来进行计数的，剩下的16位另作它用 – 用于计数值的衰减。开头说的Redis配置项 server.lfu_decay_time，也是用于控制计数的衰减的。

server.lfu_decay_time 的代表的含义是计数衰减的周期长度，单位是分钟。

当时间每过去一个衰减周期，计数值就会减1。衰减周期的计算就是redisObject->lru中那16位，它记录的就是上次进行衰减的时间。

衰减周期数就等于从上次衰减到现在经过的时间除以衰减周期长度 server.lfu_decay_time

```c
unsigned long num_periods = server.lfu_decay_time ? LFUTimeElapsed(ldt) / server.lfu_decay_time : 0;
```

代码逻辑如下：
```c
/* Return the current time in minutes, just taking the least significant

- 16 bits. The returned time is suitable to be stored as LDT (last decrement
- time) for the LFU implementation. */
unsigned long LFUGetTimeInMinutes(void) {
    return (server.unixtime/60) & 65535;
}

/* Given an object last access time, compute the minimum number of minutes

- that elapsed since the last access. Handle overflow (ldt greater than
- the current 16 bits minutes time) considering the time as wrapping
- exactly once. */
unsigned long LFUTimeElapsed(unsigned long ldt) {
    unsigned long now = LFUGetTimeInMinutes();
    if (now >= ldt) return now-ldt;
    return 65535-ldt+now;
}

/* If the object decrement time is reached decrement the LFU counter but

- do not update LFU fields of the object, we update the access time
- and counter in an explicit way when the object is really accessed.
- And we will times halve the counter according to the times of
- elapsed time than server.lfu_decay_time.
- Return the object frequency counter.

*

- This function is used in order to scan the dataset for the best object
- to fit: as we check for the candidate, we incrementally decrement the
- counter of the scanned objects if needed. */
unsigned long LFUDecrAndReturn(robj*o) {
    unsigned long ldt = o->lru >> 8;
    unsigned long counter = o->lru & 255;
    unsigned long num_periods = server.lfu_decay_time ? LFUTimeElapsed(ldt) / server.lfu_decay_time : 0;
    if (num_periods)
        counter = (num_periods > counter) ? 0 : counter - num_periods;
    return counter;
}
```

因此，即使碰到微博热点数据短时间大量访问导致访问计数被刷上去的情况，如果热点过去一段时间该数据没有被访问，redis的LFU算法会通过计数衰减逐渐把数据的计数给衰减下去。

**缓存淘汰的执行**

使用LFU方式进行缓存缓存淘汰，其实和使用LRU方式的执行过程基本完全一致，只是把idle换成了 255 - counter

### **Redis的事务**

Redis的事务本质上是通过MULTI、EXEC、WATCH等一组命令的集合，事务支持一次执行多个命令，在事务执行的过程中，会按照顺序串行化执行队列中的命令，其他客户端提交的命令请求不会插入到事务执行命令序列中。

Redis 事务的四个基础命令
MULTI，告诉 Redis 服务器开启一个事务。注意，只是开启，而不是执行 。
EXEC，告诉 Redis 开始执行事务 。
DISCARD，告诉 Redis 取消事务。
WATCH，监视某一个键值对，它的作用是在事务执行之前如果监视的键值被修改，事务会被取消
Redis 可以借助 WATCH/MULTI 命令来实现 CAS 操作

Redis事务的三个阶段
1.事务开始MULTI
2.命令入队
3.事务执行EXEC
事务执行过程中，如果服务端收到EXEC、DISCARD、WATCH、MULTI之外的请求，将会把请求放入队列中排队

Redis事务的ACID支持
原子性：通过multi和exec命令支持事务，命令要么都执行要么都不执行，但是不支持事务的回滚，比如中间有某个命令执行出错，不会回滚，仍然会继续执行后续的命令，因此不支持原子性
一致性：支持
隔离性：redis是单线程执行操作，它保证在执行事务时，不会对事务进行中断，事务可以运行直到执行完所有事务队列中的命令为止。因此，Redis 的事务是总是带有隔离性的
持久性：当开启持久化模式时（AOF模式且开启always策略），也支持持久性

高可用方案（Sentinel）
Sentinel（哨兵）是redis Master-Slave模式下的高可用性解决方案，由一个或多个Sentinel实例（Sentinel只是一个运行在特殊模式下的redis服务器）组成的Sentinel系统可以监视任意多个主服务器，以及这些主服务器下的所有从服务器，并在监视的主服务器下线时，自动将下线主服务器下的某个从服务器升级为新的主服务器，同时还会继续监视已下线的主服务器，并在它重新上线时，将它设置为新的主服务器的从服务器

- Sentinel只是一个运行在特殊模式下的Redis服务器，它使用了和普通模式不同的命令表，所以Sentinel模式能够使用的命令和普通Redis服务器能够使用的命令不同;
- Sentinel会读入用户指定的配置文件，为每个要被监视的主服务器创建相应的实例结构，并创建连向主服务器的命令连接和订阅连接，其中命令连接用于向主服务器发送命令请求，而订阅连接则用于接收指定频道的消息;
- Sentinel通过向主服务器发送INFO命令来获得主服务器属下所有从服务器的地址信息，并为这些从服务器创建相应的实例结构，以及连向这些从服务器的命令连接和订阅连接;
- 在一般情况下，Sentinel以每十秒一次的频率向被监视的主服务器和从服务器发送INFO命令，当主服务器处于下线状态，或者Sentinel正在对主服务器进行故障转移操作时，Sentinel向从服务器发送INFO命令的频率会改为每秒一次;
- 对于监视同一个主服务器和从服务器的多个Sentinel来说，它们会以每两秒一次的频率，通过向被监视服务器的__sentinel__:hello频道发送消息来向其他Sentinel宣告自己的存在;
- 每个Sentinel也会从__sentinel__:hello频道中接收其他Sentinel发来的信息，并根据这些信息为其他Sentinel创建相应的实例结构，以及命令连接;
- Sentinel只会与主服务器和从服务器创建命令连接和订阅连接，Sentinel与Sentinel之间则只创建命令连接;
- 主观下线：在默认情况下，Sentinel会以每秒一次的频率向所有与它创建了命令连接的实例（包括主服务器、从服务器、其他Sentinel在内）发送PING命令，并通过实例返回的PING命令回复来判断实例是否在线。当Sentinel将一个主服务器判断为主观下线时，它会向同样监视这个主服务器的其他Sentinel进行询问，看它们是否同意这个主服务器已经进入主观下线状态;
- 客观下线：当Sentinel将一个主服务器判断为主观下线之后，为了确认这个主服务器是否真的下线了，它会向同样监视这一主服务器的其他Sentinel进行询问，看它们是否也认为主服务器已经进入了下线状态（可以是主观下线或者客观下线）。当Sentinel从其他Sentinel那里接收到足够数量的已下线判断之后，Sentinel就会将从服务器判定为客观下线，并对主服务器执行故障转移操作；
- 从多个Sentinel中选举领头Sentinel做故障转移操作：当一个主服务器被判断为客观下线时，监视这个下线主服务器的各个Sentinel会进行协商，选举出一个领头Sentinel，并由领头Sentinel对下线主服务器执行故障转移操作，Sentinel系统选举领头Sentinel的方法是对Raft算法的实现；
- 领头Sentinel进行故障转移的步骤：在选举产生出领头Sentinel之后，领头Sentinel将对已下线的主服务器执行故障转移操作，该操作包含以下三个步骤：
  - 在已下线主服务器属下的所有从服务器里面，挑选出一个从服务器，并将其转换为主服务器；
  - 让已下线主服务器属下的所有从服务器改为复制新的主服务器；
  - 将已下线主服务器设置为新的主服务器的从服务器，当这个旧的主服务器重新上线时，它就会成为新的主服务器的从服务器；
- 新主服务器的挑选规则：
  - 删除列表中所有处于下线或者断线状态的从服务器，这可以保证列表中剩余的从服务器都是正常在线的；
  - 删除列表中所有最近五秒内没有回复过领头Sentinel的INFO命令的从服务器，这可以保证列表中剩余的从服务器都是最近成功进行过通信的；
  - 删除所有与已下线主服务器连接断开超过downaftermilliseconds*10毫秒的从服务器：downaftermilliseconds选项指定了判断主服务器下线所需的时间，而删除断开时长超过downaftermilliseconds*10毫秒的从服务器，则可以保证列表中剩余的从服务器都没有过早地与主服务器断开连接，换句话说，列表中剩余的从服务器保存的数据都是比较新的；
  - 之后，领头Sentinel将根据从服务器的优先级，对列表中剩余的从服务器进行排序，并选出其中优先级最高的从服务器。如果有多个具有相同最高优先级的从服务器，那么领头Sentinel将按照从服务器的复制偏移量，对具有相同最高优先级的所有从服务器进行排序，并选出其中偏移量最大的从服务器（复制偏移量最大的从服务器就是保存着最新数据的从服务器）；
  - 最后，如果有多个优先级最高、复制偏移量最大的从服务器，那么领头Sentinel将按照运行ID对这些从服务器进行排序，并选出其中运行ID最小的从服务器；

PS：运行id是单个redis每次启动时生成身份识别码（40位的字符），一个redis多次启动会生成多个运行id，每次重启就会发生变化
slave和master首次建立连接后，slave会保存master的run id。当复制时发现master和之前的 run_id 不同时，将会重新进行全量同步

集群方案：
1.客户端分片（sharding）：分片逻辑放在客户端实现，通过Redis客户端预先定义好的路由规则，把对Key的访问转发到不同的Redis实例中，最后把返回结果汇集;
缺点：在不同的客户端程序中，维护相同的分片逻辑成本高

2.中间代理分片：Redis客户端把请求发送到中间代理（比如Twemproxy、codis），中间代理根据路由规则发送到正确的Redis实例，最后中间代理把结果汇集返回给客户端。
中间代理的主要作用有三个：
一个是负责在客户端和Redis 节点之间转发请求和响应
一个是负责监控集群中所有 Redis 节点状态，如果发现有问题节点，及时进行主从切换
一个是维护集群的元数据，这个元数据主要就是集群所有节点的主从信息以及槽和节点关系映射表

这个架构最大的优点是对客户端透明，在客户端视角来看，整个集群和一个超大容量的单节点 Redis 是一样的。并且，由于分片算法是代理服务控制的，扩容也比较方便，新节点加入集群后，直接修改代理服务中的元数据就可以完成扩容

不过，这个架构的缺点也很突出，增加了一层代理转发，每次数据访问的链路更长了，必然会带来一定的性能损失。而且，代理服务本身又是集群的一个单点，当然，我们可以把代理服务也做成一个集群来解决单点问题，那样集群就更复杂了

3.Redis自己实现的集群3.0（p2p模式完全去中心化）：Redis把所有的Key分成了16384（2的14次方）个slot，每个Redis实例负责其中一部分slot（0个或最多16384个slot）。集群中的所有信息（节点、端口、slot等），都通过节点之间定期的数据交换而更新（gossip协议）,Redis客户端往任意一个Redis实例发出请求，如果所需数据不在该实例中，通过重定向命令引导客户端访问所需的实例

一个redis集群通常由多个节点（node）组成，在刚开始的时候，每个节点都是相互独立的，他们都处于一个只包含自己的集群当中，可通过客户端向一个节点发送CLUSTER MEET命令，可以让node节点与ip和port制定的节点进行握手（handshake），当握手成功时，Node节点就会将ip和port所指定的节点添加到node节点当前所在集群中。

每个node节点都会使用一个clusterNode结构来记录自己的状态，并为集群中的所有其他节点（包括主节点和从节点）都创建一个相应的clusterNode结构，来记录其他节点的状态。

Redis集群通过分片的方式来保存数据库中的键值对：集群的整个数据库被分为16384个槽（slot），数据库中的每个键都属于这18634个slot中的其中一个（HASH_SLOT = CRC16(key) mod 16384），集群中的每个节点可以处理0个或最多16384个slot。当数据库中的16384个slot都有节点在处理时，集群处于于上线状态（ok），相反如果数据库中有任何一个槽没有得到处理，那么集群处于下线状态（fail）

Redis cluster 解决海量数据和高并发
分片：可以解决 Redis 保存海量数据的问题，并且客观上提升了 Redis 的并发能力和查询性能。Redis Cluster 进行了分片之后，每个分片都会承接一部分并发的请求，加上 Redis 本身单节点的性能就非常高，所以大部分情况下不需要再像 MySQL 那样做读写分离来解决高并发的问题。默认情况下，集群的读写请求都是由主节点负责的，从节点只是起一个热备的作用。当然了，Redis Cluster 也支持读写分离，在从节点上读取数据

smart jedis
基于重定向的客户端，很消耗网络io，因为大部分情况下，可能都会出现一次请求重定向，才能找到正确的节点，所以大部分的客户端比如java redis客户端，都是jedis，都是smart的，本地维护一份hashslot -> node的映射表在缓存里，大部分情况下直接走本地缓存就可以找到hashslot -> node，不需要通过节点进行moved重定向

Redis cluster 解决高可用
增加从节点，做主从复制。Redis Cluster 支持给每个分片增加一个或多个从节点，每个从节点在连接到主节点上之后，会先给主节点发送一个 SYNC 命令，请求一次全量复制，也就是把主节点上全部的数据都复制到从节点上。全量复制完成之后，进入同步阶段，主节点会把刚刚全量复制期间收到的命令，以及后续收到的命令持续地转发给从节点。
如果某个分片的主节点宕机了，集群中的其他节点会在这个分片的从节点中选出一个新的节点作为主节点继续提供服务。新的主节点选举出来后，集群中的所有节点都会感知到，这样，如果客户端的请求 Key 落在故障分片上，就会被重定向到新的主节点上

高可用性与主备切换原理
1.判断节点宕机
如果一个节点认为另外一个节点宕机，name就是pfail，主观宕机；
如果多个节点都认为另外一个节点宕机了，那么就是fail，客观宕机，跟哨兵的原理几乎一样，sdown，odown；
在cluster-node-timeout内，某个节点一直没有返回pong，那么就被认为pfail；
如果一个节点认为某个节点pfail了，那么会在gossip ping消息中，ping给其他节点，如果超过半数的节点都认为pfail了，那么就会变成fail；
2.从节点过滤
对宕机的master node，从其所有的slave node中，选择一个切换成master node；
检查每个slave node与master node断开连接的时间，如果超过了cluster-node-timeout * cluster-slave-validity-factor，那么就没有资格切换成master；
3.从节点选举
对所有从节点进行排序，slave priority（可配置），offset，run id；
每个从节点，都根据自己对master复制数据的offset，来设置一个选举时间，offset越大（复制数据越多）的从节点，选举时间越靠前，优先进行选举；
所有的master node开始slave选举投票，给要进行选举的slave进行投票，如果大部分master node（N/2 + 1）都投票给了某个从节点，那么选举通过，那个从节点可以切换成master；
从节点执行主备切换，从节点切换为主节点；

Redis Cluster 非常适合构建中小规模 Redis 集群，这里的中小规模指的是，大概几个到几十个节点这样规模的 Redis 集群。但是Redis Cluster不太适合构建超大规模集群，主要原因是，它采用了去中心化的设计。Redis 的每个节点上都保存了所有槽和节点的映射关系表，采用了一种去中心化的流言 (Gossip) 协议来传播集群配置的变化。八卦协议(gossip协议)的缺点就是传播速度慢，并且是集群规模越大，传播的越慢。在集群规模太大的情况下，数据不同步的问题会被明显放大，还有一定的不确定性，如果出现问题很难排查。所以 Redis Culster 也不适合做跨机房（或地域）部署。
Gossip 协议可以保证在一段时间后，集群中的每一个实例都能获得其它所有实例的状态信息。每个实例在发送一个 Gossip 消息时，除了会传递自身的状态信息，默认还会传递集群十分之一实例的状态信息。所以，对于一个包含了 1000 个实例的集群来说，每个实例发送一个 PING 消息时，会包含 100 个实例的状态信息，总的数据量是 10400 字节，再加上发送实例自身的信息，一个 Gossip 消息大约是 10KB。此外，为了让 Slot 映射表能够在不同实例间传播，PING 消息中还带有一个长度为 16,384 bit 的 Bitmap，这个 Bitmap 的每一位对应了一个 Slot，如果某一位为 1，就表示这个 Slot 属于当前实例。这个 Bitmap 大小换算成字节后，是 2KB。我们把实例状态信息和 Slot 分配信息相加，就可以得到一个 PING 消息的大小了，大约是 12KB。PONG 消息和 PING 消息的内容一样，所以，它的大小大约是 12KB。每个实例发送了 PING 消息后，还会收到返回的 PONG 消息，两个消息加起来有 24KB。虽然从绝对值上来看，24KB 并不算很大，但是，如果实例正常处理的单个请求只有几 KB 的话，那么，实例为了维护集群状态一致传输的 PING/PONG 消息，就要比单个业务请求大了。而且，每个实例都会给其它实例发送 PING/PONG 消息。随着集群规模增加，这些心跳消息的数量也会越多，会占据一部分集群的网络通信带宽，进而会降低集群服务正常客户端请求的吞吐量

节点的clusterNode结构的slots属性和numslot属性记录了节点负责处理哪些槽。

一个节点除了会将自己负责处理的槽记录在clusterNode结构的slots属性和numslots属性之外，它还会将自己的slots数组通过消息发送给集群中的其他节点，以此来告诉其他节点自己目前负责处理哪些槽。因为集群中的每个节点都会将自己的slots数组通过消息发送给集群中的其他节点，并且每个接收到slots数组的节点都会将数组保存到相应节点的clusterNode结构里，因此，集群中的每个节点都会知道数据库中的16384个槽分别被指派给了集群中的哪些节点。

在对数据库中的16384个槽都进行了指派之后，集群就会进入上线状态，这时客户端就可以向集群中的节点发送数据命令了。
当客户端向节点发送与数据库键有关的命令时，接收命令的节点会计算出命令要处理的数据库键属于哪个槽，并检查这个槽是否指派给了自己：
1）如果键所在的槽正好就指派给了自己，那么节点就直接执行这个命令；
2）如果键所在的槽没有指派给当前节点，那么节点会向客户端返回一个MOVED错误，指引客户端转向（redirect）到正确的节点，并再次发送之前想要执行的命令；

（在线）重新分片
Redis集群的重新分片操作可以将任意数量已经指派给某个节点（源节点）的槽改为指派给另一个节点（目标节点），并且相关槽所属的键值也会从源节点被移动到目标节点。
重新分派操作可以在线（onilne）进行，在重新分片的过程中，集群不需要下线，并且源节点和目标节点都可以继续处理命令请求

在重新分片期间，源节点向目标节点迁移一个槽的过程中，可能会出现这样一种情况：属于被迁移槽的一部分键值对保存在源节点里，而另一部分键值对则保存在目标节点里。
当客户端向源节点发送一个与数据库键有关的命令，并且命令要处理的键恰好出于正在被迁移的槽时：
1）源节点会现在自己的数据库里面查找指定的键，如果找到的话，就直接执行客户端发送的命令；
2）相反，如果没有找到，那么这个键有可能已经被迁移到了目标节点，源节点将向客户端返回一个ASK错误，指引客户端转向正在导入槽的目标节点，并再次发送之前想要执行的命令；

ASK错误和MOVED错误的区别
1）MOVED错误代表槽的负责权已经从一个节点转移到另一个节点，客户端收到该错误后，后续每次遇到关于槽i的命令请求时，都可以直接将命令请求发送到MOVED错误所指向的节点；
2）与MOVED错误相反，ASK错误只是两个节点在迁移槽的过程中使用的一种临时措施，在客户端收到该错误后，只会在接下来的一次请求中将关于槽i的命令请求发送至ASK错误所指的节点，但这种转向不会对客户端后续发送关于槽i的命令请求产生任何影响；

redis分布式集群倾斜问题
主要分为两类：

- 数据存储容量倾斜，数据存储总是落到集群中少数节点；
- qps请求倾斜，qps总是落到少数节点;

导致Redis集群倾斜的常见原因

- 系统设计时，redis键空间(keyspace)设计不合理，出现”热点key",导致这类key所在节点qps过载，集群出现qps倾斜；
- 系统存在BigKey，导致BigKey所在节点的容量和QPS过载，集群出现qps和容量倾斜；
- DBA在规划集群或扩容不当，导致数据槽(slot)数分配不均匀，导致容量和请求qps倾斜;
- 系统大量使用[Keys hash tags](http://redis.io/topics/cluster-spec), 可能导致某些数据槽位的key数量多，集群出现qps和容量倾斜；
- 工程师执行monitor这类命令，导致当前节点client输出缓冲区增大；used_memory_rss被撑大；导致节点内存容量增大，出现容量倾斜；

如何有效避免Redis集群倾斜问题

- 系统设计 redis集群键空间和query pattern时，应避免出现热点key, 如果有热点key逻辑，尽量打散分布不同的节点或添加程序本地缓存；
- 系统设计 redis集群键空间 时，应避免使用大key，把key设计拆分打散；大key除了倾斜问题，对集群稳定性有严重影响；
- redis集群部署和扩缩容处理，保证数据槽位分配平均；
- 系统设计角度应避免使用keys hash tag；
- 日常运维和系统中应避免直接使用keys,monitor等命令，导致输出缓冲区堆积；这类命令建议作rename处理；
- 合量配置normal的client output buffer, 建议设置10mb,slave限制为1GB按需要临时调整 (警示:和业务确认调整再修改，避免业务出错)

Cache aside pattern
 数据库主从+外部缓存的架构情况下 如何处理读和写场景下的数据库和缓存操作顺序？
首先需要一个中间代理层来处理这部分逻辑
 读场景下
 1）先读缓存；
 2）缓存 hit，则直接从缓存返回数据；
 3）缓存 miss，则读数据库（从库），并更新缓存；

 写场景下
 1）先写数据库（主库）；
 2）数据库写成功后，淘汰缓存（注意这里不是更新缓存）；
 3）读操作读缓存 miss，查询从库，更新缓存；（这里可能存在的问题是，主从同步延迟时，从库读到的不是最新的数据，导致更新的缓存是旧值）
 针对3的解决思路是：写缓存的时候设置一个过期时间，定期淘汰，减少旧值影响时间；或者异步淘汰缓存，确保从库更新成功后再淘汰缓存，比如监听从库 binlog，从库 binlog 完成一定是从库执行完成

写场景下数据库写成功后为什么是淘汰缓存而不是更新缓存？
因为在高并发的时候，如果同时有两个写操作发生，因为写数据库 和 更新缓存 的先后顺序没法保证 可能导致最后缓存和数据库中的数据不一致

Redis过期键的删除策略
对于过期键一般有三种删除策略
定时删除：在设置键的过期时间的同时，创建一个定时器(timer)，让定时器在键的过期时间来临时，立即执行对键的删除操作；
惰性删除：放任键过期不管，但是每次从键空间中获取键时，都检查取得的键是否过期，如果过期的话，就删除该键；如果没有过期，那就返回该键；
定期删除：每隔一段时间，程序就对数据库进行一次检查，删除里面的过期键。至于删除多少过期键，以及要检查多少个数据库，则由算法决定。
下面我们来看看三种策略的优缺比较：
（1）定时删除策略对内存是最友好的：通过使用定时器，定时删除策略可以保证过期键会尽可能快地被删除，并释放过期键所占用的内存；但另一方面，定时删除策略的缺点是，他对CPU是最不友好的：在过期键比较多的情况下，删除过期键这一行为可能会占用相当一部分CPU时间，在内存不紧张但是CPU时间非常紧张的情况下，将CPU时间用在删除和当前任务无关的过期键上，无疑会对服务器的响应时间和吞吐量造成影响；
（2）惰性删除策略对CPU时间来说是最友好的：程序只会在取出键时才对键进行过期检查，这可以保证删除过期键的操作只会在非做不可的情况下进行；惰性删除策略的缺点是，它对内存是最不友好的：如果一个键已经过期，而这个键又仍然保留在数据库中，那么只要这个过期键不被删除，它所占用的内存就不会释放；
定时删除占用太多CPU时间，影响服务器的响应时间和吞吐量；惰性删除浪费太多内存，有内存泄漏的危险。定期删除策略是前两种策略的一种整合和折中：
（3）定期删除：每隔一段时间执行一次删除过期键操作，并通过限制删除操作执行的时长和频率来减少删除操作对CPU时间的影响；
通过定期删除过期键，定期删除策略有效地减少了因为过期键而带来的内存浪费；
定期删除策略的难点是确定删除操作执行的时长和频率。
Redis的过期键删除策略：Redis服务器实际使用的是惰性删除和定期删除两种策略。

CouchBase
couchbase是CouchDB和MemBase的合并。而memBase是基于Memcached的。因此couchbase联合了CouchDB的简单可靠和memcached的高性能，以及membase的可扩展性。

灵活的数据模型：couchbase中使用json格式存储对象和对象之间的关系

数据存储：
Couchbase使用bucket提供数据管理服务，bucket相当于关系数据库中的库，couchbase中没有表的概念，保存数据时，先建bucket，然后就直接插入数据了。bucket可以供集群中的多个客户端程序访问。couchbase通过bucket组织，管理和分析数据资源。

Couchbase中有两种类型的数据bucket，当启动couchbase服务的时候，可以选择需要的类型。
1)memcached buckets：只将数据存储在内存中。提供了一个分布式的(横向扩展)，纯内存的，key-value缓存。Memcached buckets 设计用于关系数据库的缓存，可以缓存经常访问的数据，由此减少web程序中数据库的查询次数。
2)couchbase buckets：数据存储在内存和硬盘。提供高可用性和可动态重新配置的分布式数据存储，提供数据持久化和复制服务。couchbase buckets100%兼容开源的分布式缓存memcached

内存配额
1）Server Quota：
Couchbase服务初始化时会给服务器分配内存限额，表示这个服务器中可用的最大内存，是node级的。初始配置在集群中的第一台服务器(node)上，所有服务器的内存配额都是一样的。例如集群中有10台服务器，服务器内存配额是16G，整个集群中共160G可用内存。如果需要加2个新的服务器，每个新的服务器需要16G的可用内存，集群中可用的内存数将是192G。
2）Bucket Quota:
Bucket内存配额是分配给一个bucket的可用内存。配置在每个节点上，是从server quota中分配出去的。例如，如果你创建了一个新的bucket，限额是1GB，在10个节点的集群中，汇总后会有10G。如果添加两个节点，集群中汇总后会有12G的bucket限额

因此，增加新的节点就可以扩展总的可用内存，从而增加存储的数据量

负载均衡
在 Couchbase 中，我们所操作的每一个bucket会逻辑划分为1024个vbucket，其数据的储存基于每个vbucket储存并且每个 vbucket都会映射到相对应的服务器节点，这种储存结构的方式叫做集群映射。如下图所示，当应用与Couchbase服务器交互时，会通过SDK的与服务器数据进行交互，当应用操作某一个的bucket的key值时，在SDK中会通过哈希的方式计算，使用公式crc32(key)%1024确定key值是属于1024个vbucket中的某个，然后根据vbucket所映射的节点服务器对数据进行操作

Couchbase的集群结构

Couchbase 群集所有节点都是对等的（没有主节点），只是在创建群或者加入集群时需要指定一个主节点，一旦结点成功加入集群，所有的结点对等

Smart client
由于 couchbase 是对等集群，所有的节点都可以同时对客户端提供服务，这就需要有方法把集群的节点信息暴露给客户端，couchbase 提供了一套机制，客户端可以获取所有节点的状态以及节点的变动，由客户端根据集群的当前状态计算 key 所在的位置

谨慎猜测：在线auto sharding的时候，smart client会根据节点变化前后的两个一致性hash环，来分别计算需要迁移的数据迁移前的节点和迁移后的节点，如果前一个节点取不到，自动到另外一个节点获取数据
相比redis cluster的不同：redis cluster每个集群节点通过一致性hash环来计算数据迁移前后的节点位置

vBucket 概念的引入，是 couchbase 实现 auto sharding，在线动态增减节点的重要基础

简单的解释 vBucket 可以从静态分片开始说起，静态分片的做法一般是用 key 算出一个 hash，得到对应的服务器，这个算法很简单，也容易理解。如以下代码所示

servers = ['server1:11211', 'server2:11211', 'server3:11211']
vbuckets = [0, 0, 1, 1, 2, 2]
server_for_key(key) = servers[vbuckets[hash(key) % vbuckets.length]]

Couchbase中的复制
集群内复制（单集群内复制）

集群内复制主要针对同一个集群中多个节点的数据进行多份复制备份，并且复制的份数会分布到不同的节点中。在数据分布中我们知道每个节点都会储存有效的vbucket和复制的vbucket。如下图展示，当应用对对数据进行写操作，此操作会先到集群节点中所对应有效的vbucket的数据进行写操作，并且有效的vbucket节点会根据DCP协议传输写操作的变更传输到复制的vbucket所对应的节点，对复制的vbucket进行变更。可复制的 vbucket的份数，可以在操作bucket的时候进行配置，备份数量为1-3份

集群内复制在Couchbase中可以由应用在写数据的时候选择一致性、可用性与性能之间的权衡，Couchbase提供了以下几种模式的复制：

1. 内存级的储存：此种模式是当应用写数据时，当数据已经储存到访问节点的内存中后，就会返回正确回复给应用，同步其它节点和持久化储存都是由异步处理。此种模式性能最高，相对的一致性和可用性也是最差；
2. 内存+持久化级的储存：此种模式是当应用写数据时，只有数据储存在访问节点的内存和硬盘中后，才会返回正确回复给应用，同步其它节点是异步处理方式。此种模式，提升了一定的可用性，但如果单节点出现问题，数据可能出现不一致性；
3. 内存+备份节点级的储存：此种模式是当应用写数据时，只有数据储存同步到其它节点的内存中时，才会返回正确回复给应用，持久化处理都是异步处理，应用可以选择同步数据的节点数量。此种模式保证了数据一定备份和容灾，但是也有一定可能数据没有持久化会丢失。
4. 内存+持久化+备份节点的储存：此种模式是当应用写数据时，数据存储必须满足所需要的节点中内存复制和持久化都完成后，才可以返回正确给应用。这种模式保证即使有效vbucket节点机器出现无法恢复的故障，仍然能够恢复，但是性能最差

在对于读的一致性的权衡，Couchbase 也提供了以下两种形式：

1. 读取时获取一致性的的数据：此种方式是当数据更新后所有的应用读到数据都是一样的。主要原理是读和写都是操作有效vbucket，但牺牲了可用性；
2. 读取时可以获取不一致性的数据：此种方式适合对于对数据一致性不是很重要，对可用性比较注重的场景。主要原理是读的时候，有效vbucket不可用时，数据会从备份vbucket中获取数据

跨数据中心复制（多集群间复制）

跨数据中心复制主要是针对多个集群间的数据复制，此种复制主要以异步的方式通过XDCR协议同步数据到其它集群中备份，从而实现单集群或机房出现问题级的容灾。跨数据中心复制是以bucket为单位进行复制的

DCP (Database Change Protocol)
DCP 协议是一个高效的二进制协议，它主要用于集群内的数据复制、索引复制、备份数据等等。主要概念有以下几点：

1. 有序复制，基于每个vbucket存在一个顺序序列号，同步时根据序列号进行更新；
2. 重启恢复，当同步连接中断后，重新连接后，会对冲突数据进行恢复；
3. 一致性，使用快照数据同步数据统一性；
4. 内存间复制。

XDCR (Cross Data Center Replication)
XDCR提供了多个有效vbucket的数据的复制，主要用于跨数据中心的多集群间的复制。主要概念有一下几点：

1. 基于bucket复制，两个集群的同一个bucket可以实现单向或者双向复制；
2. 通过DCP协议保持持续性复制，一个XDCR连接中包括多个DCP数据流。这些流可以根据不同的分区对目的集群进行同步复制；
3. 支持多种集群拓扑复制。集群间可以通过单向，双向复制。多个集群可以实现1对1,1对多,多对1等的集群复制拓扑图；
4. 安全复制。数据中心见传输数据可以使用SSL进行加密；
5. 最终一致性和解决数据冲突的能力。当出现冲突数据，会使用元数据的序列值，CAS值，文档标签和过期时间限制对数据进行冲突解决。

跨机房部署

在分布式系统中，跨机房问题一直都是比较复杂问题。机房之间的网络延时较大，且不稳定。跨机房问题主要包含两个方面：数据同步以及服务切换。
在Couchbase中可以以一下两种方式跨机房：
1）集群整体切换，这种方式是两个机房部署了相同的Couchbase集群，由XDCR以异步方式同步集群副本，当出现问题时，可切换集群。这种方式的问题是 当主机房整体出现故障时，有两种选择：要么将服务切换到备机房，忍受数据丢失的风险；要么停止服务，直到主机房恢复为止。因此，主备机房切换往往是手工 的，允许用户根据业务的特点选择“丢失数据”或者“停止服务”。
2）单个集群跨机房，这种方式是将单个集群部署到多个机房，允许不同数据分片的主副本位于不同的机房。这种方式主要是考虑到写数据的时候，一致性比较强的数据是同步到每个节点中才算写成功的案例，当机房出现问题时，大部分数据是可以继续可用。

自动分片集群技术

当从CoucBASE集群添加或删除新服务器时，数据会自动重新分配到集群中的节点，并在服务客户端请求时不停机重新平衡。均匀的过程在集群中自动分发数据称为自动分片（Auto-sharding）。如果更多的RAM和I/O容量是需要，只需添加服务器即可。在集群节点之间均匀均衡的情况下，数据是连续可用的。客户端请求被路由到使用数据局部性的最接近客户端的服务器。当从靠近客户端的服务器提供数据时，数据局部性提高响应时间并减少网络流量

相比memcache差异
数据分片策略的差异：
memcached的做法是用 key 算出一个 hash，得到服务器列表中的对应服务器。这个列表需要动态维护，还需要一个hash算法用于处理集群拓扑结构的变化
这种算法很简单，也很容易理解，但也有几个问题：
1、如果一台服务器失效，会造成该分片的所有 key 失效；
2、如果服务器容量不同，管理非常麻烦；
3、运维、配置非常不方便；

为了把 key 跟服务器解耦，couchbase 引入了 vBucket。每个key都属于一个vbucket，查找对应的value时先用hash函数计算这个key属于哪个vbucket，再从vBucket 与服务器对应表中查找这个vbucket属于哪个服务器，映射表保存vbucket和服务器的对应关系，一个bucket一行，一个服务器可以对应多个vbucket。
1、key hash 对应一个 vBucket，不再直接对应服务器。
2、集群维护一个全局的 vBucket 与服务器对应表

例如，集群中有3个服务器，客户端要查找 一个key对应的value值，首先计算key属于哪个Vbucket，在这个例子中，hash结果是vB8 ，通过查映射表，客户端确定vB8对应到服务器C，然后get操作直接发送到服务器C

一段时间后，需要加一个新的服务器D到集群，vbuckets映射表更新为：

这时，客户端再想取key对应的value值，hash算法结果仍为vB8，但是新的映射表会将vB8映射到服务器D。

由于 vBucket 把 key 跟服务器的静态对应关系解耦合，基于 vBucket 可以实现一些非常强大有趣的功能，例如：
1）Replica：以 vBucket 为单位的主从备份。如果某个节点失效，只需要更新 vBucket 映射表，马上启用备份数据；
2）动态扩容：新增加一个节点后，可以把部分 vBucket 转移到新节点上，并更新 vBucket 映射表；

参考：
<https://zhuanlan.zhihu.com/p/49962194>
<https://cloud.tencent.com/developer/article/1188393>
