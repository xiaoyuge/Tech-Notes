## **分布式事务**

事务的概念虽然最初起源于数据库系统，但今天已经有所延伸，不再局限于数据库本身了。所有需要保证数据一致性的应用场景，包括但不限于数据库、事务内存、缓存、消息队列、分布式存储等等，都有可能用到事务

1. 本地事务（Local Transaction，或者叫局部事务，强一致性）

    本地事务是指仅操作单一事务资源的、不需要全局事务管理器进行协调的事务

    本地事务是一种最基础的事务解决方案，只适用于单个服务使用单个数据源的场景。从应用角度看，它是直接依赖于数据源本身提供的事务能力来工作的，在程序代码层面，最多只能对事务接口做一层标准化的包装（如JDBC接口），并不能深入参与到事务的运作过程中，事务的开启、终止、提交、回滚、嵌套、设置隔离级别，乃至与应用代码贴近的事务传播方式，全部都要依赖底层数据源的支持才能工作

2. 分布式事务（强一致性）

    指单个（或多个）服务使用多个数据源场景的强一致性事务解决方案，实现方案是基于X/OPEN提出的DTP模型，其中的XA规范定义了TM和RM的交互接口规范，但XA规范是一个通用的与语言无关的规范，Java中专门定义了JSR 907 Java Transaction API，是XA规范的Java版，其把XA规范中规定的DTP模型交互接口抽象成Java接口中的方法，并规定每个方法要实现什么样的功能

    但现在业界对于多个服务多个数据源的分布式服务场景（典型的微服务场景），因为性能等问题基本放弃了ACID的强一致性处理方案
    详细见《使用Atomikos实现JTA分布式事务》

3. 分布式事务（最终一致性）：柔性事务，基于BASE理论，实现最终一致性


### **（1）分布式事务（强一致性）**

X/OPEN DTP：在DTP（Distributed Transaction Processing）模型中，规定了模型的五个组成元素：


DTP模型的交互流程示例


（1）事务管理器(Transaction Manager)：
处于图中最为核心的位置，其他的事务参与者都是与事务管理器进行交互。事务管理器提供事务声明，事务资源管理，同步，事务上下文传播等功能。JTA规范定义了事务管理器与其他事务参与者交互的接口，而JTS规范定义了事务管理器的实现要求，因此我们看到事务管理器底层是基于JTS的
（2）应用服务器(application server)：
顾名思义，是应用程序运行的容器。JTA规范规定，事务管理器的功能应该由application server提供，如上图中的EJB Server。一些常见的其他web容器，如：jboss、weblogic、websphere等，都可以作为application server，这些web容器都实现了JTA规范。特别需要注意的是，并不是所有的web容器都实现了JTA规范，如tomcat并没有实现JTA规范，但Bittronix、Atomikos和JBossTM（以前叫Arjuna）等都以JAR包的形式实现了JTA的接口，称为JOTM（Java Open TransactionManager，Java开源事务管理器），使得我们也能够在Tomcat、Jetty这样的JavaSE环境下使用JTA
（3）应用程序(application)：
简单来说，就是我们自己编写的应用，部署到了实现了JTA规范的application server中，之后我们就可以我们JTA规范中定义的UserTransaction类来声明一个分布式事务。通常情况下，application server为了简化开发者的工作量，并不一定要求开发者使用UserTransaction来声明一个事务，开发者可以在需要使用分布式事务的方法上添加一个注解，就像spring的声明式事务一样，来声明一个分布式事务。
特别需要注意的是，JTA规范规定事务管理器的功能由application server提供。但是如果我们的应用不是一个web应用，而是一个本地应用，不需要被部署到application server中，无法使用application server提供的事务管理器功能。又或者我们使用的web容器并没有事务管理器的功能，如tomcat。对于这些情况，我们可以直接使用一些第三方的事务管理器类库JOTM（Java Open Transaction Manager），如Bittronix、Atomikos和JBossTM。将事务管理器直接整合进应用中，不再依赖于application server。
（4）资源管理器(resource manager)：
理论上任何可以存储数据的软件，都可以认为是资源管理器RM。最典型的RM就是关系型数据库了，如mysql。另外一种比较常见的资源管理器是消息中间件，如ActiveMQ、RabbitMQ等， 这些都是真正的资源管理器。
正常情况下，一个数据库驱动供应商只需要实现JDBC规范即可，一个消息中间件供应商只需要实现JMS规范即可。 引入了分布式事务的概念后，DB、MQ等在DTP模型中的作用都是RM，二者是等价的，需要由TM统一进行协调。
为此，JTA规范定义了一个XAResource接口，其定义RM必须要提供给TM调用的一些方法。之后，不管这个RM是DB，还是MQ，TM并不关心，因为其操作的是XAResource接口。而其他规范(如JDBC、JMS)的实现者，同时也对此接口进行实现。如MysqlXAConnection，就实现了XAResource接口。
（5）通信资源管理器(Communication Resource Manager)：
这个是DTP模型中就已经存在的概念，对于需要跨应用的分布式事务，事务管理器彼此之间需要通信，这是就是通过CRM这个组件来完成的。JTA规范中，规定CRM需要实现JTS规范定义的接口。
下图更加直观的演示了JTA规范中各个模型组件之间是如何交互的：

说明如下：
* application 运行在application server中
* application 需要访问3个资源管理器(RM)上资源：1个MQ资源和2个DB资源。
* 由于这些资源服务器是独立部署的，如果需要同时进行更新数据的话并保证一致性的话，则需要使用到分布式事务，需要有一个事务管理器来统一协调。
* Application Server提供了事务管理器的功能
* 作为资源管理器的DB和MQ的客户端驱动包，都实现了XAResource接口，以供事务管理器调用

JTA
JTA的规范接口中，主要围绕以下几个接口：
* UserTransaction：编程人员接口
* TransactionManager：留给TM厂商（主要是应用服务器厂商如Jboss、Weblogic、websphere等）实现的与事务管理有关的接口
* Transaction：留给TM厂商实现的事务
* XAResource：留给RM厂商（比如数据库、消息中间件等）实现的与持久化资源有关的接口



2PC:两阶段提交（Two-Phase Commit）
两阶段提交的算法如下:
第一阶段：
1.  协调者会问所有的参与者结点，是否可以执行提交操作;
2.  各个参与者开始事务执行的准备工作：如：为资源上锁，预留资源，写undo/redo log等；
3.  参与者响应协调者，如果事务的准备工作成功，则回应“可以提交”，否则回应“拒绝提交”；
第二阶段：
1.如果所有的参与者都回应“可以提交”，那么，协调者向所有的参与者发送“正式提交”的命令。参与者完成正式提交，并释放所有资源，然后回应“完成”，协调者收集各结点的“完成”回应后结束这个Global Transaction。
2.如果有一个参与者回应“拒绝提交”，那么，协调者向所有的参与者发送“回滚操作”，并释放所有资源，然后回应“回滚完成”，协调者收集各结点的“回滚”回应后，取消这个Global Transaction


两阶段提交的问题：
1、性能问题。在两段式提交的执行过程中，所有参与节点都是事务阻塞型的，需要长时间锁定资源。在两段式提交过程中，所有参与者相当于被绑定为一个统一调度的整体，期间要经过两次远程服务调用，三次数据持久化（准备阶段写重做日志，协调者做状态持久化，提交阶段在日志写入提交记录），整个过程将持续到参与者集群中最慢的那一个处理操作结束为止，这决定了两段式提交的性能通常都较差，这会导致系统整体的并发吞吐量变低，在诸如抢购这种高并发高性能要求的业务场景中是不可接受的。
2、单点故障问题。事务协调者在链路中有着至关重要的作用，一旦协调者发生故障，没有正常发送Commit或Rollback的指令，参与者会一直等待并阻塞下去，整个系统将无法工作，因此需要投入巨大的精力来保障事务协调者的高可用性。
3、数据不一致问题。在阶段二中，如果协调者向参与者发送 commit 请求之后，发生了网络异常，会导致只有一部分参与者接收到了 commit 请求，没有接收到 commit 请求的参与者最终会执行回滚操作，从而造成数据不一致现象。在抢购业务中，这样的数据不一致有可能会对企业或消费者造成巨大的经济损失

3PC：三阶段提交（Three-Phase Commit）
三阶段提交把原本的两段式提交的准备阶段再细分为两个阶段，分别称为CanCommit、PreCommit，把提交阶段改称为DoCommit阶段。其中，新增的CanCommit是一个询问阶段，即协调者让每个参与的数据库根据自身状态，评估该事务是否有可能顺利完成。

将准备阶段一分为二的理由是，准备阶段是一个较重的操作，因为一旦协调者发出开始准备的消息，每个参与者都将马上开始写重做日志，它们所涉及的数据资源即被锁住，如果此时某一个参与者宣告无法完成提交，相当于大家都做了一轮无用功。所以，增加一轮询问阶段，如果都得到了正面的响应，那事务能够成功提交的把握就比较大了，这也意味着因某个参与者提交时发生崩溃而导致大家全部回滚的风险相对变小。

三段提交的核心理念是：在询问的时候并不锁定资源，除非所有人都同意了，才开始锁资源

理论上来说，如果第一阶段所有的节点返回成功，那么有理由相信成功提交的概率很大。这样一来，可以降低参与者Cohorts的状态未知的概率。也就是说，一旦参与者收到了PreCommit，意味他知道大家其实都同意修改.因此，如果节点处在P状态（PreCommit）的时候发生了F/T的问题，三段提交比两段提交的好处是，三段提交可以继续直接把状态变成C状态（Commit），而两段提交则不知所措。

相对于2PC，3PC主要解决的单点故障问题，并减少阻塞（解决性能问题），因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行commit。而不会一直持有事务资源并处于阻塞状态。但是这种机制也会导致数据一致性问题，因为，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。这样就和其他接到abort命令并执行回滚的参与者之间存在数据不一致的情况。

无论是二阶段提交还是三阶段提交都无法彻底解决分布式的一致性问题。Google Chubby的作者Mike Burrows说过， there is only one consensus protocol, and that’s Paxos” – all other approaches are just broken versions of Paxos. 意即世上只有一种一致性算法，那就是Paxos，所有其他一致性算法都是Paxos算法的不完整版

分布式共识算法

“分布式系统中如何对某个值达成一致”这个问题，可以把该问题划分为三个子问题来考虑，可以证明（具体证明就不列在这里了，感兴趣的读者可参考Raft的论文）当以下三个问题同时被解决时，即等价于达成共识：
* 如何选主（LeaderElection）；
* 如何把数据复制到各个节点上（Entity Replication）；
* 如何保证过程是安全的（Safety）

Paxos、Raft、ZAB等分布式算法经常会被称作“强一致性”的分布式共识协议，其实这样的描述有语病嫌疑，但我们都明白它的意思其实是：“尽管系统内部节点可以存在不一致的状态，但从系统外部看来，不一致的情况并不会被观察到，所以整体上看系统是强一致性的。”
与它们相对的，还有另一类被冠以“最终一致性”的分布式共识协议，这表明系统中不一致的状态有可能会在一定时间内被外部直接观察到，比如Gossip协议

一致性的分类
强一致性（尽管系统内部节点可以存在不一致的状态，但从系统外部看来，不一致的情况并不会被观察到，所以整体上看系统是强一致性的）
说明：保证系统改变提交以后立即改变集群的状态
模型：
Paxos
Raft（muti-paxos）
ZAB（muti-paxos）

最终一致性（系统中不一致的状态有可能会在一定时间内被外部直接观察到）
说明：系统不保证改变提交以后立即改变集群的状态，但是随着时间的推移最终状态是一致的。
模型：
DNS系统
Gossip协议

一致性算法实现举例
Google的Chubby分布式锁服务，采用了Paxos算法
etcd分布式键值数据库，采用了Raft算法
ZooKeeper分布式应用协调服务，Chubby的开源实现，采用ZAB算法
Consul的DC内数据复制使用的Raft，跨DC的数据同步使用的Gossip
Redis Cluster集群节点间的数据同步用的是Gossip

Paxos：分布式协议的标准
Paxos算法：
简单说来，Paxos的目的是让整个集群的结点对某个值的变更达成一致。Paxos算法基本上来说是个民主选举的算法——大多数的决定会成个整个集群的统一决定。任何一个点都可以提出要修改某个数据的提案，是否通过这个提案取决于这个集群中是否有超过半数的结点同意（所以Paxos算法需要集群中的结点是单数）

Raft：

Gossip：
Gossip protocol 最早是在 1987 年发表在 ACM 上的论文 《Epidemic Algorithms for Replicated Database Maintenance》中被提出。主要用在分布式数据库系统中各个副本节点同步数据之用，这种场景的一个最大特点就是组成的网络的节点都是对等节点，是非结构化网络，这区别与之前介绍的用于结构化网络中的 DHT 算法 Kadmelia

Gossip过程是由种子节点发起，当一个种子节点有状态需要更新到网络中的其他节点时，它会随机的选择周围几个节点散播消息，收到消息的节点也会重复该过程，直至最终网络中所有的节点都收到了消息。这个过程可能需要一定的时间，由于不能保证某个时刻所有节点都收到消息，但是理论上最终所有节点都会收到消息，因此它是一个最终一致性协议

注意：Gossip 过程是异步的，也就是说发消息的节点不会关注对方是否收到，即不等待响应；不管对方有没有收到，它都会每隔 1 秒向周围节点发消息。异步是它的优点，而消息冗余则是它的缺点

Gossip 的特点（优势）
1）扩展性
网络可以允许节点的任意增加和减少，新增加的节点的状态最终会与其他节点一致。
2）容错
网络中任何节点的宕机和重启都不会影响 Gossip 消息的传播，Gossip 协议具有天然的分布式系统容错特性。
3）去中心化
Gossip 协议不要求任何中心节点，所有节点都可以是对等的，任何一个节点无需知道整个网络状况，只要网络是连通的，任意一个节点就可以把消息散播到全网。
4）一致性收敛
Gossip 协议中的消息会以一传十、十传百一样的指数级速度在网络中快速传播，因此系统状态的不一致可以在很快的时间内收敛到一致。消息传播速度达到了 logN。
5）简单
Gossip 协议的过程极其简单，实现起来几乎没有太多复杂性。

Gossip 的缺陷
分布式网络中，没有一种完美的解决方案，Gossip 协议跟其他协议一样，也有一些不可避免的缺陷，主要是两个：
1）消息的延迟
由于 Gossip 协议中，节点只会随机向少数几个节点发送消息，消息最终是通过多个轮次的散播而到达全网的，因此使用 Gossip 协议会造成不可避免的消息延迟。不适合用在对实时性要求较高的场景下。
2）消息冗余
Gossip 协议规定，节点会定期随机选择周围节点发送消息，而收到消息的节点也会重复该步骤，因此就不可避免的存在消息重复发送给同一节点的情况，造成了消息的冗余，同时也增加了收到消息的节点的处理压力。而且，由于是定期发送，因此，即使收到了消息的节点还会反复收到重复消息，加重了消息的冗余。

Gossip 类型
Gossip 有两种类型：
* Anti-Entropy（反熵）：以固定的概率传播所有的数据
* Rumor-Mongering（谣言传播）：仅传播新到达的数据

Anti-entropy（anti-disorder）：一个节点会把所有的数据都跟其他节点共享，以便消除节点之间数据的任何不一致，它可以保证最终、完全的一致。换句话说，在这种模式下可以消除不同节点中数据的 disorder，因此 Anti-entropy 就是 anti-disorder。换句话说，它可以提高系统中节点之间的 similarity。但这种情况下，消息会不断反复的交换，因此消息数量是非常庞大的，无限制的（unbounded），这对一个系统来说是一个巨大的开销；

Rumor Mongering：消息只包含最新 update，体积更小。而且，一个 Rumor 消息在某个时间点之后会被标记为 removed，并且不再被传播，因此，消息可以发送得更频繁，但是系统有一定的概率会不一致。同时，因为某个时间点之后消息不再传播，因此消息是有限的，系统开销小


4.2 柔性事务（最终一致性）：TCC/SAGA/Seata/事务消息/本地消息表
柔性事务的核心思想是放弃传统分布式事务中对于严格强一致性的要求，允许在事务执行过程中存在数据不一致的中间状态，在业务上需要容忍中间状态的存在。柔性事务会提供完善的机制，保证在一段时间的中间状态后，系统能走向最终一致状态。

BASE理论：
柔性事务遵循 BASE理论。BASE理论是对CAP理论的延伸（对AP的补充），是为了解决CAP理论提出的分布式系统的一致性和可用性不能兼得的问题，和ACID是相反的设计哲学。核心思想是即使无法做到强一致性（Strong Consistency，CAP的一致性就是强一致性），但应用可以采用适合的方式达到最终一致性（Eventual Consitency）。

BASE是指基本可用（Basically Available）、柔性状态（ Soft State）、最终一致性（ Eventual Consistency）。
1.基本可用（Basically Available）
基本可用是指分布式系统在出现故障的时候，允许损失部分可用性，即保证核心可用。
电商大促时，为了应对访问量激增，部分用户可能会被引导到降级页面，服务层也可能只提供降级服务。这就是损失部分可用性的体现。

2.柔性状态（ Soft State）
柔性状态是指允许系统存在中间状态，而该中间状态不会影响系统整体可用性。分布式存储中一般一份数据至少会有三个副本，允许不同节点间副本同步的延时就是软状态的体现。mysql replication的异步复制也是一种体现。

3.最终一致性（ Eventual Consistency）
最终一致性是指系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。弱一致性和强一致性相反，最终一致性是弱一致性的一种特殊情况。

BASE模型的软状态是实现BASE理论的方法，基本可用和最终一致是目标

遵循 BASE理论的柔性事务放弃了隔离性，减小了事务中锁的粒度，使得应用能够更好的利用数据库的并发性能，实现吞吐量的线性扩展。异步执行方式可以更好地适应分布式环境，在网络抖动、节点故障的情况下能够尽量保障服务的可用性。因此在高并发、大流量的业务中，柔性事务是最佳的选择

4.2.1 TCC：Try-Confirm-Cancel(阿里提出)
TCC 就是一种解决多个微服务之间的分布式事务问题的方案。TCC 是 Try、Confirm、Cancel 三个词的缩写，其本质是一个应用层面上的 2PC，同样分为两个阶段：
* 准备阶段 ：协调者调用所有的每个微服务提供的 try 接口，将整个全局事务涉及到的资源锁定住，若锁定成功 try 接口向协调者返回 yes；
* 提交阶段 ：若所有的服务的 try 接口在阶段一都返回 yes，则进入提交阶段，协调者调用所有服务的 confirm 接口，各个服务进行事务提交。如果有任何一个服务的 try 接口在阶段一返回 no 或者超时，则协调者调用所有服务的 cancel 接口

既然 TCC 是一种服务层面上的 2PC。它是如何解决 2PC 无法应对宕机问题的缺陷的呢？
答案是不断重试。由于 try 操作锁住了全局事务涉及的所有资源，保证了业务操作的所有前置条件得到满足，因此无论是 confirm 阶段失败还是 cancel 阶段失败都能通过不断重试直至 confirm 或 cancel 成功（所谓成功就是所有的服务都对 confirm 或者 cancel 返回了 ACK）
这里还有个关键问题，在不断重试 confirm 和 cancel 的过程中（考虑到网络二将军问题的存在）有可能重复进行了 confirm 或 cancel，因此还要再保证 confirm 和 cancel 操作具有幂等性，也就是整个全局事务中，每个参与者只进行一次 confirm 或者 cancel。实现 confirm 和 cancel 操作的幂等性，有很多解决方案，例如每个参与者可以维护一个去重表（可以利用数据库表实现也可以使用内存型 KV 组件实现），记录每个全局事务（以全局事务标记 XID 区分）是否进行过 confirm 或 cancel 操作，若已经进行过，则不再重复执行

4.2.2 Saga模型


分为协同式和编排式

协同式Saga
以下图为例


不同系统服务执行完本地 ACID 事务之后，通过消息中间件来传递消息，比如
1.订单系统下单成功后发消息给库存系统扣减库存，扣减成功后发消息给支付系统来付款；
2.反过来，如果支付系统支付失败，则触发事务回滚动作，向库存系统发送回滚消息，库存系统接受到回滚消息后则执行退还库存操作，然后向订单系统发送回滚消息，订单系统接收到回滚消息后执行相应回滚动作；

编排式Saga


有一个集中的编排者进行事务协调
1.客户端触发事务编排者给订单系统发创建订单的消息，订单系统收到消息后利用本地事务创建订单；
2.订单系统成功执行本地 ACID 事务后，通过 MQ向订单编排者发回消息告知订单创建成功；
3.事务编排者收到消息后，依次给库存系统和支付系统发消息，要求他们分别完成本地 ACID 事务，最终整个事务协调完成；
4.假如在事务执行的过程中，支付环节出现了问题失败了，它就会通过 MQ 给事务编排者发送支付失败消息，事务编排者收到支付失败消息，则会启动补偿流程，依次反向向库存和订单系统发送反向补偿消息，最终实现整个事务的回滚；

下图是一个不通过 MQ 实现的编排式 Saga 模型


参与分布式事务的接口，需要通过注解的方式表明参与分布式事务
Proxy：事前事后拦截器（动态代理），负责1.生成TXID；2.调用正向接口；3.调用分布式事务补偿服务记录调用上线文信息，比如方法名，参数类型，参数值，执行状态（成功还是失败）等
分布式事务补偿服务：负责调用补偿接口




异步补偿机制：分布式事务补偿服务调用补偿接口是异步的，不是在本地事务执行失败后立即调用的
幂等补偿接口：补偿接口可能重复调用多次
记录请求调用链路：因为要按调用顺序调用补偿接口


分布式事务场景的抽象
业务服务A，包括多个服务实例A1、A2、A3等，访问DB
业务服务B，包括多个服务实例B1、B2、B3等，访问cache
业务服务C，包括多个服务实例C1、C2、C3等，访问MQ
 
在并发请求的情况下
请求1：A1 -> B2 ->  C3
请求2：A2 -> B3 -> C1

这个时候，服务A的两个服务实例A1和A2就可能会同时访问同一个资源，比如订单，这个时候，基于场景的需求，分布式锁可能就会派上用场

uber 的开源Saga中间件实现：cadence

协同式 Saga  vs 编排式 Saga
1.协同式 Saga 的好处是没有集中的编排者，也即没有集中的单点问题，整个系统是分散式无中心的，不足是每次添加一个新的消息类型，可能需要修改好几个参与方的接口和业务逻辑。另外如果后续增加事务参与方，那么消息交互的复杂度就会变大，整个系统的交互方式会变得难以理解，所以一般仅适用于小规模系统；
2.编排式 Saga 的编排流程是集中的，比较容易理解，也比较容易去跟踪系统的行为，也利用集中的流程管理，后续如果引入新的参与方，一般只需要调整编排者的编排流程，不影响其他的参与者。不足是潜在是有单点问题的，需要考虑高可用部署的问题。另外随着参与者的增多，集中式编排器的流程逻辑也会变得越来越复杂；

在实践中考虑到可理解性和可扩展性，更推荐采用编排式 Saga
Saga 模式只要求本地事务遵循 ACID，全局事务在执行过程中是会出现暂时的数据不一致的，也即 Saga 不保证全局事务的隔离性

4.2.3 Seata
Seata 是一款开源的分布式事务解决方案，致力于提供高性能和简单易用的分布式事务服务。Seata 将为用户提供了 AT、TCC、SAGA 和 XA 事务模式，为用户打造一站式的分布式解决方案



4.2.4 事务消息(阿里的RocketMQ支持事务消息)——2PC模型的一个落地方案
过程如下：
producer端：
(1) 发送Prepared消息到消息集群，同时拿到消息的地址，prepared状态的消息不会被consumer看到并消费；
(2) update DB（本地事务）； 
(3) 根据update DB结果成功或失败，根据之前拿到的消息的地址，修改消息的状态为commit或者rollback；


存在的问题：
1.业务侵入大：业务方需要提供事务回查接口；
2.消息中间件选型有要求：必须支持事务消息；
所以，这个方案不是一个普适的方案（不够优雅）

关键点：如果第3步修改消息状态的操作失败了怎么办？
RocketMQ会定期扫描消息集群中的事物消息，如果发现了Prepared消息，它会向消息发送端(生产者)确认，根据发送端设置的策略来决定是取消这条消息还是继续发送出去（业务方提供事务是否成功的回查接口），这样就保证了消息发送与本地事务同时成功或同时失败;

与非事务消息方案的不同是：其实就是把“扫描消息表”这个事情，不让业务方做，而是消息中间件帮着做了

至于消息表，其实还是没有省掉。因为消息中间件要询问发送方，事物是否执行成功，还是需要一个“变相的本地消息表”，记录事物执行状态

注：凡是通过业务补偿，或是在业务应用层上做的分布式事务的玩法，基本都是两阶段提交或是两阶段提交的变种，换句话说，迄今为止，在应用层上解决事务问题，只有『两阶段提交』这样的方式，而在数据层解决事务问题，Paxos算法是不二只选。

4.2.5 本地消息（非事务消息，2PC更普适的解决方案，也叫可靠消息队列、最大努力交付Best-Effort Delivery）
（1）Producer端准备1张消息表，把update DB和insert message（通知消息）这2个操作，放在一个本地事务里面；
（2）准备一个后台定时任务，持续地把通知消息表中的message传送给消息中间件，在失败或不确定（未收到消息ACK）的情况下，都不断重试重传，即在不确定成功的情况下，允许消息重复，但一定得保证通知消息发出去，顺序也不打乱；
（3）Consumer端准备一个消息判重表，处理过的消息，记在判重表里面，实现业务的幂等，然后把更新处理消息状态的操作和业务db操作放入本地事务，保证同时成功或失败；
通过上面3步，我们基本就解决了这里update db和发送网络消息这2个操作的原子性问题。

在这个方案里，只要第一步业务操作完成了，后续的关联业务就没有失败回滚的概念，只许成功，不许失败，这种依赖持续重试来保证可靠性的解决方案，有一个专门的名字叫“最大努力交付”，譬如TCP协议中未收到ACK应答自动重新发包的可靠性保障就属于最大努力交付。而可靠事件队列还有一种更普通的形式，被称为“最大努力一次提交”（BestEffort1PC），指的是将最有可能出错的业务以本地事务的方式完成后，采用不断重试的方式（不限于消息系统）来促使同一个分布式事务中的其他关联业务全部完成。

场景实例：


整个流程如下：
1）最终用户向Fenix’sBookstore发送交易请求：购买一本价值100元的《深入理解Java虚拟机》。

2）Fenix’sBookstore首先应对用户账号扣款、商家账号收款、库存商品出库这三个操作有一个出错概率的先验评估，根据出错概率的大小来安排它们的操作顺序，这种评估一般直接体现在程序代码中，一些大型系统也可能会实现动态排序。譬如，根据统计，最有可能出现的交易异常是用户购买了商品，但是不同意扣款，或者账号余额不足；其次是仓库发现商品库存不够，无法发货；风险最低的是收款，如果到了商家收款环节，一般就不会出什么意外了。那最容易出错的就应该最先进行，即：账号扣款→仓库出库→商家收款。

3）账号服务进行扣款业务，如扣款成功，则在自己的数据库建立一张消息表，里面存入一条消息：“事务ID：某UUID，扣款：100元（状态：已完成），仓库出库《深入理解Java虚拟机》：1本（状态：进行中），某商家收款：100元（状态：进行中）”。注意，这个步骤中“扣款业务”和“写入消息”是使用同一个本地事务写入账号服务自己的数据库的。

4）在系统中建立一个消息服务，定时轮询消息表，将状态是“进行中”的消息同时发送到库存和商家服务节点中去（也可以串行地发，即一个成功后再发送另一个，但在我们讨论的场景中没必要）。这时候可能产生以下几种情况。
*  商家和仓库服务都成功完成了收款和出库工作，向用户账号服务器返回执行结果，用户账号服务把消息状态从“进行中”更新为“已完成”。整个事务顺利结束，达到最终一致性的状态。
* 商家或仓库服务中至少有一个因网络原因，未能收到来自用户账号服务的消息。此时，由于用户账号服务器中存储的消息状态一直处于“进行中”，所以消息服务器将在每次轮询的时候持续地向未响应的服务重复发送消息，这个操作的可重复性决定了所有被消息服务器发送的消息都必须具备幂等性，通常的设计是让消息带上一个唯一的事务ID，以保证一个事务中的出库、收款动作会且只会被处理一次。
* 商家或仓库服务有某个或全部无法完成工作，譬如仓库发现《深入理解Java虚拟机》没有库存了，此时，仍然是持续自动重发消息，直至操作成功（譬如补充了新库存），或者被人工介入为止。由此可见，可靠事件队列只要第一步业务完成了，后续就没有失败回滚的概念，只许成功，不许失败。
* 商家和仓库服务成功完成了收款和出库工作，但回复的应答消息因网络原因丢失，此时，用户账号服务仍会重新发出下一条消息，但因操作具备幂等性，所以不会导致重复出库和收款，只会导致商家、仓库服务器重新发送一条应答消息，此过程持续自动重复直至双方网络通信恢复正常
* 也有一些支持分布式事务的消息框架，如RocketMQ，原生就支持分布式事务操作，这时候上述第二、四种情况也可以交由消息框架来保障。

以上这种依靠持续重试来保证可靠性的解决方案在计算机的其他领域中已被频繁使用，也有了专门的名字——“最大努力交付”（BestEffortDelivery），譬如TCP协议中未收到ACK应答自动重新发包的可靠性保障就属于最大努力交付。而可靠事件队列还有一种更普通的形式，被称为“最大努力一次提交”（BestEffort1PC），指的是将最有可能出错的业务以本地事务的方式完成后，采用不断重试的方式（不限于消息系统）来促使同一个分布式事务中的其他关联业务全部完成


4.2.6 事务状态表
一种类似 TCC 的事务解决方案，借助事务状态表来实现。假设要在一个分布式事务中实现调用 repo-service 扣减库存、调用order-service 生成订单两个过程。在这种方案中，协调者 shopping-service 维护一张如下的事务状态表
分布式事务 ID
事务内容
事务状态
global_trx_id_1
操作 1：调用 repo-service 扣减库存 操作 2：调用 order-service 生成订单
状态 1：初始 状态 2：操作 1 成功 状态 3：操作 1、2 成功
初始状态为 1，每成功调用一个服务则更新一次状态，最后所有的服务调用成功，状态更新到 3。
具体实现方法可以通过动态代理的方式，对参与全局事务的方法进行拦截，然后将方法调用的山下文信息写入这张表，并根据方法调用的结果更新事务状态。
同时，需要启动一个后台任务，扫描这张表中事务的状态，如果一个分布式事务一直（设置一个事务周期阈值）未到状态 3，说明这条事务没有成功执行，于是可以重新调用 repo-service 扣减库存、调用 order-service 生成订单。直至所有的调用成功，事务状态到 3。
如果多次重试仍未使得状态到 3，可以将事务状态置为 error，通过人工介入进行干预。
由于存在服务的调用重试，因此每个服务的接口要根据全局的分布式事务 ID 做幂等

注意：
不同于传统事务的强一致性保证，柔性事务需要经历一个中间状态，才到达成事务的最终一致性。有某些特殊情况下，这个中间状态会持续非常长的时间，甚至需要人工主动介入才能实现最终一致性。因此，我们需要通过定期对账机制来进行排查，在必要的时候发起人工主动介入流程，修复不一致的数据。事实上，在任何柔性事务的实现中，每日对账都是必不可少的数据安全保障性手段


附录：
1. [以万亿级企业分布式事务架构设计为例，打造百万年薪架构师顶级思维模型](https://qcs.h5.xeknow.com/s/1c7ugX)

2. [万亿级企业分布式事务架构设计的全攻略实践](https://apprz8zztzy8571.h5.xiaoeknow.com/v1/course/alive/l_60517586e4b05a6195c09b62?type=2&pro_id=p_604b45f0e4b05a6195bf5f12)

3. [万亿级企业分布式事务架构设计真实案例篇](https://apprz8zztzy8571.h5.xiaoeknow.com/v1/course/alive/l_6051752ee4b07f4195006ec2?type=2&pro_id=p_604b45f0e4b05a6195bf5f12)