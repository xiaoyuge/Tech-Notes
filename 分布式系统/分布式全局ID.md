# **分布式全局ID**

在复杂分布式系统中，往往需要对大量的数据和消息进行唯一标识，比如对数据分库分表后需要有一个唯一ID来标识一条数据或消息，数据库的自增ID显然不能满足需求。此时一个能够生成全局唯一ID的系统是非常必要的。业务系统对全局唯一ID的要求主要有如下这些：
1）全局唯一性：不能出现重复的ID号；
2)   趋势递增：由于多数RDBMS使用B-tree的数据结构来存储索引数据，在主键的选择上面我们应该尽量使用有序的主键保证写入性能；
3）单调递增：保证下一个ID一定大于上一个ID；
4)   信息安全：如果ID是连续的，容易猜测并生成ID进行业务信息扒取或者可以根据ID值估计业务量，因此在一些应用场景下，会需要ID无规律、不规则；
其中3和4的需求是互斥的，无法用同一个方案满足。

常见方法介绍：

## **UUID**

UUID(Universally Unique Identifier)的标准型式包含32个16进制数字，以连字号分为五段，形式为8-4-4-4-12的36个字符，示例：550e8400-e29b-41d4-a716-446655440000

优点:

1) 性能非常高：本地生成，没有网络消耗；

缺点：

1) 不易于存储：UUID太长，16字节128位（2个16进制数用1个字节），通常以36长度的字符串表示，很多场景不适用；
2) 信息不安全：基于MAC地址生成UUID的算法可能会造成MAC地址泄露；
3) ID作为主键时在特定的环境会存在一些问题，比如做DB主键的场景下，UUID就非常不适用。MySQL官方有明确的建议主键要尽量越短越好，36个字符长度的UUID不符合要求。另外，对MySQL索引不利，如果作为数据库主键，在InnoDB引擎下，**UUID的无序性可能会引起数据位置频繁变动，严重影响性能**，因为Mysql索引是B+树结构，数据记录都是保存在B+树的叶子节点（大小相当于磁盘的页）上，如果主键是有序的，它会把每一条记录都存储在上一条记录的后面，但如果主键使用的是无序的数值，例如UUID，这样在插入数据时Innodb无法简单地把新的数据插入到最后，而是需要为这条数据寻找合适的位置，这就额外增加了时间；

## **数据库生成方案**

以MySQL为例，利用给字段设置auto_increment_increment和auto_increment_offset来保证ID自增，每次业务使用下列SQL读写MySQL得到ID号：

```sql
begin; 
REPLACE INTO Tickets64 (stub) VALUES ('a'); //stub字段需要建unique索引
SELECT LAST_INSERT_ID(); 
commit;
```

PS：这里用到了replace into 的特性，用insert不合适，因为会在数据库生成务必要的非常多的垃圾数据：

1. replace into 首先尝试插入数据到表中，如果发现表中已经有此行数据（根据主键或者唯一索引判断）则先删除此行数据，然后插入新的数据。
2. 否则，直接插入新数据。
插入数据的表必须有主键或者是唯一索引！否则的话，replace into 会直接插入数据，这将导致表中出现重复的数据

该方案优点： 

1. 非常简单，利用现有数据库系统的功能实现，成本小，有DBA专业维护;
2. ID号单调递增，可以实现一些对ID有特殊要求的业务;

缺点：

1. 强依赖DB，且DB是单点，当DB异常时整个系统不可用，属于致命问题。配置主从复制可以尽可能的增加可用性，但是数据一致性在特殊情况下难以保证。主从切换时的不一致可能会导致重复发号;
2. ID发号性能瓶颈限制在单台MySQL的读写性能（每次发号都要访问数据库，发号服务的QPS上不去）；

基于上述问题，一个可能的改进方案：

针对单台机器的单点问题，我们可以多部署几台DB机器，但要保证不同DB发号不重复，因此给每台机器设置不同的发号初始值，同时步长和机器数相等。比如，假设我们要**部署N台机器，步长需设置为N，每台的初始值依次为0,1,2...N-1**，同时DB前放置多台发号Server进行负载均衡，每个server都可以访问任意一个DB

这种架构能够满足性能的需求，且解决了DB单点问题，某台DB宕机不影响继续发号，但有以下几个缺点:

1. 发号Sever水平扩展还行，但DB的水平扩展比较困难，比如定义好了步长和机器台数之后，如果要添加机器该怎么做？当机器数很多的时候，系统水平扩展方案复杂难以实现；
2. ID没有了单调递增的特性，只能趋势递增，这个缺点对于一般业务需求不是很重要，可以容忍；
3. 数据库压力还是很大，每次获取ID都得读写一次数据库，只能靠堆机器来提高性能；——我觉得这个是关键，每次发号都要进行DB访问其实是没必要的

基于以上问题，可以进一步优化，思路是

1. 原方案每次获取ID都得读写一次数据库，造成数据库压力大，可以改为发号Server批量获取，每次获取一个segment(step决定大小)号段的值。用完之后再去数据库获取新的号段，可以大大的减轻数据库的压力；
2. 各个业务不同的发号需求用biz_tag字段来区分，每个biz-tag的ID获取相互隔离，互不影响。如果以后有性能需求需要对数据库扩容，不需要上述描述的复杂的扩容操作，只需要对biz_tag分库分表就行;

数据库设计字段如下

| field   | Type | Null | Key | Default|
|:-----:|:-----:|:-----:|:-----:|:-----:|
| biz_tag | varchar(128) | NO   | PRI |
|max_id      | bigint(20)   | NO   |     | 1
|step        | int(11)      | NO   |     | NULL
|desc        | varchar(256) | YES  |     | NULL    
|update_time | timestamp    | NO   |     | CURRENT_TIMESTAMP | on update CURRENT_TIMESTAMP |          

biz_tag用来区分业务，max_id表示该biz_tag目前所被分配的ID号段的最大值，step表示每次分配的号段长度。原来获取ID每次都需要写数据库，现在只需要把step设置得足够大，比如1000。那么只有当1000个号被消耗完了之后才会去重新读写一次数据库。读写数据库的频率从1减小到了1/step

DB前部署多个发号server进行并发取号段，sql如下：

```sql
Begin 
UPDATE table SET max_id = max_id + step WHERE biz_tag=xxx 
SELECT biz_tag, max_id, step FROM table WHERE biz_tag=xxx 
Commit
```

假设有A、B和C三台server，A是1~1000的号段，当这个号段用完时，会去加载另一个长度为step=1000的号段，假设另外两台号段都没有更新，这个时候A Server新加载的号段就应该是3001~4000。同时数据库对应的biz_tag这条数据的max_id会从3000被更新成4000

优点：

1. 发号Sever服务可以很方便的线性扩展，性能完全能够支撑大多数业务场景；
2. ID号码是趋势递增的，满足上述数据库存储的主键要求；
3. 容灾性高：发号Server服务内部有号段缓存，即使DB宕机，短时间内仍能正常对外提供服务；
4. 可以自定义max_id的大小，非常方便业务从原有的ID方式上迁移过来；

缺点：

1. ID号码不够随机，能够泄露发号数量的信息，不太安全;
2. 当号段使用完之后还是可能会hang在更新数据库的I/O上，响应时间数据会出现偶尔的尖刺；
3. DB宕机长时间不恢复会造成整个系统不可用；

PS：这里有一个点需要注意一下，如果发号server宕机了怎么办？
宕机后重启的时候，可以让发号server都去重新查询一下DB，重新获取一个新的号段，但可能造成的一个问题是重启之前内存中已经预加载的号段可能有部分号是没有发出去的，这样就可能会造成ID空洞

另外，针对上述缺点2，即每次号段用完重新获取新号段的可能hang住问题，详细解释一下原因就是：
原发号服务取号段的时机是在号段消耗完的时候进行的，也就意味着号段临界点的ID下发时间取决于下一次从DB取回号段的时间，并且在这期间进来的请求也会因为DB号段没有取回来，导致线程阻塞。如果请求DB的网络和DB的性能稳定，这种情况对系统的影响是不大的，但是假如取DB的时候网络发生抖动，或者DB发生慢查询就会导致整个系统的响应时间变慢。

针对上述问题和原因，我们可以采取提前预取号段的方法进行优化，主要思路是：

1. 当号段消费到某个点时就异步的把下一个号段加载到内存中。而不需要等到号段用尽的时候才去更新号段。具体来说就是采用双buffer的方式，发号服务内部有两个号段缓存区segment。当前号段已下发10%时（可配置），如果下一个号段未更新，则另启一个更新线程去更新下一个号段。当前号段全部下发完后，如果下个号段准备好了则切换到下个号段为当前segment接着下发，循环往复。通常推荐segment长度设置为服务高峰期发号QPS的600倍（10分钟），这样即使DB宕机，Leaf仍能持续发号10-20分钟不受影响;

2. 对于缺点3的『DB可用性』问题，可以采用master-slave模式，比如一主两从的方式，同时分机房部署，Master和Slave之间采用半同步方式同步数据，同时使用数据库中间件做主从切换。当然这种方案在一些情况会退化成异步模式，甚至在非常极端情况下仍然会造成数据不一致的情况，但是出现的概率非常小；

上述方案可以生成趋势递增的的ID，这也导致ID是可计算的，不适用于一些敏感业务场景，为避免这个问题，可以继续改进一下生成ID的值，比如如下的snowflake方案

## **Snowflake方案**

snowflake方案的bit位设计，即是“1+41+10+12”的方式组装ID号，第一位不用，固定为0，后面41位是时间戳，接着10位是workerId，最后12位是序列号

对于workerID的分配，当服务集群数量较小的情况下，完全可以手动配置。如果发号服务规模较大，手动配置成本太高，可以使用Zookeeper持久顺序节点的特性自动对snowflake节点配置wokerID。发号服务是按照下面几个步骤启动的：

1. 启动发号服务，连接Zookeeper，在leaf_forever父节点下检查自己是否已经注册过（是否有创建自己的顺序子节点）；
2. 如果有注册过直接取回自己的workerID（zk顺序节点生成的int类型ID号），启动服务;
3. 如果没有注册过，就在该父节点下面创建一个持久顺序节点leaf_forever/发号服务name-？，创建成功后取回顺序号当做自己的workerID号，启动服务;

除了每次会去ZK拿数据以外，也会在本机文件系统上缓存一个workerID文件。当ZooKeeper出现问题，恰好机器出现问题需要重启时，能保证服务能够正常启动。这样做到了对三方组件的弱依赖，一定程度上提高了SLA。

### **解决时钟问题**

因为这种方案依赖时间，如果机器的时钟发生了回拨，那么就会有可能生成重复的ID号，需要解决时钟回退的问题，解决方案是，服务启动时首先检查自己是否写过ZooKeeper leaf_forever节点：

1. 若写过，则用自身系统时间与leaf_forever/${self}节点记录时间做比较，**若小于** leaf_forever/${self}时间则认为机器时间发生了大步长回拨，服务启动失败并报警；
2. 若未写过，证明是新服务节点，直接创建持久节点leaf_forever/${self}并写入自身系统时间，接下来综合对比其余Leaf节点的系统时间来判断自身系统时间是否准确，具体做法是取leaf_temporary下的所有临时节点(所有运行中的Leaf-snowflake节点)的服务IP：Port，然后通过RPC请求得到所有节点的系统时间，计算sum(time)/nodeSize；
3. 若abs( 系统时间-sum(time)/nodeSize ) < 阈值，认为当前系统时间准确，正常启动服务，同时写临时节点leaf_temporary/${self} 维持租约；
4. 否则认为本机系统时间发生大步长偏移，启动失败并报警；
5. 每隔一段时间(3s)上报自身系统时间写入leaf_forever/${self}；

### **snowflake算法原理**

snowflake生产的ID是一个18位的long型数字，二进制结构表示如下(每部分用-分开):

0 - 00000000 00000000 00000000 00000000 00000000 0 - 00000 - 00000 - 00000000 0000

第一位未使用，接下来的41位为毫秒级时间(41位的长度可以使用69年，从1970-01-01 08:00:00)，然后是5位datacenterId（最大支持2^5＝32个，二进制表示从00000-11111，也即是十进制0-31），和5位workerId（最大支持2^5＝32个，原理同datacenterId），所以datacenterId*workerId最多支持部署1024个节点，最后12位是毫秒内的计数（12位的计数顺序号支持每个节点每毫秒产生2^12＝4096个ID序号）
所有位数加起来共64位，恰好是一个Long型（转换为字符串长度为18）.
单台机器实例，通过时间戳保证前41位是唯一的，分布式系统多台机器实例下，通过对每个机器实例分配不同的datacenterId和workerId避免中间的10位碰撞。最后12位每毫秒从0递增生产ID

snowflake满足了以下个要求： 

1. 只用64位就能达到要求，而无需128位（UUID）； 
2. 考虑到排序的要求，标识的排序跟时间上基本能保持一致；
3. 满足了预期服务时间，即在多少年内此算法适用；