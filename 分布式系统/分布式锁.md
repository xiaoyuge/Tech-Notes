
通过参考多线程和多进程环境下的锁，可以发现锁的实现有很多共通之处，它们都需要满足一些最基本的条件：\
1） 需要有存储锁的空间，并且锁的空间是可以访问到的:在多线程中是内存，在多进程中是内存或者磁盘。更重要的是，这个空间是可以被访问到的。多线程中，不同的线程都可以访问到堆中的成员变量。在多进程中，不同的进程可以访问到共享内存中的数据或者存储在磁盘中的文件。但是在分布式环境中，不同的主机很难访问对方的内存或磁盘。这就需要一个都能访问到的外部空间来作为存储空间；
2） 锁需要被唯一标识:不同的共享资源，必然需要用不同的锁进行保护，因此相应的锁必须有唯一的标识。在多线程环境中，锁可以是一个对象，那么对这个对象的引用便是这个唯一标识。多进程环境中，信号量在共享内存中也是由引用来作为唯一的标识。如果不在内存中，可以用硬盘中的文件名作为唯一标识（比如有名信号量），因此，在分布式环境中，只要给这个锁设定一个名称，并且保证这个名称是全局唯一的，那么就可以作为唯一标识;
3） 锁要有至少两种状态：为了给临界区加锁和解锁，需要存储两种不同的状态，如ReentrantLock中的status，0表示没有线程竞争，大于0表示有线程竞争；信号量大于0表示可以进入临界区，小于等于0则表示需要被阻塞。因此只要在分布式环境中，锁的状态有两种或以上：如有锁、没锁；存在、不存在等等，均可以实现

另外，分布式锁还要解决如下几个问题：
1.锁状态判断的原子性；
2.网络断开或服务节点宕机，锁释放问题；
3.如何保证节点释放的锁是自己加的锁；
4.可重入：线程中的可重入，指的是外层函数获得锁之后，内层也可以获得锁，ReentrantLock和synchronized都是可重入锁；衍生到分布式环境中，一般仍然指的是线程的可重入，在绝大多数分布式环境中，都要求分布式锁是可重入的；
5.惊群效应（Herd Effect）：在分布式锁中，惊群效应指的是，在有多个请求等待获取锁的时候，一旦占有锁的线程释放之后，如果所有等待的方都同时被唤醒，尝试抢占锁。但是这样的情况会造成比较大的开销，那么在实现分布式锁的时候，应该尽量避免惊群效应的产生;
6.公平锁和非公平锁：不同的需求，可能需要不同的分布式锁。非公平锁普遍比公平锁开销小。但是业务需求如果必须要锁的竞争者按顺序获得锁，那么就需要实现公平锁;
7.阻塞锁和自旋锁：针对不同的使用场景，阻塞锁和自旋锁的效率也会有所不同。阻塞锁会有上下文切换，如果并发量比较高且临界区的操作耗时比较短，那么造成的性能开销就比较大了。但是如果临界区操作耗时比较长，一直保持自旋，也会对CPU造成更大的负荷;
PS：自旋锁只是将当前线程不停地执行循环体，不进行线程状态的改变，所以响应速度更快。但当线程数不停增加时，性能下降明显，因为每个线程都需要执行，占用CPU时间；阻塞锁与自旋锁不同，改变了线程的运行状态，是让线程进入阻塞状态进行等待，当获得相应的信号（唤醒，时间） 时，才可以进入线程的准备就绪状态，准备就绪状态的所有线程，通过竞争，进入运行状态；
PS：关键看临界区（同步代码块）的执行耗时，如果并发量不是特别高且临界区耗时比较高，适合自旋，否则适合阻塞；

常见解决方案
1）ZooKeeper的实现
ZooKeeper（以下简称“ZK”）中有一种节点叫做顺序节点，假如我们在/lock/目录下创建3个节点，ZK集群会按照发起创建的顺序来创建节点，节点分别为/lock/0000000001、/lock/0000000002、/lock/0000000003。ZK中还有一种名为临时节点的节点，临时节点由某个客户端创建，当客户端与ZK集群断开连接，则该节点自动被删除。EPHEMERAL_SEQUENTIAL为临时顺序节点，根据ZK中节点是否存在，可以作为分布式锁的锁状态，以此来实现一个分布式锁，下面是分布式锁的基本逻辑： 
1）客户端调用create()方法创建名为“/dlm-locks/lockname/lock-”的临时顺序节点；
2）客户端调用getChildren(“lockname”)方法来获取所有已经创建的子节点；
3）客户端获取到所有子节点path之后，如果发现自己在步骤1中创建的节点是所有节点中序号最小的，那么就认为这个客户端获得了锁；
4）如果创建的节点不是所有节点中需要最小的，那么则监视比自己创建节点的序列号小的最大的节点，进入等待。直到下次监视的子节点变更的时候，再进行子节点的获取，判断是否获取锁；
5）释放锁的过程相对比较简单，就是删除自己创建的那个子节点即可，不过也仍需要考虑删除节点失败等异常情况；

该方案对上述问题的解决：
1.锁状态的原子性由zookeeper的创建节点操作来保证；
2.zk服务端是多节点冗余，因此一个服务节点宕机应该不会影响使用；不太确定网络故障导致客户端和zk服务节点断开或客户端宕机的情况下，是否会删除节点释放锁？？
3.每个客户端加的锁都有自己的顺序节点编号唯一区分，因此不会释放错锁；
4.利用ThreadLocal存储进入的次数，每次加锁次数加1，每次解锁次数减1。如果判断出是当前线程持有锁，就不用走获取锁的流程；
5.因为采取的是临时顺序节点的方式，因此每个客户端只会监视比自己节点序号小的最大节点状态变化即可，因此每当有客户端释放锁，只会有一个客户端被通知，避免了惊群效应；
6.临时顺序节点实现的是公平锁；
7.阻塞锁；

有了zookeeper的一致性文件系统，锁的问题变得容易。锁服务可以分为两类，一个是保持独占（排他锁），另一个是控制顺序（公平锁）

1.排它锁：
1. 在Java开发中，有两种常见的方式来定义锁，分别是synchronized和ReentrantLock。ZooKeeper通过其上的一个数据节点表示一个锁，例如创建临时节点/exclusive/lock就可以表示一个锁。
2. 在获取排他锁时，所有客户端节点都会调用create()接口，在/exclusive下创建节点/exclusive/lock。ZK保证在所有客户端中，只有一个客户端能够创建成功，就可以认为该客户端获取到锁。同时，所有没有获取到锁的客户端需要在/exclusive/lock节点注册一个Watcher监听，以便实时监听lock节点的变更情况。
3. 在下面的两种情况下，可能会释放锁。
当前获取锁的客户端机器发生宕机，ZK上的临时节点会被移除
正常执行完业务逻辑后，客户端会主动删除创建的临时节点
无论在哪种情况下移除了lock节点，ZK都会通知所有注册了Watcher监听的客户端。这些客户端在接收到通知后，重新发起分布式锁获取，即重复“获取锁”过程。

问题：
对于上述的场景，当大量客户端去竞争锁的时候，会发生“惊群”效应。惊群效应指的是在分布式锁竞争的过程中，大量的"Watcher通知"和“创建/exclusive/lock”两个操作重复运行，并且绝大多数运行结果都创建节点失败，从而继续等待下一次通知。若在集群规模较大的情况下，会对ZooKeeper服务器以及客户端服务器造成巨大的性能影响和网络冲击。
改进措施：
1）客户端调用create()方法创建名为“/exclusive/lock-”节点，这里节点类型创建类型设置为EPHEMERAL_SEQUENTIAL；
2）客户端调用getChildren(“/exclusive”)方法来获取所有已经创建的子节点，若发现自身的节点序号是/exclusive目录下最小的点，则获得锁；否则，监视比自己创建的节点的序列号小的最大节点，进入等待。
3）这样，避免了"惊群效应"，多个客户端共同等待锁，锁释放时只有一个客户端会被唤醒
PS：这里其实实现了一个公平锁


2.读写锁（读锁共享，写锁互斥）
同样使用ZK上的数据节点表示一个锁，是一个类似于“/shared_lock/[Hostname]-请求类型-序号”的临时顺序节点，例如/shared_lock/192.168.0.1-R-0000000001就代表了共享锁。
在获取共享锁时，所有客户端都会到/shared_lock节点下创建一个临时顺序节点，若是读请求，则创建/shared_lock/192.168.0.1-R-0000000001；若是写请求，则创建/shared_lock/192.168.0.1-W-0000000001。
根据共享锁的定义，不同的事务都可以同时对同一个数据对象进行读取操作，而更新操作必须在当前没有任何事务进行读写操作的情况下进行。基于这个原则，我们来看看如何通过ZK的节点来确定分布式读写顺序，大致可以分为4个步骤：
(1)创建完节点后，获取/shared_lock节点下的所有子节点，并对该节点注册子节点变更的Watcher监听。
(2)确定自己的节点序号在所有子节点中的顺序。
(3)对于读请求：若没有比自己序号小的子节点，或是所有比自己序号小的请求都是读请求，那么表明自己已经成功获取到了共享锁，同时开始执行读取逻辑；若比自己序号小的子节点中有写请求，那么需要进入等待。
对于写请求：若自己是序号最小的子节点，则执行读取逻辑；否则进入等待。
(4)接收到Watcher通知后，重复步骤(1)

问题：
对于上述的场景，当大量客户端去竞争锁的时候，会发生“惊群”效应。惊群效应指的是在分布式锁竞争的过程中，大量的"Watcher通知"和“子节点列表获取”两个操作重复运行，并且绝大多数运行结果都判断自己并非是序号最小的节点，从而继续等待下一次通知。若在集群规模较大的情况下，会对ZooKeeper服务器以及客户端服务器造成巨大的性能影响和网络冲击。
改进措施：
上面提到的共享锁实现，从整体思路上来说完全正确。这里的主要改动在于：每个锁竞争者只需要关注shared_lock节点下序号比自己小的那个节点是否存在即可，具体改进如下：
1.客户端调用create()方法创建一个类似于“shared_lock/[Hostname]-请求类型-序号”的临时顺序节点。
2.客户端调用getChildren()接口来获取所有已经创建的子节点列表，注意，这里不注册任何Watcher。
3.若无法获取共享锁，调用exist()对比自己小的节点注册Watcher。
4.读请求：向比自己序号小的最后一个写请求节点注册Watcher监听。
5.写请求：向比自己序号小的最后一个节点注册Watcher监听。
6.等待Watcher通知，继续进入步骤2。

开源的基于ZK的Menagerie的源码就是一个典型的例子：https://github.com/sfines/menagerie
Menagerie中的lock首先实现了可重入锁，利用ThreadLocal存储进入的次数，每次加锁次数加1，每次解锁次数减1。如果判断出是当前线程持有锁，就不用走获取锁的流程。
通过tryAcquireDistributed方法尝试获取锁，循环判断前序节点是否存在，如果存在则监视该节点并且返回获取失败。如果前序节点不存在，则再判断更前一个节点。如果判断出自己是第一个节点，则返回获取成功。
为了在别的线程占有锁的时候阻塞，代码中使用JUC的condition来完成。如果获取尝试锁失败，则进入等待且放弃localLock，等待前序节点唤醒。而localLock是一个本地的公平锁，使得condition可以公平的进行唤醒，配合循环判断前序节点，实现了一个公平锁。
这种实现方式非常类似于ReentrantLock的CHL队列，而且zk的临时节点可以直接避免网络断开或主机宕机，锁状态无法清除的问题，顺序节点可以避免惊群效应。这些特性都使得利用ZK实现分布式锁成为了最普遍的方案之一。

2）Redis实现

（1）Redis单机方案：用Redis来实现分布式锁最简单的方式就是在实例里创建一个键值（通过setnx key value EX/PX和del命令来获取锁、锁超时释放和主动释放锁），创建出来的键值一般都是有一个超时时间的（这个是Redis自带的超时特性），所以每个锁最终都会释放，而当一个客户端想要释放锁时，它只需要删除这个键值即可；
存在的问题：
* 锁误删除：假设线程A获取到了锁，并设置了锁过期时间为30秒，但线程A的执行时间超过了30秒，锁过期自动释放，此时线程B获取到了锁。随后线程A执行完用del命令释放锁，但此时线程B加的锁还未执行完，线程A实际释放的是线程B加的锁；解决办法：在加锁的时候把线程id当做value，每个线程在删除锁之前，验证key对应的value是否是自己的线程id，保证每个线程只能释放自己加的锁；
* 超时解锁导致并发：假设线程A获取到了锁，并设置了锁过期时间为30秒，但线程A的执行时间超过了30秒，锁过期自动释放，此时线程B获取到了锁，此时线程A和线程B并发执行。解决办法：1.将过期时间设置的足够长，确保业务逻辑能够在锁自动超时释放前执行完；2.为获取锁的线程增加守护线程，为将要过期但未释放的锁增加有效时间；
* 锁不可重入：当线程在持有锁的情况下再次请求加锁，如果一个锁支持一个线程多次加锁，那么这个锁就是可重入的。如果一个不可重入锁被再次请求加锁，再次加锁会失败。解决办法：Redis可通过对每个线程加的锁进行重入计数（incr key=线程id），加锁时加1，解锁时减1，当计数归0时释放锁；
* 无法等待锁释放：解决办法：1.通过客户端轮询的方式，当未获取到锁时，等待一段时间重新尝试获取锁，直到重新获取锁或等待超时，这种方式比较消耗服务器资源，当并发量比较大时，会影响服务器性能；2.使用redis的发布订阅功能，当获取锁失败的时候，订阅锁释放消息，获取锁成功后释放时，发送锁释放消息；
* 单点故障：如果Redis的master节点宕机了怎么办？有人可能会说：加一个slave节点！在master宕机时用slave就行了！但是其实这个方案明显是不可行的，因为Redis的复制是异步的，考虑如下的场景：
            1.客户端A在master节点拿到了锁;
            2.master节点在把A创建的key写入slave之前宕机了;
            3.slave变成了master节点； 
            4.B也得到了和A持有的相同的锁（因为原来的slave里还没有A持有锁的信息）；

因此，在一个非分布式的、单点的、保证永不宕机的环境下，采用redis单实例实现分布式锁的正确方式，总结起来包括如下几个关键点：
1.死锁问题：通过setnx key tid_毫秒的unix时间戳 expire 实现加锁，expire时间尽量长保证业务逻辑能在锁自动释放前执行完；
2.错误释放锁问题：通过del key tid_毫秒的unix时间戳 实现释放锁，保证释放的是自己加的锁；
3.可重入锁问题：通过redis incr/decr tid_毫秒的unix时间戳 实现可重入锁计数，也可考虑在setnx的时候，value设置成一个map，分别存储加锁的id和加锁次数；
4.锁等待和重新获取问题：通过redis pub/sub实现监听锁释放；

（2）分布式版本的实现——RedLock
目前官方权威的用Redis实现分布式锁管理器的算法，称为RedLock
该算法的Java实现是Redisson，Redisson是一个官方推荐的Redis客户端并且实现了很多分布式的功能，源码：https://github.com/mrniko/redisson

在分布式版本的算法里我们假设我们有N个Redis master节点，这些节点都是完全独立的，我们不用任何复制或者其他隐含的分布式协调算法，我们已经描述了如何在单节点环境下安全地获取和释放锁。因此我们理所当然地应当用这个方法在每个单节点里来获取和释放锁。在我们的例子里面我们把N设成5，这个数字是一个相对比较合理的数值，因此我们需要在不同的计算机或者虚拟机上运行5个master节点来保证他们大多数情况下都不会同时宕机。一个客户端需要做如下操作来获取锁：
1.获取当前时间（单位是毫秒）；
2.轮流用相同的key和随机值在N个节点上请求锁，在这一步里，客户端在每个master上请求锁时，会有一个和锁的锁释放时间相比小的多的请求锁超时时间。比如如果锁自动释放时间是10秒钟，那每个节点锁请求的超时时间可能是5-50毫秒的范围，这个可以防止一个客户端在某个宕掉的master节点上阻塞过长时间，如果一个master节点不可用了，我们应该尽快尝试下一个master节点；
3.客户端计算第二步中获取锁所花的时间，只有当客户端在大多数master节点上成功获取了锁（在这里是3个），而且总共消耗的时间不超过锁释放时间，这个锁就认为是获取成功了；
4.如果锁获取成功了，那现在锁自动释放时间就是最初的锁释放时间减去之前获取锁所消耗的时间；
5.如果锁获取失败了，不管是因为获取成功的锁不超过一半（N/2+1)还是因为总消耗时间超过了锁释放时间，客户端都会到每个master节点上释放锁，即便是那些他认为没有获取成功的锁;