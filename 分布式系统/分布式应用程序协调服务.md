## **分布式应用程序协调服务（zookeeper）**

ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，提供了文件系统和通知机制，它具有以下特性：

1. 顺序一致性 ：从一个客户端发起的事务请求，最终都会严格按照其发起顺序被应用到Zookeeper中；
2. 原子性 ：所有事务请求的处理结果在整个集群中所有机器上都是一致的；不存在部分机器应用了该事务，而另一部分没有应用的情况；
3. 单一视图 ：所有客户端看到的服务端数据模型都是一致的；
4. 可靠性 ：一旦服务端成功应用了一个事务，则其引起的改变会一直保留，直到被另外一个事务所更改；
5. 实时性 ：一旦一个事务被成功应用后，Zookeeper可以保证客户端立即可以读取到这个事务变更后的最新状态的数据；

### **简单数据模型**

Zookeeper通过树形结构来存储数据，它由一系列被称为ZNode的数据节点组成，类似于常见的文件系统。不过和常见的文件系统不同，Zookeeper将数据全量存储在内存中，以此来实现高吞吐，减少访问延迟

![zk-simple-data-model](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/resources/zk-simple-data-model.png)

Zookeeper数据模型是由一系列基本数据单元 Znode (数据节点)组成的节点树，其中根节点为 / 。每个节点上都会保存自己的数据和节点信息
有四种类型的znode：

1. PERSISTENT-持久化目录节点：客户端与zookeeper断开连接后，该节点依旧存在；
2. PERSISTENT_SEQUENTIAL-持久化顺序编号目录节点：客户端与zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号；
3. EPHEMERAL-临时目录节点：生命周期与客户端会话绑定，一旦客户端会话失效（客户端与zookeeper断开连接不一定会话失效），该客户端创建的所有临时节点都会被移除；
4. EPHEMERAL_SEQUENTIAL-临时顺序编号目录节点：一旦客户端会话失效（客户端与zookeeper断开连接不一定会话失效），节点被删除，只是Zookeeper给该节点名称进行顺序编号；

### **zookeeper集群**

可以由一组Zookeeper服务构成Zookeeper集群，集群中每台机器都会单独在内存中维护自身的状态，并且每台机器之间都保持着通讯，只要集群中有半数机器能够正常工作，那么整个集群就可以正常提供服务

逻辑架构

![zk-cluster-logic-arch](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/resources/zk-cluster-logic-arch.png)

部署架构
部署方式主要有2种，同机房部署和跨机房部署

同机房部署一般在同一机房内部署5台虚机。因为集群中的机器会进行数据之间的同步，相对于跨机房而言，同机房机器彼此间的网络通信质量更加良好


同城三机房部署

对于比较重要的线上业务，需要保证其有接近100%的可用性。为了防止某个机房不可用(断电或其他故障引起的整个机房不可用)，这时需要进行跨机房部署。在跨机房部署中，我们一般采用同城三机房部署方式。因为集群中一般是5台机器，根据2:2:1的配置在3个机房中分别部署机器(Note:1. 这样无论哪个机房挂掉后，整个集群始终保持可用状态。 2. 除非特殊原因，强烈要求3机房同城部署)





PS：ZooKeeper集群保持可用性的条件为：集群中的机器必须保持过半或过半以上存活。为保持ZooKeeper集群的高可用率，设定ZooKeeper集群内的机器数为5台。这样，ZooKeeper集群挂掉2台后，仍然可以保持可用状态

Zookeeper集群中的机器分为以下三种角色：

1. Leader ：为客户端提供读写服务，并维护集群状态，它是由集群选举所产生的，所有的跟随者Follower与观察者Observer节点的写请求都会转交给Leader执行。Leader接受到一个写请求后，首先会发送给所有的Follower，统计Follower写入成功的数量，当有超过半数的Follower写入成功后，Leader就会认为这个写请求提交成功，通知所有的Follower commit这个写操作；

2. Follower ：为客户端提供读写服务（实际写请求是转发给leader），并定期向Leader汇报自己的节点状态。同时也参与写操作“过半写成功”的策略和Leader的选举；

3. Observer ：为客户端提供读写服务（实际写请求是转发给leader），并定期向Leader汇报自己的节点状态，**但不参与写操作“过半写成功”的策略和Leader的选举**，因此Observer可以在不影响写性能的情况下提升集群的读性能，Observer的主要作用是提高zookeeper集群的读性能。因为zookeeper的一个写操作是要经过半数以上的Follower确认才能够写成功的。那么当zookeeper集群中的节点越多时，zookeeper的写性能就越差。为了在提高zookeeper读性能（也就是支持更多的客户端连接）的同时又不影响zookeeper的写性能，zookeeper集群多了一个Observer的角色。Observer 虽然无投票权，但仍须同步 Leader 的数据从而在处理读请求时可以返回尽可能新的数据（不是强一致性的？）

PS：Follower/Observer 均可接受写请求，但不能直接处理，而需要将写请求转发给 Leader 处理

长连接和会话（session）
Zookeeper客户端通过TCP长连接连接到服务集群，会话(Session)从第一次连接开始就已经建立，之后通过心跳检测机制来保持有效的会话状态。通过这个连接，客户端可以发送请求并接收响应，同时也可以接收到Watch事件的通知
关于会话中另外一个核心的概念是sessionTimeOut(会话超时时间)，当由于网络故障或者客户端主动断开等原因，导致连接断开，此时只要在会话超时时间之内重新建立连接，则之前创建的会话依然有效
在ZooKeeper中，客户端和服务端建立连接后，会话随之建立，生成一个全局唯一的会话ID(Session ID)。服务器和客户端之间维持的是一个长连接，在SESSION_TIMEOUT时间内，服务器会确定客户端是否正常连接(客户端会定时向服务器发送heart_beat，服务器重置下次SESSION_TIMEOUT时间)。因此，在正常情况下，Session一直有效，并且ZK集群所有机器上都保存这个Session信息

ZAB协议（ZooKeeper Atomic Broadcast，ZooKeeper原子消息广播协议）
Zab协议有两种模式，分别是恢复模式和广播模式
1.恢复模式：当服务者启动或者领导者崩溃后，zab就进入恢复模式，当领导者被选出来，且大多数follower完成了和领导者的数据同步以后，恢复模式结束；
2.广播模式：通过该协议，Zookeepe基于主从模式的系统架构来保持集群中各个副本之间数据的一致性。具体如下：
Zookeeper使用一个单一的主进程来接收并处理客户端的所有事务请求，并采用原子广播协议将数据状态的变更以事务Proposal的形式广播到所有的副本进程上去。如下图


所有的事务请求必须由唯一的Leader服务来处理，Leader服务将事务请求转换为事务Proposal，并将该Proposal分发给集群中所有的Follower服务。如果有半数的Follower服务进行了正确的反馈，那么Leader就会再次向所有的Follower发出Commit消息，要求将前一个Proposal进行提交

ZAB协议的消息广播过程使用的是原子广播协议。在整个消息的广播过程中，Leader服务器会每个事物请求生成对应的Proposal，并为其分配一个全局唯一的递增的事务ID(ZXID)，之后再对其进行广播。具体过程如下（类似一个二阶段提交的过程）：
Leader服务会为每一个Follower服务器分配一个单独的队列，然后将事务Proposal依次放入队列中，并根据FIFO(先进先出)的策略进行消息发送。Follower服务在接收到Proposal后，会将其以事务日志的形式写入本地磁盘中，并在写入成功后反馈给Leader一个Ack响应。当Leader接收到超过半数Follower的Ack响应后，就会广播一个Commit消息给所有的Follower以通知其进行事务提交，之后Leader自身也会完成对事务的提交。而每一个Follower则在接收到Commit消息后，完成事务的提交，如下图：


ZooKeeper应用场景（能做什么）
1）命名服务   
在分布式系统中，通常需要一个全局唯一的名字，如生成全局唯一的订单号等，Zookeeper可以通过顺序节点的特性来生成全局唯一ID，从而可以对分布式系统提供命名服务

2）Master选举
分布式系统一个重要的模式就是主从模式(Master/Salves)，Zookeeper可以用于该模式下的Matser选举。可以让所有服务节点去竞争性地创建同一个ZNode，由于Zookeeper不能有路径相同的ZNode，必然只有一个服务节点能够创建成功，这样该服务节点就可以成为Master节点

3）配置管理（中心）   
程序总是需要配置的，如果程序分散部署在多台机器上，要逐个改变配置就变得困难。现在把这些配置全部放到zookeeper上去，保存在Zookeeper 的某个目录节点中，然后所有相关应用程序对这个目录节点进行监听，一旦配置信息发生变化，每个应用程序就会收到 Zookeeper的通知，然后从Zookeeper获取新的配置信息应用到系统中就好

4）集群管理   
所谓集群管理无在乎两点：是否有机器退出和加入、选举master。 
对于第一点，所有机器约定在父目录GroupMembers下创建临时目录节点，然后监听父目录节点的子节点变化消息。一旦有机器挂掉，该机器与 zookeeper的连接断开，其所创建的临时目录节点被删除，所有其他机器都收到通知：某个兄弟目录被删除，于是，所有人都知道：它下线了。
新机器加入也是类似，所有机器收到通知：新兄弟目录加入，highcount又有了
对于第二点，选举 master，我们稍微改变一下，所有机器创建临时顺序编号目录节点，每次选取编号最小的机器作为master就好

5）分布式锁
有了zookeeper的一致性文件系统，锁的问题变得容易。锁服务可以分为两类，一个是保持独占（排他锁），另一个是控制顺序（公平锁）

1.排它锁：

      1. 在Java开发中，有两种常见的方式来定义锁，分别是synchronized和ReentrantLock。ZooKeeper通过其上的一个数据节点表示一个锁，例如创建临时节点/exclusive/lock就可以表示一个锁。
      2. 在获取排他锁时，所有客户端节点都会调用create()接口，在/exclusive下创建节点/exclusive/lock。ZK保证在所有客户端中，只有一个客户端能够创建成功，就可以认为该客户端获取到锁。同时，所有没有获取到锁的客户端需要在/exclusive/lock节点注册一个Watcher监听，以便实时监听lock节点的变更情况。
      3. 在下面的两种情况下，可能会释放锁。
      当前获取锁的客户端机器发生宕机，ZK上的临时节点会被移除
      正常执行完业务逻辑后，客户端会主动删除创建的临时节点
      无论在哪种情况下移除了lock节点，ZK都会通知所有注册了Watcher监听的客户端。这些客户端在接收到通知后，重新发起分布式锁获取，即重复“获取锁”过程。

问题：
对于上述的场景，当大量客户端去竞争锁的时候，会发生“惊群”效应。惊群效应指的是在分布式锁竞争的过程中，大量的"Watcher通知"和“创建/exclusive/lock”两个操作重复运行，并且绝大多数运行结果都创建节点失败，从而继续等待下一次通知。若在集群规模较大的情况下，会对ZooKeeper服务器以及客户端服务器造成巨大的性能影响和网络冲击。

改进措施：
1）客户端调用create()方法创建名为“/exclusive/lock-”节点，这里节点类型创建类型设置为EPHEMERAL_SEQUENTIAL；
2）客户端调用getChildren(“/exclusive”)方法来获取所有已经创建的子节点，若发现自身的节点序号是/exclusive目录下最小的点，则获得锁；否则，监视比自己创建的节点的序列号小的最大节点，进入等待。
3）这样，避免了"惊群效应"，多个客户端共同等待锁，锁释放时只有一个客户端会被唤醒
PS：这里其实实现了一个公平锁


2.读写锁（读锁共享，写锁互斥）
同样使用ZK上的数据节点表示一个锁，是一个类似于“/shared_lock/[Hostname]-请求类型-序号”的临时顺序节点，例如/shared_lock/192.168.0.1-R-0000000001就代表了共享锁。
在获取共享锁时，所有客户端都会到/shared_lock节点下创建一个临时顺序节点，若是读请求，则创建/shared_lock/192.168.0.1-R-0000000001；若是写请求，则创建/shared_lock/192.168.0.1-W-0000000001。
根据共享锁的定义，不同的事务都可以同时对同一个数据对象进行读取操作，而更新操作必须在当前没有任何事务进行读写操作的情况下进行。基于这个原则，我们来看看如何通过ZK的节点来确定分布式读写顺序，大致可以分为4个步骤：
(1)创建完节点后，获取/shared_lock节点下的所有子节点，并对该节点注册子节点变更的Watcher监听。
(2)确定自己的节点序号在所有子节点中的顺序。
(3)对于读请求：若没有比自己序号小的子节点，或是所有比自己序号小的请求都是读请求，那么表明自己已经成功获取到了共享锁，同时开始执行读取逻辑；若比自己序号小的子节点中有写请求，那么需要进入等待。
对于写请求：若自己是序号最小的子节点，则执行读取逻辑；否则进入等待。
(4)接收到Watcher通知后，重复步骤(1)

问题：
对于上述的场景，当大量客户端去竞争锁的时候，会发生“惊群”效应。惊群效应指的是在分布式锁竞争的过程中，大量的"Watcher通知"和“子节点列表获取”两个操作重复运行，并且绝大多数运行结果都判断自己并非是序号最小的节点，从而继续等待下一次通知。若在集群规模较大的情况下，会对ZooKeeper服务器以及客户端服务器造成巨大的性能影响和网络冲击。
改进措施：
上面提到的共享锁实现，从整体思路上来说完全正确。这里的主要改动在于：每个锁竞争者只需要关注shared_lock节点下序号比自己小的那个节点是否存在即可，具体改进如下：
1.客户端调用create()方法创建一个类似于“shared_lock/[Hostname]-请求类型-序号”的临时顺序节点。
2.客户端调用getChildren()接口来获取所有已经创建的子节点列表，注意，这里不注册任何Watcher。
3.若无法获取共享锁，调用exist()对比自己小的节点注册Watcher。
4.读请求：向比自己序号小的最后一个写请求节点注册Watcher监听。
5.写请求：向比自己序号小的最后一个节点注册Watcher监听。
6.等待Watcher通知，继续进入步骤2。

5）队列管理
两种类型的队列：
1、同步队列，当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达。 
2、队列按照 FIFO 方式进行入队和出队操作。 
第一类，在约定目录下创建临时目录节点，监听节点数目是否是我们要求的数目 
第二类，和分布式锁服务中的控制时序场景基本原理一致，入列有编号，出列按编号

6）全局唯一ID（单调递增）
通过调用ZooKeeper节点创建的API接口可以创建一个顺序节点，并且在API返回值中会返回这个节点的完整名字。利用这个特性，就可以借助ZooKeeper来生成全局唯一的ID


如图图中，每个节点只有唯一的名字，每个名字即为全局唯一的ID

6）服务发现
服务发现就是服务或者应用之间互相定位的过程，它也并不是什么新鲜概念。这些服务发现方式可以让用户不必关心服务提供者的具体网络位置(IP地址、端口等)和配置步骤，只需要选择和连接即可使用这些服务。这种方式的实现首先需要有一个注册中心，ZK使用一个节点表示其服务注册中心，如图所示


/A: 在ZK上创建的根节点，在这个节点下创建新的节点，进而创建服务。
/A/service: 这个表示服务节点，其下有服务提供者和服务消费者两种类型的节点。
/A/service/providers: 这是服务提供者的根节点，其子节点代表了每个服务的真正提供者。
/A/service/consumers: 这是服务消费者的根节点，其子节点代表了每个服务的真正消费者。
服务提供者：
 服务提供者在初始化启动的时候，会首先在ZooKeeper的/A/service/providers节点下创建一个临时子节点，并写入自己的URL地址，这就代表该服务是service的一个提供者。
服务消费者：
 服务消费者会在启动的时候， 读取并订阅ZooKeeper上/A/service/providers节点下的所有子节点，并解析出所有提供者的URL地址来作为该服务地址列表，然后开始发起正常调用。同时，服务消费者还会在/A/service/consumers节点下创建一个临时节点，并写入自己的URL地址，这就代表了该服务是service的一个消费者。
变更通知：当某个服务提供者发生了变更，或者无法对外提供服务时。相应地，该临时节点也会在ZooKeeper上发生变化，服务的消费者也可以感受到对应的变化，如下图所示：


著名的CAP理论指出，一个分布式系统不可能同时满足C(一致性)、A(可用性)和P(分区容错性)。由于分区容错性在是分布式系统中必须要保证的，因此我们只能在A和C之间进行权衡。在此Zookeeper保证的是CP, 而Eureka则是AP

zookeeper保证cp
当向注册中心查询服务列表时，我们可以容忍注册中心返回的是几分钟以前的注册信息，但不能接受服务直接down掉不可用。也就是说，服务注册功能对可用性的要求要高于一致性。但是zk会出现这样一种情况，当master节点因为网络故障与其他节点失去联系时，剩余节点会重新进行leader选举。问题在于，选举leader的时间太长，30 ~ 120s, 且选举期间整个zk集群都是不可用的，这就导致在选举期间注册服务瘫痪。在云部署的环境下，因网络问题使得zk集群失去master节点是较大概率会发生的事，虽然服务能够最终恢复，但是漫长的选举时间导致的注册长期不可用是不能容忍的。


Eureka保证AP
Eureka看明白了这一点，因此在设计时就优先保证可用性。Eureka各个节点都是平等的，几个节点挂掉不会影响正常节点的工作，剩余的节点依然可以提供注册和查询服务。而Eureka的客户端在向某个Eureka注册或时如果发现连接失败，则会自动切换至其它节点，只要有一台Eureka还在，就能保证注册服务可用(保证可用性)，只不过查到的信息可能不是最新的(不保证强一致性)。除此之外，Eureka还有一种自我保护机制，如果在15分钟内超过85%的节点都没有正常的心跳，那么Eureka就认为客户端与注册中心出现了网络故障，此时会出现以下几种情况： 
1. Eureka不再从注册列表中移除因为长时间没收到心跳而应该过期的服务 
2. Eureka仍然能够接受新服务的注册和查询请求，但是不会被同步到其它节点上(即保证当前节点依然可用) 
3. 当网络稳定时，当前实例新的注册信息会被同步到其它节点中

Zookeeper VS Etcd
Etcd提供了更完善易用的watch功能：
ZooKeeper
1.watch children只能watch子节点，不能递归watch孙节点
2. watch children只能watch子节点的创建和删除，不能watch子节点值的变化
3.watch node只能对已经存在的node进行watch，对不存在的node需要watch existence
除了上述的这些不足以外，在其官网文档中自己也提到，在watch被触发和重新设置之间发生的事件将被丢弃，无法被捕捉。

Etcd
      Etcd支持单点watch，prefix watch以及ranged watch。
      和ZooKeeper不同，Etcd不会根据事件的不同而要求调用不同的watch API，三类watch的区别仅在于对key的处理不同：
1. 单点watch仅对传入的单个key进行watch；
2. ranged watch可以对传入的key的范围进行watch，范围内的key的事件都会被捕捉；
3. 而prefix则可以对所有具有给定prefix的key进行watch。

从功能的角度来看，Etcd只需要调用一次watch操作就可以捕捉所有的事件，相比ZooKeeper大大简化了客户端开发者的工作量。
 ZooKeeper的watch获得的channel只能使用一次，而Etcd的watch获得的channel可以被复用，新的事件通知会被不断推送进来，而无需客户端重复进行watch，这种行为也更符合我们对go channel的预期。
ZooKeeper对事件丢失的问题没有解决办法。Etcd则提供了版本号帮助客户端尽量捕捉每一次变化。要注意的是每一次变化都会产生一个新的版本号，而这些版本不会被永久保留。Etcd会根据其版本留存策略定时将超出阈值的旧版本从版本历史中清除