# **分布式服务注册与发现**
服务注册中心到底选用CP模型还是AP模型，还是要依照业务场景进行决定
- 如果对数据一致性要求较高，且可以容忍一定时间的不可用，就选用CP模型
- 如果可以容忍一定时延的数据不一致性，但不能容忍不可用现象发生，则要选用AP模型

## **Eureka**
Eureka的选择是优先**保证高可用性**，相对牺牲系统中服务状态的一致性。

**Eureka的各个节点间采用异步复制来交换服务注册信息**，当有新服务注册进来时，并不需要等待信息在其他节点复制完成，而是马上在该服务发现节点宣告服务可见，只是不保证在其他节点上多长时间后才会可见。

同时，当有旧的服务发生变动，譬如下线或者断网，只会由超时机制来控制何时从哪一个服务注册表中移除，变动信息不会实时同步给所有服务端与客户端。

这样的设计使得**不论是Eureka的服务端还是客户端，都能够持有自己的服务注册表缓存**，并以TTL（TimetoLive）机制来进行更新，哪怕服务注册中心完全崩溃，客户端仍然可以维持最低限度的可用。

Eureka的服务发现模型适合于节点关系相对固定、服务一般不会频繁上下线的系统，以较小的同步代价换取了最高的可用性

Eureka能够选择这种模型的底气在于万一客户端拿到了已经发生变动的错误地址，也**能够通过Ribbon和Hystrix模块配合来兜底，实现故障转移（Failover）或者快速失败（Failfast）**

### **支持跨区域高可用**
提供Java based client，同时提供Restful API支持多语言对接
主要解决云中服务实例动态启停、漂移的问题（相比通过nginx等负载均衡服务实现服务发现，提供了动态发现能力）

### **Eureka逻辑架构**
![Eurake逻辑架构](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/resources/eureka_logic_arch.png)

- Service Provider会向Eureka Server做Register（服务注册）、Renew（服务续约，heartbeat）、Cancel（服务下线）等操作；
- Eureka Server之间对上述三个操作会**通过replicate进行同步**，从而保证**状态最终一致**；
- Service Consumer会向Eureka Server获取注册服务列表（会周期性更新，默认30秒），并消费服务；

### **How Peer Replicates**
在前面的Register、Renew、Cancel接口实现中，我们看到了都会有replicateToPeers操作，这个就是用来做Peer之间的状态同步。

通过这种方式，Service Provider只需要通知到任意一个Eureka Server后就能保证状态会在所有的Eureka Server中得到更新。

具体实现方式其实很简单，就是接收到Service Provider请求的Eureka Server，把请求再次转发到其它的Eureka Server，调用同样的接口，传入同样的参数，除了会在header中标记isReplication=true，从而避免重复的replicate。

**Peer之间的状态是采用异步的方式同步的，所以不保证节点间的状态一定是一致的，不过基本能保证最终状态是一致的**

结合服务发现的场景，实际上也并不需要节点间的状态强一致。在一段时间内（比如30秒），节点A比节点B多一个服务实例或少一个服务实例，在业务上也是完全可以接受的（Service Consumer侧一般也会实现错误重试和负载均衡机制）。
所以按照CAP理论，Eureka的选择就是放弃C，选择AP

### **分布式系统的数据复制方式**
分布式系统的数据在多个副本之间的复制方式，主要有：
- **主从复制**
就是 Master-Slave 模式，有一个主副本，其他为从副本，**所有写操作都提交到主副本**，再由主副本更新到其他从副本。
写压力都集中在主副本上，是系统的瓶颈，从副本可以分担读请求。
- **对等复制**
就是 Peer to Peer 模式，副本间不分主从，任何副本都可以接收写操作，然后每个副本间互相进行数据更新。
对等复制模式，任何副本都可以接收写请求，不存在写压力瓶颈，但**各个副本间数据同步时可能产生数据冲突**。
Eureka 采用的就是 Peer to Peer 模式

### **数据冲突问题解决**
比如 server A 向 server B 发起同步请求，如果 A 的数据比 B 的还旧，B 不可能接受 A 的数据，那么 B 是如何知道 A 的数据是旧的呢？这时 A 又应该怎么办呢？
数据的新旧一般是通过版本号来定义的，Eureka 是通过 lastDirtyTimestamp 这个类似版本号的属性来实现的。lastDirtyTimestamp 是注册中心里面服务实例的一个属性，表示此服务实例最近一次变更时间

### **eureka部署架构**
支持跨AZ跨Zone部署，且不同zone之间会replicate数据，实现最终一致性\
![部署架构](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/resources/eureka_deploy_arch.png)

Service Consumer和Service Provider本质上是同一个Eureka客户端

详细见《深度剖析服务发现组件Netflix Eureka》

### **健康检查机制**
一般分为**主动健康检查**和**被动健康检查**
- 主动检查就是主动的ping或调用endpoint的healhcheck接口
- 被动检查就是上报heartbeat

Eureka采取的是主动发送heartbeat的监控检查方式（renew接口），默认30s报一次
Eg.
PUT /eureka/apps/{appid}/{instanceId}?status=UP
同时，eureka提供了一种服务可自定义自己是否监控的机制，叫HealthCheckHandler

## **Ribbon**
客户端软负载组件，支持Eureka对接，支持多种可插拔LB策略\
![ribbon设计](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/resources/ribbon_arch.png)


## **Consul**
Consul的选择是优先保证高可靠性（一致性），相对牺牲系统服务发现的可用性。

Consul采用**Raft算法**，要求多数节点写入成功后服务的注册或变动才算完成，严格地保证了在集群外部读取到的服务发现结果的一致性；同时采用**Gossip协议**，支持多数据中心之间更大规模的服务同步。

Consul优先保证高可靠性在一定程度上是基于产品现实情况而做的技术决策，它不像NetflixOSS那样有着全家桶式的微服务组件，万一从服务发现中取到错误地址，就没有其他组件为它兜底了

### **Consul中的Gossip**
Consul用了两种不同的Gossip池。把这两种池分别叫做LAN池和WAN池。

- **LAN池**：Consul中的每个数据中心有一个LAN池，它包含了这个数据中心的所有成员，包括clients和servers。LAN池用于以下几个目的:
  - 成员关系信息允许client自动发现server, 减少了所需要的配置量；
  - 分布式失败检测机制使得由整个集群来做失败检测这件事， 而不是集中到几台机器上；
  - gossip池使得类似领导人选举这样的事件变得可靠而且迅速；
- **WAN池**：WAN池是全局唯一的，因为所有的server都应该加入到WAN池中，无论它位于哪个数据中心。由WAN池提供的成员关系信息允许server做一些跨数据中心的请求。一体化的失败检测机制允许Consul优雅地去处理整个数据中心失去连接， 或者仅仅是别的数据中心的某一台失去了连接。

consul在gossip上的实现实际上是使用的memberlist库，也是自家公司提供的。
其实现了集群内节点发现、 节点失效探测、节点故障转移、节点状态同步等。

CP系统，集群内通过Raft协议来保证强一致性，在没有leader节点的情况下，consul是没法提供服务的，如果发生网络分区，少数派节点也无法提供服务的

### **consul官方提供三种数据一性的方式**：
- **default**：默认模式，在脑裂情况下，也可以读取到值，但可能是旧值，这是一种权衡
- **consistent**：强一致模式
- **stale**：允许在没有leader的情况下也能读取到值，效率高，但是读取旧值的可能性非常大

## **服务注册中心选型**
可用性与一致性的矛盾，是分布式系统永恒的话题，在服务发现这个场景里，权衡的主要关注点是相对更能容忍服务列表不可用的后果，还是服务数据不准确的后果，其次才到性能高低，功能是否强大，使用是否方便等因素。有了选择权衡，很自然就引来了一个“务实”的话题，现在有那么多服务发现框架，哪一款最好？或者说应该如何挑选适合的服务发现框架？当下，直接以服务发现、服务注册中心为目标的组件库，或者间接用来实现这个目标的工具主要有以下三类：
- **在分布式K/V存储框架上自己开发的服务发现**：典型代表是ZooKeeper、Doozerd、etcd。**这些K/V框架提供了分布式环境下读写操作的共识算法**，etcd采用的是我们学习过的Raft算法，ZooKeeper采用的是ZAB算法，这也是一种MultiPaxos的派生算法，所以采用这种方案，就不必纠结CP还是AP的问题，它们都是CP的（也曾有公司采用Redis来做服务发现，这种自然是AP的）。这类框架的宣传语中往往会主动提及“高可用性”，潜台词是“在保证一致性和分区容忍性的前提下，尽最大努力实现最高的可用性”，譬如etcd的宣传语就是“高可用的集中配置和服务发现”（Highly Available KeyValue Store for Shared Configurationand Service Discovery）。这些K/V框架的一个共同特点是在整体较高复杂度的架构和算法的外部，维持着极为简单的应用接口，只有基本的CRUD和Watch等少量API，所以要在上面完成功能齐全的服务发现，很多基础的能力，譬如服务如何注册、如何做健康检查等都必须自己去实现，如今一般也只有“大厂”才会直接基于这些框架去做服务发现了；
- **以基础设施（主要是指DNS服务器）来实现服务发现**：典型代表是SkyDNS、CoreDNS。在kubernetes1.3之前的版本使用SkyDNS作为默认的DNS服务，其工作原理是从APIServer中监听集群服务的变化，然后根据服务生成NS、SRV等DNS记录存放到etcd中，kubelet会设置每个Pod的DNS服务的地址为SkyDNS的地址，需要调用服务时，只需查询DNS把域名转换成IP列表便可实现分布式的服务发现。在Kubernetes1.3之后，SkyDNS不再是默认的DNS服务器，而是由只将DNS记录存储在内存中的KubeDNS代替，到了1.11版，就更推荐采用扩展性很强的CoreDNS，此时可以通过各种插件来决定是否要采用etcd存储、重定向、定制DNS记录、记录日志等等。**采用这种方案，是CP还是AP就取决于后端采用何种存储**，如果是基于etcd实现，那自然是CP的，如果是基于内存异步复制的方案实现，那就是AP的（仅针对DNS服务器本身，不考虑本地DNS缓存的TTL刷新）。以基础设施来做服务发现，好处是对应用透明，**任何语言、框架、工具都肯定支持HTTP、DNS，所以完全不受程序技术选型的约束**，但坏处是透明的并不一定是简单的，你必须自己考虑如何去做客户端负载均衡、如何调用远程方法等这些问题，而且必须遵循或者说受限于这些基础设施本身所采用的实现机制，譬如服务健康检查里，服务的缓存期限就应该由TTL决定，这是DNS协议所规定的，如果想改用KeepAlive长连接来实时判断服务是否存活就相对麻烦；
- **专门用于服务发现的框架和工具**：典型代表是Eureka、Consul和Nacos。在这一类框架中，你可以自己决定是CP还是AP，譬如CP的Consul、AP的Eureka，还有同时支持CP和AP的Nacos（**Nacos采用类Raft协议实现CP，采用自研的Distro协议实现AP**，这里“同时”是“都支持”的意思，但必须二取其一，不是说CAP全能满足）。将它们划归一类是因为它们对应用并不是透明的，尽管Consul的主体逻辑是在服务进程之外，以边车的形式提供；尽管Consul、Nacos也支持基于DNS的服务发现；尽管这些框架都基本做到了以声明代替编码，譬如在SpringCloud中只改动pom.xml、配置文件和注解即可实现，但它们依然是可以被应用程序感知的。所以或多或少还需要考虑程序语言、技术框架的集成问题。但这个特点其实并不全是坏处，譬如采用Eureka做服务注册，那在远程调用服务时就可以用OpenFeign做客户端，因为它们本身就已做好了集成，写个声明式接口就能运行；在做负载均衡时可以采用Ribbon做客户端，需要换均衡算法时改个配置就成。这些“不透明”实际上都为编码开发带来了一定便捷，但前提是你选用的语言和框架必须支持；