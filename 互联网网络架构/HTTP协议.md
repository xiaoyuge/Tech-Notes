## **HTTP协议**
目前普遍使用的是http1.1版本，定义了从web服务器传输超文本到浏览器的传输协议，http是应用层协议（对应网络七层协议的第7层，同属于这一层的还有 ftp、smtp、telnet 协议等），本身是无状态的（通常使用cookie+seession实现有状态，但不限这种方式）,request/response模式（也即客户端不发起请求，服务端无法将消息推送给客户端），默认端口号是80

### **HTTP1.1 vs HTTP1.0的区别**
1) 默认长连接(http header中connection=keep-alive);
2) 增加了host域;
3) 增加了header消息：返回状态码100（continue），客户端在发request消息body之前先用request header试探一下server，看server要不要接收request body，再决定要不要发request body,客户端在Request头部中要包含Expect: 100-continue
关于header消息的应用场景：
http 100-continue用于客户端在发送POST数据给服务器前，征询服务器情况，看服务器是否处理POST的数据，如果不处理，客户端则不上传POST数据，如果处理，则POST上传数据。在现实应用中，通过在POST大数据时，才会使用100-continue协议。比如，在使用curl做POST的时候, 当要POST的数据大于1024字节的时候, curl并不会直接就发起POST请求, 而是会分为俩步，第一步发送一个请求, 包含一个Expect:100-continue, 询问Server使用愿意接受数据，第二步接收到Server返回的100-continue应答以后, 才把数据POST给Server
4) HTTP1.1增加了OPTIONS, PUT, DELETE, TRACE, CONNECT这些Request方法（Http1.0只定义了get、post和head方法）;

### **Http 2.0 VS Http 1.1**
1) Http 1.x在一条tcp连接上多个请求只能串行执行。而http2.0支持多路复用，同域名下所有通信都在单个连接上完成，同个域名只需要占用一个 TCP 连接，使用一个连接并行发送多个请求和响应；
在http1.1中
Keep-Alive还是存在如下问题：
    - 串行的文件传输
    - 同域并行请求限制带来的阻塞（6~8）个
    
在 HTTP/2 中：\
- 同域名下所有通信都在单个连接上完成，同个域名只需要占用一个 TCP 连接，使用一个连接并行发送多个请求和响应。
- 单个连接可以承载任意数量的双向数据流，单个连接上可以并行交错的请求和响应，之间互不干扰。
- 数据流以消息的形式发送，而消息又由一个或多个帧组成，多个帧之间可以乱序发送，因为根据帧首部的流标识可以重新组装

在HTTP/2中，帧（Frame）才是最小粒度的信息单位，它可以用来描述各种数据，譬如请求的Headers、Body，或者用来做控制标识，譬如打开流、关闭流。这里说的流（Stream）是一个逻辑上的数据通道概念，每个帧都附带一个流ID以标识这个帧属于哪个流。这样，在同一个TCP连接中传输的多个数据帧就可以根据流ID轻易区分开来，在客户端毫不费力地将不同流中的数据重组出不同HTTP请求和响应报文来。这项设计是HTTP/2的最重要的技术特征一，被称为HTTP/2多路复用（HTTP/2Multiplexing）技术
有了多路复用的支持，HTTP/2就可以对每个域名只维持一个TCP连接（OneConnectionPerOrigin）并以任意顺序传输任意数量的资源了，这样既减轻了服务器的连接压力，也不需要开发者去考虑域名分片这种事情来突破浏览器对每个域名最多6个的连接数限制。更重要的是，没有TCP连接数的压力，就无须刻意压缩HTTP请求，所有通过合并、内联文件（无论是图片、样式、脚本）以减少请求数的需求都不再成立，甚至会被当作徒增副作用的反模式\
![http2](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E4%BA%92%E8%81%94%E7%BD%91%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/resources/HTTP2.png)

2.要支持Http2.0，前提是必须支持https，所以http2.0天然是更安全的；

HTTP OPTIONS请求
旨在发送一种“探测”请求以确定针对某个目标地址的请求必须具有怎样的约束（比如应该采用怎样的HTTP方法以及自定义的请求报头），然后根据其约束发送真正的请求。比如针对“跨域资源”的预检（Preflight）请求采用的HTTP方法就是OPTIONS
用于返回服务器针对特定资源所支持的HTTP请求方法，若请求成功，服务器会在HTTP响应头中包含一个名为“Allow”的头，值是所支持的方法，如“GET, POST”

HTTP Head请求（http1.0中提供）
向服务器索要与GET请求相一致的响应，只不过响应体将不会被返回。这一方法可以在不必传输整个响应内容的情况下，就可以获取包含在响应消息头中的元信息，主要用于根据响应状态码确认资源对象是否存在或者测试资源是否被修改过

HTTP PUT请求 和 POST请求的区别：
主要根据幂等性进行区分，先理解一下幂等性的概念
幂等性概念：一个幂等操作的特点就是其任意多次执行所产生的影响均与依次一次执行的影响相同
REST请求中的幂等性操作:GET，PUT，DELETE都是幂等操作，而POST不是
GET请求：很好理解，对资源做查询多次，此实现的结果都是一样的；
PUT请求：将A修改为B，它第一次请求值变为了B，再进行多次此操作，最终的结果还是B，与一次执行的结果是一样的，所以PUT是幂等操作；
DELETE请求：第一次将资源删除后，后面多次进行此删除请求，最终结果是一样的，将资源删除掉了；
POST请求：不是幂等操作，因为一次请求添加一份新资源，二次请求则添加了两份新资源，多次请求会产生不同的结果，因此POST不是幂等操作；

HTTP GET请求与POST请求的区别:
1.Get方式在通过URL提交数据，数据在URL中可以看到；POST方式，数据放置在请求消息主体内提交;
2.GET方式提交的数据最多只能有1024字节（好像还依赖浏览器限制），而POST则没有此限制;
4.安全性问题;
5.幂等性：get请求是幂等的，post请求不是；

一些重要的HTTP请求头域：
1.Referer:允许客户端指定请求uri的源资源地址，这可以允许服务器生成回退链表;
2.Cache-Control：指定请求和响应遵循的缓存机制，在请求消息或响应消息中设置Cache-Control并不会修改另一个消息处理过程中的缓存处理过程（即影响范围是某次请求响应）;
3.Connection:指定是否使用长连接，值keep-alive表示使用，值 close 表示不使用，http1.1协议默认使用长连接,不论request还是response的header中包含了值为close的connection，都表明当前正在使用的tcp链接在当次请求处理完毕后会被断掉。以后client再进行新的请求时就必须创建新的tcp链接了,设置为 keep-alive 的长连接需要设置 timeout 时间，避免链接长时间占用系统资源；
4.If-Modified-Since/If-None-Match：只有当所请求的内容在指定的日期之后又经过修改才返回内容，否则返回304“Not Modified”应答;(ctrl+F5强制刷新不经过这个过程)
5.Accept-Encoding:浏览器能够进行解码的数据编码方式;
6.Accept:浏览器可以接受的MIME类型；
7.Accept-Charset:浏览器可以接受的字符集；
8.Content-Length：表示请求消息正文的长度;

在Web开发中，我们大多都习惯使用HTTP请求头中的某些属性来获取客户端的IP地址，常见的属性是REMOTE_ADDR和HTTP_X_FORWARDED_FOR
REMOTE_ADDR：该属性的值是客户端跟服务器“握手”时候的IP。如果使用了“匿名代理”，REMOTE_ADDR将显示代理服务器的IP；
X-Forwarded-For（简称XFF）：是用来识别通过HTTP代理或负载均衡方式连接到Web服务器的客户端最原始的IP地址的HTTP请求头字段。
这一HTTP头一般格式如下：X-Forwarded-For: client1, proxy1, proxy2
其中的值通过逗号+空格，把多个IP地址区分开, 最左边(client1)是最原始客户端的IP地址, 代理服务器每成功收到一个请求，就把请求来源IP地址添加到右边。 在上面这个例子中，这个请求成功通过了三台代理服务器：proxy1, proxy2 及 proxy3。请求由client1发出，到达了proxy3(proxy3可能是请求的终点)。请求刚从client1中发出时，XFF是空的，请求被发往proxy1；通过proxy1的时候，client1被添加到XFF中，之后请求被发往proxy2;通过proxy2的时候，proxy1被添加到XFF中，之后请求被发往proxy3；通过proxy3时，proxy2被添加到XFF中，之后请求的的去向不明，如果proxy3不是请求终点，请求会被继续转发。

一些重要的响应头域：
1.Content-Encoding：文档的编码（Encode）方法,只有在解码之后才可以得到Content-Type头指定的内容类型;
2.Content-Length：表示内容长度。只有当浏览器使用持久HTTP连接时才需要这个数据;
3.Content-Type： 表示后面的文档属于什么MIME类型;
4.Last-Modified：文档的最后改动时间。客户可以通过If-Modified-Since请求头提供一个日期，该请求将被视为一个条件GET，只有改动时间迟于指定时间的文档才会返回，否则返回一个304（Not Modified）状态;
5.expires：需和Last-Modified/etag结合使用，用于控制请求文件的有效时间，当请求数据在有效期内时客户端浏览器从缓存请求数据而不是服务器端.当缓存中数据失效或过期，才决定从服务器更新数据,Expires的一个缺点就是，返回的到期时间是服务器端的时间，这样存在一个问题，如果客户端的时间与服务器的时间相差很大，那么误差就很大，所以在HTTP 1.1版开始，使用Cache-Control: max-age=秒替代;

各种常见返回状态码含义：
1.100：服务端告诉客户端，必须继续发出body请求
2.200：成功
3.302：重定向（1.0协议）
4.303：重定向（1.1协议）
4.304：客户端缓存的文件没有变化
5.403：禁止访问
6.404：没有找到文件
7.500：服务器内部错误

请求和响应头中的一个重要信息：Cookie
Cookie的生命周期问题
设置Cookie对象的有效时间， setMaxAge()方法便可以设置Cookie对象的有效时间，
例如：Cookie c = new Cookie("username","john");
c.setMaxAge(60);//60秒的意思
c.setMaxAge(60*60);//一小时
c.setMaxAge(365*24*60*60);//一年

会话cookie：
如果不设置过期时间，则表示这个cookie生命周期为浏览器会话期间，只要关闭浏览器窗口，cookie就消失了
这种生命期为浏览会话期的cookie被称为会话cookie。会话cookie一般是保存在内存里（相比持久化cookie保存在硬盘上）
持久化cookie：
如果设置了过期时间，浏览器就会把cookie保存到硬盘上，关闭后再次打开浏览器，这些cookie依然有效直到超过设定的过期时间。存储在硬盘上的cookie可以在不同的浏览器进程间共享，比如两个IE窗口。而对于保存在内存的cookie，不同的浏览器有不同的处理方式。
cookie.setmaxage设置为0时，会马上在浏览器上删除指定的cookie；
cookie.setmaxage设置为-1时，代表关闭当前浏览器即失效（会话 cookie）；
cookie.setmaxage设置为大于0的值时，代表持久化cookie；

cookie 和 seesion
目的是为了解决http无状态的问题，seesion可以基于cookie来实现，也可以用其他方式来实现（比如UrlRewrite或隐藏表单字段）
UrlRewrite是指服务器在发送给浏览器页面的所有链接中都携带标识状态的参数（比如JSESSIONID），这样客户端点击任何一个链接都会把该参数带回服务器

浏览器客户端缓存的常见流程
1.服务器收到请求时，会在200 OK中回送该资源的Last-Modified或ETag头（或两者都有），并配合Expires（http1.0）或Cache-Control:max-age（http1.1）头使用（两者都存在的时候，cache-control覆盖Expires）；
2.客户端将该资源保存在cache中，并记录Last-Modified和ETag的值;
3.当客户端需要请求相同的资源时，首先看Expires或Cache-Control约定的资源缓存有效时间是否过期，如果没有过期则直接从缓存中返回（响应状态码200）;
4.如果约定的缓存资源有效期到了，则会发送请求并在请求中携带If-Modified-Since或If-None-Match两个头，两个头的值分别是之前服务器返回的Last-Modified和ETag头的值，服务器通过这两个请求头的值判断本地资源内容是否未发生变化，如果没有发生变化，则客户端不需要重新下载，返回304响应；
5.如果服务器通过这两个请求头的值判断本地资源内容发生了变化，则返回新的资源内容，成功返回状态码200；

有Last-Modified为什么要引入Etag？
Etag主要为了解决Last-Modified无法解决的一些问题：
1、一些文件也许会周期性的更改，但是他的内容并不改变(仅仅改变的修改时间)，这个时候我们并不希望客户端认为这个文件被修改了，而重新GET;
2、某些文件修改非常频繁，比如在秒以下的时间内进行修改，(比方说1s内修改了N次)，If-Modified-Since能检查到的粒度是s级的，这种修改无法判断(或者说UNIX记录MTIME只能精确到秒)
3、某些服务器不能精确的得到文件的最后修改时间；

HTTP服务端可以根据自己的意愿来选择如何生成这个ETag标识，譬如Apache服务端的ETag值默认是对文件的索引节点（INode）、大小和最后修改时间进行哈希计算后得到的。ETag是HTTP中一致性最强的缓存机制，ETag也是HTTP中性能最差的缓存机制，在每次请求时，服务端都必须对资源进行哈希计算，相比简单获取一下修改时间，开销要大了很多。ETag和LastModified是允许一起使用的，服务端会优先验证ETag，在ETag一致的情况下，再去对比LastModified，这是为了防止有一些HTTP服务端未将文件修改日期纳入哈希范围内

Http的内容协商机制
在HTTP协议的设计中，一个URL地址是有可能提供多份不同版本的资源的，譬如，一段文字的不同语言版本，一个文件的不同编码格式版本，一份数据的不同压缩方式版本，等等。因此针对请求的缓存机制，也必须能够提供对应的支持。为此，HTTP协议设计了以Accept*（Accept、Accept
Language、AcceptCharset、AcceptEncoding）开头的一套请求Header和对应的以Content*（ContentLanguage、ContentType、ContentEncoding）开头的响应Header，这些Header被称为HTTP的内容协商机制。

Http传输链路优化的前端设计原则
（1）减少请求数量（Minimize HTTP Requests）：请求每次都需要建立通信链路进行数据传输，这些开销很昂贵，减少请求的数量可有效提高访问性能，对于前端开发者，可用于减少请求数量的手段包括：
    ·雪碧图（CSS Sprite）
    ·CSS、JS文件合并/内联（Concatenation/Inline）
    ·分段文档（Multipart Document）
    ·媒体（图片、音频）内联（Data Base64 URI）
    ·合并Ajax请求（Batch Ajax Request）
（2）扩大并发请求数（Split Components Across Domain）：对于每个域名，现代浏览器（Chrome、Firefox）一般支持6个（IE为8～13个）并发请求。如果希望更快地加载大量图片或其他资源，需要进行域名分片（Domain Sharding），将图片同步到不同主机或者同一个主机的不同域名上
（3）启用压缩传输（GZip Component）：启用压缩能够大幅度减少需要在网络上传输的内容的大小，节省网络流量
（4）避免页面重定向（Avoid Redirect）：当页面发生了重定向，就会造成整个文档的传输延迟。在HTML文档到达之前，页面中不会呈现任何东西，降低了用户体验
（5）按重要性调节资源优先级（Put Stylesheet at theTop，Put Script at the Bottom）：将重要的、马上就要使用的、对客户端展示影响大的资源，放在HTML的头部，以便优先下载

但上述优化措施并非只有好处，它们也带来了诸多不良的副作用。
（1）如果你用雪碧图将多张图片合并，意味着任何场景下哪怕只用到其中一张小图，也必须完整加载整张大图片；任何场景下哪怕对一张小图要进行修改，都会导致整个缓存失效，类似地，样式、脚本等其他
（2）如果你使用了媒体内嵌，除了要承受Base64编码导致传输容量膨胀1/3的代价外（Base64以8位表示6位数据），也将无法有效利用缓存。
（3）如果你合并了异步请求，这就会导致所有请求的返回时间都受最慢的那个请求的拖累，导致整体响应速度下降
（4）如果你把图片放到不同子域下面，将会导致更大的DNS解析负担，而且浏览器对两个不同子域下的同一图片必须持有两份缓存，也使得缓存效率下降

Http传输压缩
Http很早就支持了GZip压缩，因为HTTP传输的内容主要是文本数据，譬如HTML、CSS、Script等，而对于文本数据启用压缩的收益是非常高的，传输数据量一般会降至原有的20%左右。对于那些不适合压缩的资源，Web服务器能根据MIME类型自动判断是否对响应进行压缩，这样，已经采用过压缩算法存储的资源，如JPEG、PNG图片，便不会被二次压缩，空耗性能

压缩与持久化连接机制是存在冲突的：
在网络时代的早期，服务器处理能力还很薄弱，为了启用压缩，会把静态资源先预先压缩为.gz文件的形式存放起来，当客户端可以接收压缩版本的资源时（请求的Header中包含AcceptEncoding:gzip）就返回压缩后的版本（响应的Header中包含ContentEncoding:gzip），否则返回未压缩的原版，这种方式被称为“静态预压缩”（StaticPrecompression）。

而现代的Web服务器处理能力有了大幅提升，已经没有人再采用麻烦的预压缩方式了，都是由服务器对符合条件的请求在输出时进行“即时压缩”（OnTheFlyCompression），整个压缩过程全部在内存的数据流中完成，不必等资源压缩完成再返回响应，这样可以显著提高“首字节时间”（Time To First Byte，TTFB），改善Web性能体验。而这个过程中唯一不好的地方就是服务器再也没有办法给出ContentLength这个响应Header了，因为输出Header时服务器还不知道压缩后资源的确切大小

持久连接机制不再依靠TCP连接是否关闭来判断资源请求是否结束，它会重用同一个连接以便向同一个域名请求多个资源，这样，客户端就必须要有除了关闭连接之外的其他机制来判断一个资源什么时候算传递完毕，这个机制最初（在HTTP/1.0时）就只有Content-Length，即依靠请求Header中明确给出资源的长度判断，传输到达该长度即宣告一个资源的传输已结束。由于启用即时压缩后就无法给出ContentLength了，如果是HTTP/1.0的话，持久连接和即时压缩只能二选一，事实上在HTTP/1.0中对两者都支持，却默认都是不启用的

HTTP/1.1版本中修复了这个缺陷，增加了另一种“分块传输编码”（Chunked Transfer Encoding）的资源结束断机制，彻底解决了ContentLength与持久连接的冲突问题。分块编码的原理相当简单：在响应Header中加入“TransferEncoding:chunked”之后，就代表这个响应报文将采用分块编码。此时，报文中的Body需要改为用一系列“分块”来传输。每个分块包含十六进制的长度值和对应长度的数据内容，长度值独占一行，数据从下一行开始，最后以一个长度值为0的分块来表示资源结束。

http短轮询 vs http长轮询
http 长轮询是服务器收到请求后如果有数据, 立刻响应请求; 如果没有数据就会 hold 一段时间, 这段时间内如果有数据立刻响应请求; 如果时间到了还没有数据, 则响应 http 请求;浏览器受到 http 响应后立在发送一个同样 http 请求查询是否有数据
长轮询的局限:
1）浏览器端对同一服务器同时 http 连接有最大限制, 最好同一用户只存在一个长轮询;
2)   服务器端没有数据 hold 住连接时会造成浪费, 容易产生服务器瓶颈;

http短轮询是服务器收到请求不管是否有数据都直接响应 http 请求; 浏览器受到 http 响应隔一段时间在发送同样的 http 请求查询是否有数据
短轮询的局限:实时性低

两者相同点：
可以看出 http 长轮询和 http 短轮询的都会 hold 一段时间;
两者不同点：
间隔发生在服务端还是浏览器端: http 长轮询在服务端会 hold 一段时间, http 短轮询在浏览器端 hold一段时间;

url encode/decode
使用post时数据会放到http报文中间传输过去，是不需要做任何操作的，浏览器和服务端自动encode和自动decode
当使用get方法传输数据时，而且传输的数据中包含中文、特殊符号等字符时，就需要用encode进行编码
为什么要做url encode？
正确传递和获取参数：比如参数是一个回调的url 地址、参数中带@、&、中文等特殊字符等；

Base64编码
Base64是网络上最常见的用于传输8Bit字节码的编码方式之一，Base64就是一种基于64个可打印字符来表示二进制数据的方法。
Base64编码是从二进制到字符的过程，可用于在HTTP环境下传递较长的标识信息。例如，在Java Persistence系统Hibernate中，就采用了Base64来将一个较长的唯一标识符（一般为128-bit的UUID）编码为一个字符串，用作HTTP表单和HTTP GET URL中的参数。在其他应用程序中，也常常需要把二进制数据编码为适合放在URL（包括隐藏表单域）中的形式。此时，采用Base64编码具有不可读性，需要解码后才能阅读。

为什么有了Base64编码，还要 url encode？
Base64编码使用的字符包括大小写字母各26个，加上10个数字，和加号“+”，斜杠“/”，一共64个字符，等号“=”用来作为后缀用途。
其中的+, /, = 都是需要urlencode的，所以无法取代。

Javarscript
浏览器中有三个常驻的线程，分别是JS引擎，界面渲染，事件响应。由于这三个线程同时要访问DOM树，所以为了线程安全，浏览器内部需要做互斥：当JS引擎在执行代码的时候，界面渲染和事件响应两个线程是被暂停的。所以当JS出现死循环，浏览器无法响应点击，也无法更新界面。现在的浏览器的JS引擎都是单线程的，尽管多线程功能强大，但是线程同步比较复杂，并且危险，稍有不慎就会崩溃死锁。单线程的好处不必考虑线程同步这样的复杂问题，简单而安全

异步Ajax：JS是单线程的，当一个函数执行的时候，JS引擎会锁住DOM树，其他事件的响应代码只能在队列中等待，并且此时页面卡死。那么异步Ajax是怎么回事呢？一个常用的开发实践就是发起一个异步的Ajax，界面显示一个进度条样式的Gif，说好的单线程呢？事实上异步Ajax确实用了多线程，只是Ajax请求的Http连接部分由浏览器另外开了一个线程执行，执行完毕之后给JS引擎发送一个事件，这时候异步请求的回调代码得以执行。它的执行流程是这样的\
![ajax](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E4%BA%92%E8%81%94%E7%BD%91%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/resources/ajax.png)

Http请求的执行在另外一个线程中，由于这个线程并不会操作DOM树，所以是可以保证线程安全的。发起Ajax请求和回调函数中间是没有JS执行的，所以页面不会卡死
在HTML5中，引入了Web Worker这个概念。它能够在另外一个线程中执行计算密集的JS代码而不引起页面卡死，这是真正的多线程。然而为了保证线程安全，Worker中的代码是不能访问DOM的

总结
1. 避免编写计算密集的前端代码
2. 使用异步Ajax
3. 避免编写一个需要较长时间来执行的JS代码，比如生成一个大型的表。遇到这种情况，可以分批执行，比如用setInterval来每秒生成20行，或是用户向下拖动滚动条时候再继续产生新的行。
4. 在页面初始化时候不要执行很多的初始化代码，否则会影响页面渲染变慢。一些不需要立即执行的代码可以在页面渲染完成之后再执行，比如绑定事件，生成菜单之类的控件。
5. 对于复杂页面（像淘宝首页），可以结合异步Ajax分批产生页面。先生成页面框架，页面内容自上而下用异步Ajax逐步加载并填充到框架中。这样能够让用户更早的看到页面。
6. setTimeout(function, 0)是有用的。它可以让callback作为另外一个事件响应代码来执行。实现了当前事件的代码执行完成之后，再渲染DOM，再执行setTimeout的callback。这样能够让一部分代码延后执行，并且在这之前渲染DOM