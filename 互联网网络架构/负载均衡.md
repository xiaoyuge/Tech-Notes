## **负载均衡**

负载均衡从形式上都可以分为两种：四层负载均衡和七层负载均衡
- 四层负载均衡的优势是性能高，七层负载均衡的优势是功能强
- 做多级混合负载均衡，通常应该是低层负载均衡在前，高层负载均衡在后

我们所说的“四层”“七层”指的是经典的OSI七层模型中的第四层传输层和第七层应用层

所说的“四层负载均衡”其实是多种均衡器工作模式的统称，“四层”是说这些工作模式的共同特点是维持同一个TCP连接，而不是说它只工作在第四层。事实上，这些模式主要都工作在第二层（数据链路层，改写MAC地址）和第三层（网络层，改写IP地址）上，单纯只处理第四层（传输层，可以改写TCP、UDP等协议的内容和端口）的数据无法做到负载均衡的转发，因为OSI的下三层是媒体层（MediaLayer），上四层是主机层（HostLayer），既然流量都已经到达目标主机上了，也就谈不上什么流量转发，最多只能做代理。但出于习惯和方便，现在几乎所有的资料都把它们统称为四层负载均衡

几种常见的四层负载均衡工作模式
（1）数据链路层负载均衡
数据链路层负载均衡所做的工作，是修改请求的数据帧中的MAC目标地址，让用户原本发送给负载均衡器的请求的数据帧，被二层交换机根据新的MAC目标地址转发到服务器集群中对应的服务器（后文称为“真实服务器”，RealServer）的网卡上，这样真实服务器就获得了一个原本目标并不是发送给它的数据帧。由于二层负载均衡器在转发请求过程中只修改了帧的MAC目标地址，不涉及更上层协议（没有修改Payload的数据），所以在更上层（第三层）看来，所有数据都是未曾改变过的\
![data-link-balance](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E4%BA%92%E8%81%94%E7%BD%91%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/resources/data-link-balance.png)

约束
1.由于第三层的数据包，即IP数据包中包含了源（客户端）和目标（均衡器）的IP地址，只有真实服务器保证自己的IP地址与数据包中的目标IP地址一致，这个数据包才能被正确处理。因此，使用这种负载均衡模式时，需要把真实物理服务器集群中所有机器的虚拟IP地址（VirtualIPAddress，VIP）配置成与负载均衡器的虚拟IP一样，才能使经均衡器转发后的数据包在真实服务器中顺利地使用
2.在网络侧受到的约束也很大，二层负载均衡器直接改写目标MAC地址的工作原理决定了它与真实服务器的通信必须是二层可达的，通俗地说就是必须位于同一个子网当中，无法跨VLAN

优势
也正是因为实际处理请求的真实物理服务器IP和数据请求中的目的IP是一致的，所以响应结果就不再需要通过负载均衡器进行地址交换，而是可将响应结果的数据包直接从真实服务器返回给用户的客户端，避免负载均衡器网卡带宽成为瓶颈，因此数据链路层负载均衡的效率是相当高的


所以，优势（效率高）和 劣势（不能跨子网）共同决定了数据链路层负载均衡最适合作为数据中心的第一级

（2）网络层负载均衡
根据OSI七层模型，在第三层网络层传输的单位是分组数据包（Packet），这是一种在分组交换网络（Packet Switching Network，PSN）中传输的结构化数据单位。以IP协议为例，一个IP数据包由头部（Header）和荷载（Payload）两部分组成，Header的长度最大为60字节，其中包括20字节的固定数据和最长不超过40字节的可选数据。按照IPv4标准，一个典型的分组数据包的Header部分具有如下表所示的结构\
![network-package](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E4%BA%92%E8%81%94%E7%BD%91%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/resources/network-package.png)

无须过多关注表格中的其他信息，只要知道在IP分组数据包的Header中带有源和目标的IP地址即可。源和目标IP地址说明了数据是从分组交换网络中哪台机器发送到哪台机器的，我们可以沿用与二层改写MAC地址相似的思路，通过改变这里面的IP地址来实现数据包的转发。具体有两种常见的修改方式：

1.第一种是保持原数据包不变，新创建一个数据包：把原数据包的Header和Payload整体作为新数据包的Payload，并在这个新数据包的Header中写入真实服务器的IP作为目标地址，然后把它发送出去。经过三层交换机的转发，真实服务器收到数据包后，必须在接收入口处设计一个针对性的拆包机制，把由负载均衡器自动添加的那层Header扔掉，还原出原数据包来进行使用。这样，真实服务器就同样拿到了一个原本不是发给它（目标IP不是它）的数据包，达到了流量转发的目的，这种方式也成为“IP隧道（IP Tunnel）”。\
![ip-channel-balance](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E4%BA%92%E8%81%94%E7%BD%91%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/resources/ip-channel-balance.png)

约束
1.第一个缺点是它要求真实服务器必须支持“IP隧道协议”（IPEncapsulation），即它得学会自己拆包扔掉一层Header，这个其实并不是什么大问题，现在几乎所有的Linux系统都支持IP隧道协议；
2.另外一个缺点是这种模式仍必须通过专门的配置，必须保证所有的真实服务器与负载均衡器有相同的虚拟IP地址，因为回复该数据包时，需要使用这个虚拟IP作为响应数据包的源地址，这样客户端收到这个数据包时才能正确解析。这个限制就相对麻烦一些，它与“透明”的原则冲突，需由系统管理员介入

优势
1.尽管因为要封装新的数据包，IP隧道转发模式的效率比直接路由模式的效率低，但由于并没有修改原数据包中的任何信息，所以IP隧道转发模式仍然具备三角传输的特性，即负载均衡器转发来的请求，可以由真实服务器去直接应答，无须经过负载均衡器原路返回
2.而且由于IP隧道工作在网络层，所以可以跨越VLAN，因此摆脱了直接路由模式中网络侧的约束

2.第二种修改方式改变目标数据包（NAT模式）：直接修改数据包Header中的目标地址，修改后原本由用户发给负载均衡器的数据包也会被三层交换机转发到真实服务器的网卡上，而且因为没有经过IP隧道的额外包装，也就无须再拆包了。但问题是是这种模式是通过修改目标IP地址才到达真实服务器的，如果真实服务器直接将应答包返回客户端，即这个应答数据包的源IP是真实服务器的IP，也即均衡器修改以后的IP地址，则客户端不可能认识该IP，自然就无法正常处理这个应答了。因此，只有让应答流量继续回到负载均衡器，由负载均衡器把应答包的源IP改回自己的IP，再发给客户端，才能保证客户端与真实服务器之间的正常通信\
![nat-balance](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E4%BA%92%E8%81%94%E7%BD%91%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/resources/nat-balance.png)

劣势
在流量压力比较大的时候，NAT模式的负载均衡会带来较大的性能损失，比起直接路由和IP隧道模式，甚至会出现数量级上的下降。这点是显而易见的，由负载均衡器代表整个服务集群来进行应答，各个服务器的响应数据都会互相争抢均衡器的出口带宽

还有一种更加彻底的NAT模式：即负载均衡器在转发时，不仅会修改目标IP地址，还会修改源IP地址，这样源地址就改成负载均衡器自己的IP，称作SourceNAT（SNAT）。这样做的好处是真实服务器无须配置网关就能够让应答流量经过正常的三层路由回到负载均衡器上，做到彻底的透明。但是缺点是由于做了SNAT，真实服务器处理请求时就无法拿到客户端的IP地址，从真实服务器的视角来看，所有的流量都来自于负载均衡器，这样有一些需要根据目标IP进行控制的业务逻辑就无法进行了

（3）应用层负载均衡
前面介绍的四层负载均衡工作模式都属于“转发”，即直接将承载着TCP报文的底层数据格式（IP数据包或以太网帧）转发到真实服务器上，此时客户端与响应请求的真实服务器维持着同一条TCP通道。但工作在四层之后的负载均衡模式就无法再转发了，只能代理，此时真实服务器、负载均衡器、客户端三者之间由两条独立的TCP通道来维持通信\
![forward-proxy](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E4%BA%92%E8%81%94%E7%BD%91%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/resources/forward-proxy.png)


“代理”这个词，根据“哪一方能感知到”的原则，可以分为“正向代理”“反向代理”和“透明代理”三类。
1.正向代理就是我们通常简称的代理，指在客户端设置的，代表客户端与服务器通信的代理服务，它是客户端可知，而对服务器透明的；
2.反向代理是指在服务器侧设置的，代表真实服务器与客户端通信的代理服务，此时它对客户端来说是透明的；
3.至于透明代理是指对双方都透明的，配置在网络中间设备上的代理服务，譬如，架设在路由器上的透明翻墙代理；

七层负载均衡器属于反向代理的一种
如果只论网络性能，七层负载均衡器肯定比不过四层负载均衡器，原因如下
1.它比四层负载均衡器至少多一轮TCP握手；
2.有着跟NAT转发模式一样的带宽问题（响应需要统一通过负载均衡器）；
3.通常要耗费更多的CPU，因为可用的解析规则远比四层丰富；

所以如果用七层负载均衡器去做下载站、视频站这种流量应用是不合适的，起码不能作为第一级均衡器。但是，如果网站的性能瓶颈不是网络性能，而是整个服务集群对外所体现出来的服务性能，那么七层负载均衡器就有它的用武之地了。因为七层负载均衡器工作在应用层，可以感知应用层通信的具体内容，往往能够做出更明智的决策，玩出更多的花样。

常见负载均衡策略
* 轮询均衡（RoundRobin）：每一次来自网络的请求轮流分配给内部的服务器，从1至N然后重新开始。此种均衡算法适合于服务器集群中的所有服务器都有相同的软硬件配置并且平均服务请求相对均衡的情况。
* 权重轮询均衡（WeightedRoundRobin）：根据服务器的不同处理能力，给每个服务器分配不同的权重值，使其能够接受相应权值数的服务请求。譬如：设置服务器A的权值为1，B的权值为3，C的权值为6，则服务器A、B、C将分别接收到10%、30%、60%的服务请求。此种均衡算法能确保高性能的服务器得到更多的使用率，避免低性能的服务器负载过重。
* 随机均衡（Random）：把来自客户端的请求随机分配给内部的多个服务器，在数据量足够大的场景下能达到相对均衡的分布；
* 权重随机均衡（WeightedRandom）：此种均衡算法类似于权重轮询算法，不过在分配处理请求时是随机选择的过程；
* 一致性哈希均衡（ConsistencyHash）：将请求中的某些数据（可以是MAC、IP地址，也可以是更上层协议中的某些参数信息）作为特征值来计算需要落在的节点，算法一般会保证同一个特征值每次都一定落在相同的服务器上。这里的一致性是指保证当服务集群某个真实服务器出现故障时，只影响该服务器的哈希值，而不会导致整个服务器集群的哈希键值重新分布；
* 响应速度均衡（ResponseTime）：负载均衡设备对内部各服务器发出一个探测请求（例如Ping），然后根据内部各服务器对探测请求的最快响应时间来决定哪一台服务器响应客户端的服务请求。此种均衡算法能较好地反映服务器的当前运行状态，但最快响应时间仅仅指的是负载均衡设备与服务器之间的最快响应时间，而不是客户端与服务器之间的最快响应时间；
* 最少连接数均衡（LeastConnection）：客户端的每一次请求服务在服务器停留的时间可能会有较大差异，随着工作时间增加，如果采用简单的轮询或随机均衡算法，每一台服务器上的连接进程可能会产生极大的不平衡，并不能达到真正的负载均衡。最少连接数均衡算法会对内部需负载的每一台服务器有一个数据记录，记录当前该服务器正在处理的连接数量，当有新的服务连接请求时，将把当前请求分配给连接数最少的服务器，使均衡更加符合实际情况，使负载更加均衡。此种均衡策略适合长时处理的请求服务，如FTP传输；

### **Nginx**
#### **Nginx优点（高性能、高可靠、可扩展）**：
- **高并发高性能**：基于IO多路复用机制（epoll）实现的IO模型，非阻塞，并发性能好（多进程单线程），一台nginx可支持千万并发连接、静态资源请求下百万rps；
- **可扩展性好**：模块化设计，第三方模块生态圈非常丰富、甚至有Tengine和Openresty这样的第三方插件在nginx之上又生成了一套新的生态圈
- **高可靠性**：指服务器可以持续不间断的长时间运行，因为其通常运行在企业内网的边缘节点上，这种场景需要4个9、5个9甚至更高的可用性，nginx采用多进程而非多线程的架构来保障高可靠，work进程之间互相隔离，master进程负责worker进程的监督管理，比如worker进程故障时候的新worker进程创建和failover；
- **热部署（不停机升级nginx）**：指可以在不停止服务的情况下升级master，这个特性非常重要，因为在nginx上可能跑了数百万的并发请求，不能随意kill掉进程重启；
- **reload（不停机升级配置）**：指可以在不停止服务的情况下，使用新的nginx.conf配置文件启动worker；    
- **BSD许可证**：指nginx不仅是开源免费的，而且可以在有定制需求的场景下修改nginx源代码后用于商业场景是合法的；

#### **Nginx的不足**：
- **缺乏动态性**：动态指的是程序可以在运行时、在不重新加载的情况下，去修改参数、配置，乃至修改自身的代码。具体到 Nginx 和 OpenResty 领域，你去修改上游、SSL证书、限流限速阈值，而不用重启服务，就属于实现了动态。开源版本的 Nginx 并不支持动态特性，所以，你要对上游、SSL证书做变更，就必须通过修改配置文件、重启服务的方式才能生效。而商业版本的 Nginx Plus 提供了部分动态的能力，你可以用 REST API 来完成更新；
- **二次开发成本较高**：Nginx 采用 C 语言开发，二次开发门槛较高。市场应用广泛，更多是基于 nginx.conf 预留配置参数，如：反向代理、负载均衡、静态 web 服务器等，如果想让 Nginx 访问 MySQL ，定制化开发一些业务逻辑，难度很高

#### **Nginx vs  Apache**
相比Apache，Nginx 是个“轻量级”的 Web 服务器。“轻量级”是相对于“重量级”而言的。“重量级”就是指服务器进程很“重”，占用很多资源，当处理 HTTP 请求时会消耗大量的 CPU 和内存，受到这些资源的限制很难提高性能。而 Nginx 作为“轻量级”的服务器，它的 CPU、内存占用都非常少，同样的资源配置下就能够为更多的用户提供服务。

在 Nginx 之前，Web 服务器（如Apache）的工作模式大多是“Per-Process”或者“Per-Thread”，**对每一个请求使用单独的进程或者线程处理**。这就存在创建进程或线程的成本，还会有进程、线程“上下文切换”的额外开销。如果请求数量很多，CPU 就会在多个进程、线程之间切换时“疲于奔命”，平白地浪费了计算时间。

Nginx 则完全不同，“一反惯例”地没有使用多线程，而是使用了“多进程 + 单线程”的工作模式。Nginx 在启动的时候会预先创建好固定数量的 worker 进程（通常与CPU核数一样），在之后的运行过程中不会再 fork 出新进程，这就是多进程，而且可以自动把进程“绑定”到独立的 CPU 上，这样就完全消除了进程创建和切换的成本，能够充分利用多核 CPU 的计算能力。

Apache目前一共有三种稳定的MPM（Multi-Processing Module，多进程处理模块）模式。它们分别是prefork，worker、event，它们同时也代表这Apache的演变和发展
1) **Prefork MPM**  \
Prefork MPM实现了一个非线程的、预派生的web服务器。它在Apache启动之初，就先预派生一些子进程，然后等待连接；可以减少频繁创建和销毁进程的开销，每个子进程只有一个线程，在一个时间点内，只能处理一个请求。这是一个成熟稳定，可以兼容新老模块，也不需要担心线程安全问题，但是一个进程相对占用资源，消耗大量内存，不擅长处理高并发的场景


2) **Worker MPM** \
和prefork模式相比，worker使用了多进程和多线程的混合模式，worker模式也同样会先预派生一些子进程，然后每个子进程创建一些线程，同时包括一个监听线程，每个请求过来会被分配到一个线程来服务。线程比起进程会更轻量，因为线程是通过共享父进程的内存空间，因此，内存的占用会减少一些，在高并发的场景下会比prefork有更多可用的线程，表现会更优秀一些；另外，如果一个线程出现了问题也会导致同一进程下的线程出现问题，如果是多个线程出现问题，也只是影响Apache的一部分，而不是全部。由于用到多进程多线程，需要考虑到线程的安全了，在使用keep-alive长连接的时候，某个线程会一直被占用，即使中间没有请求，需要等待到超时才会被释放（该问题在prefork模式下也存在）



3) **Event MPM** \  
这是Apache最新的工作模式，它和worker模式很像，不同的是在于它解决了keep-alive长连接的时候占用线程资源被浪费的问题，在event工作模式中，会有一些专门的线程用来管理这些keep-alive类型的线程，当有真实请求过来的时候，将请求传递给服务器的线程，执行完毕后，又允许它释放。这增强了在高并发场景下的请求处理



#### **nginx组成**：
1) 二进制可执行文件:由各个模块的源代码编译出的一个文件；
2) nginx.conf配置文件：控制nginx的行为；
3) accless.log访问日志：记录每一条http请求；
4) error.log错误日志：定位问题；

#### **ngnix每次版本发布包括**
1) feature：新增了哪些功能
2) bugfix：修复了哪些Bug
3) change：进行了哪些重构

#### **nginx进程结构**
nginx采取的是多进程结构而不是多线程结构，目的是为了更健壮，可靠性更好
进程间的通讯，是使用共享内存和信号

所有的请求都是通过worker进程来处理，master进程只是为了管理和监控worker进程，且worker进程的数量需要保持和cpu核数一致





通常我们通过向Master进程发送信号，来管理worker进程


 Master 进程，一如其名，扮演“管理者”的角色，并不负责处理终端的请求。它是用来管理 Worker 进程的，包括接受管理员发送的信号量、监控 Worker 的运行状态。当 Worker 进程异常退出时，Master 进程会重新启动一个新的 Worker 进程

Worker 进程则是“一线员工”，用来处理终端用户的请求。它是从 Master 进程 fork 出来的，彼此之间相互独立，互不影响。多进程的模式比 Apache 多线程的模式要先进很多，没有线程间加锁，也方便调试。即使某个进程崩溃退出了，也不会影响其他 Worker 进程正常工作。同时每个worker进程中只有一个线程，这就省去了并发情况下的加锁以及线程的切换带来的性能损耗

Worker一个线程如何支持高并发的业务场景？
以 Linux 为例，其采用的是 epoll 模型（即事件驱动模型），该模型是 IO 多路复用思想的一种实现方式，是非阻塞的，什么意思呢？就是一个请求进来后，会由一个 worker 进程去处理，当程序代码执行到 IO 时，比如调用外部服务或是通过 upstream 分发请求到后端 Web 服务时，IO 是阻塞的，但是 worker 进程不会一直在这等着，而是等 IO 有结果了再处理，在这期间它会去处理别的请求，这样就可以充分利用 CPU 资源去处理多个请求了。

Web 服务器从根本上来说是“I/O 密集型”而不是“CPU 密集型”，处理能力的关键在于网络数据收发而不是 CPU 计算（这里暂时不考虑 HTTPS 的加解密），而网络 I/O 会因为各式各样的原因不得不等待，比如数据还没到达、对端没有响应、缓冲区满发不出去等等。

Nginx 里使用的 epoll，就好像是 HTTP/2 里的“多路复用”技术，它把多个 HTTP 请求处理打散成碎片，都“复用”到一个单线程里，不按照先来后到的顺序处理，而是只当连接上真正可读、可写的时候才处理，如果可能发生阻塞就立刻切换出去，处理其他的请求。通过这种方式，Nginx 就完全消除了 I/O 阻塞，把 CPU 利用得“满满当当”，又因为网络收发并不会消耗太多 CPU 计算能力，也不需要切换进程、线程，所以整体的 CPU 负载是相当低的

Linux 支持的以 IO 多路复用思想来实现的模型还有 select 和 poll，为什么选择了 epoll 呢？因为 epoll 的效率更高
假设一个 work process 处理了 1000 个连接，但其中只有 10 个 IO 完成了，并可以继续往下执行，select/poll 的做法是遍历这 1000 个 FD（File Description，可以理解成每个建立了连接的一个标识），找到那 10 个就绪状态的，并把没做完的事情继续做完，这样检索的效率明显很低。所以 epoll 的做法是当这 10 个 IO 准备就绪时，通过系统的回调函数将就绪的 FD 放到一个专门的就绪列表中，这样系统只需要去找这个就绪列表就可以了，这大大提高了系统的响应效率

reload流程（指使用新的nginx.conf配置文件启动新的worker进程）





        ，上面步骤中master进程打开新的监听端口的场景是配置文件中可能新增一个监听端口


二进制热升级流程（指用新的nginx binary文件启动新的master进程）




一个报文



nginx中http请求处理时的11个阶段





RealIP模块



RealIp模块拿到X-Forward-For和X-Real-IP这两个字段的值后，会替换binary_remote_addr和remote_addr这两个变量的值，并提供Realip_remote_addr和realip_remote_port两个变量来维护原来的src_ip和src_port


rewrite模块





Limit_conn模块


Limit_req模块



location + proxy_pass + upstream 实现基于url路由并基于某种负载均衡策略访问后端服务
Nginx.conf参考配置
server{
    server_name:域名
    Location url匹配 {
        proxy_pass：上游服务名
    }
}
upstream 上游服务名{
     负载均衡策略比如IP_HASH、随机、轮询、加权轮询等；
     Server x.x.x.x:port;
     Server x.x.x.x:port;
}