## **高并发系统设计的通用方法论**

### **什么是好的架构设计**

在聊方法论之前，先表达一个观点：好的架构不是被发明和设计出来的，而是持续演进的结果，而什么是好的架构？适合当下业务且方便演进的架构就是好的架构。

所有业务的发展，都有一个从0到1以及从1到n的过程，架构师有一个很容易犯的错误是在业务还不复杂的时候对架构过度设计，引入了不必要的复杂性，当业务真正高速增长的时候，架构背上了沉重的设计包袱，反而不能够快速灵活的演进变化。

当下，微服务架构被各大公司普遍采用，已经成了大规模分布式架构的实施标准，导致现在一旦谈架构，言必称微服务。但微服务架构也并不是银弹，在不同的业务发展阶段，我们不能以微服务架构为唯一标准，因为一旦我们实施微服务，就必将引入分布式架构带来的诸多问题，比如RPC调用的网络通讯稳定性、性能、容错等问题、不同分布式节点的数据一致性问题等。所以，在初期业务还很简单的时候，单体架构可能是更适合你的解决方案，所以我们说微服务架构不是代替单体架构的目的，两者各有其应用的场景。

而当业务跨过了从0到1，在从1到n快速狂奔的时候，我们的业务量、用户量开始出现爆发式增长，原来的单体架构可能开始出现瓶颈，这个时候我们就会开始考虑如何对架构进行演进优化来应对未来的增长，因此，这也引出了我们的第一个方法论：Sale-up VS Sale-out。

### **Scale-up VS Scale-out**

#### **Scale-up**

通过购买性能更好的硬件来提升单服务器的并发处理能力，也即尽量提升单服务器的性能，将单服务器的性能发挥到极致，比方说目前系统 4 核 4G 每秒可以处理 200 次请求，那么如果要处理 400 次请求呢？很简单，我们把机器的硬件提升到 8 核 8G（硬件资源的提升可能不是线性的，这里仅为参考）

另外，单服务器提升性能除了硬件升级，还可以考虑代码的优化来提升单服务处理并发请求的能力，这里的优化主要涉及到IO模型和并发模型:

- **I/O模型**：同步阻塞、同步非阻塞、IO多路复用、异步等；
- **并发模型**：多进程（PPC、prefork、pool）、多线程（tpc、prethread、pool）、协程（coroutine）等；

而且，针对不同的应用场景，我们重点优化的方向也不一样，比如：

- **针对IO密集型应用**：我们需要考虑更高效的IO模型，比如非阻塞IO、IO多路复用、异步IO等
- **针对CPU密集型应用**：我们需要考虑更高效的并发模型，尽量在占用更少量资源的情况下尽量利用CPU多核能力（比如协程）

#### **Scale-out**

相比Scale up则是另外一个思路，它通过将多个低性能的机器组成一个分布式集群来共同抵御高并发流量的冲击。沿用刚才的例子，我们可以使用两台 4 核 4G 的机器来处理那 400 次请求。这个思路下其实还隐含两种方案：

  1. 简单地把单体架构代码从一台服务器物理部署到多台服务器；
  2. 把单体架构代码先按照DDD的方法完成领域分析，然后根据Bounded Context把单体代码逻辑拆分成多个微服务代码，然后把不同的微服务代码物理部署到多台服务器；
  
如果我们的业务还不复杂，各业务之间的访问量、对性能或安全性的要求没有什么特别的差异，我们可以先试用方案1简单的完成单体架构的复制部署。

如果随着业务越来越复杂，不同业务之间的访问量出现很大差异，对性能或安全性要求的标准出现不一致的时候，出于隔离差异的需要，我们则需要考虑方案2，先按照DDD做代码逻辑拆分，然后根据不同业务场景的需要，做技术选型，比如对性能要求极高的考虑用c++实现，性能要求没那么严苛但又要求高并发低延时的可以考虑用Go（带GC的编程语言，在一些对性能极其敏感的领域并不是最好的选择），业务复杂对稳定性和开发效率要求高性能够用就行则可以考虑Java（极其成熟丰富的生态和高度活跃的社区）等，最后把不同的微服务实例部署到不同的服务器上。

#### **什么时候选择 Scale-up，什么时候选择 Scale-out ？**

这里我发现大家也容易有一个思维的误区，即当系统面对高并发出现瓶颈需要扩容的时候，大家首先想到的就是Scale-out。但其实，根据KISS原则，我们首先应该考虑反而是Scale-up。

KISS原则是英语 Keep It Simple Stupid 的首字母缩略字，是一种归纳过的经验原则。KISS 原则是指在设计当中应当注重简约的原则，也即大多数系统的设计应保持简洁和单纯，而不掺入非必要的复杂性，这样的系统运作成效会取得最优。

Scale-out 虽然能够突破单机的限制，但也会引入一些复杂问题。比如，如果某个节点出现故障如何保证整体可用性？当多个节点有状态需要同步时如何保证状态信息在不同节点的一致性？如何做到使用方无感知的增加和删除节点？等等

因此，如果我们遵循KISS原则，当系统面对高并发出现瓶颈需要扩容的时候，我们应该优先考虑使用 Scale-up 的方式，因为这种方案足够简单，所谓能用堆砌硬件解决的问题就用硬件来解决，但是当系统并发超过了单机的极限时，我们就要使用 Scale-out 的方式。

### **分而治之**

这是想跟大家聊的高并发设计的第二个通用方法论。分而治之又分为应用层和数据层：

- **应用层的分而治之**：
  1. **Scale-out**：没错，Sale-out其实也是分而治之思想的体现，其采用分布式部署的方式把流量分流开，让每个服务器都承担一部分并发和流量，同时需要服务是stateless的；
  2. **应用分层**：比如MVC、表现层|逻辑层|数据访问层等，不同的分层专注做这一层该做的事情，可以提升复用性，比如某一层具有一定的通用性，那么我们可以把它抽取独立出来。同时，应用分层后还可以方便的按分层进行横向扩展，比如如果业务逻辑里面包含有比较复杂的计算，导致 CPU 成为性能的瓶颈，那这样就可以把逻辑层单独抽取出来独立部署，然后只对逻辑层来做扩展，这样我们就能针对不同分层分而治之来应对不同的性能需求；
  3. **读写分离**：比如CQRS架构，就非常适合读多写少的场景，而我们大多数系统都是这种场景。当我们把读和写分而治之之后，就可以针对读和写分别进行技术选型和针对性优化；

- **数据层的分而治之**：常见的如分库分表、Sharding（数据分片），map-reduce等；

### **空间换时间**

我们在算法优化的时候，经常会计算空间复杂度和时间复杂度，其中有一个优化策略就是用空间换时间。同样的思路和策略，我们也可以用在高并发架构设计的场景，而典型的应用就是我们常说的缓存。

缓存就是数据交换的缓冲区，其本质是一个内存 Hash，是一种利用空间换时间的设计，其目标就是更快、更近、极大的提高数据访问速度，通常有如下几种实现手段：

- 将数据写入读取速度更快的存储（设备）；
- 将数据缓存到离应用最近的位置；
- 将数据缓存到离用户最近的位置；
缓存中的数据可能是提前计算好的结果、数据的副本等。

不过，引入缓存会增加系统的复杂度，典型的比如数据库与缓存的一致性问题。所以，在引入缓存前，我们还是得考虑上面提到的KISS原则，权衡是否值得。在权衡再三后仍然需要引入缓存的理由，总结起来无外乎以下两种：

- **为缓解CPU压力而引入缓存**：譬如把方法运行结果存储起来、把原本要实时计算的内容提前算好、对一些公用的数据进行复用，这可以节省CPU算力，顺带提升响应性能；
- **为缓解I/O压力而引入缓存**：譬如把原本对网络、磁盘等较慢介质的读写访问变为对内存等较快介质的访问，将原本对单点部件（如数据库）的读写访问变为对可扩缩部件（如缓存中间件）的访问，顺带提升响应性能；

因此，缓存虽然是典型以空间换时间来提升性能的手段，但它的出发点是缓解CPU和I/O资源在峰值流量下的压力，“顺带”而非“专门”地提升响应性能。这里不得不再一次提一下KISS原则，如果我们可以通过增强CPU、I/O本身的性能（或者扩展服务器的数量）来满足需要的话，那升级硬件往往是更好的解决方案，即使需要一些额外的投入成本，也通常要优于引入缓存后可能带来的复杂性和风险。

### **异步**

这是最后一个我们要介绍的高并发设计方法论。

Sync（同步）和 Async（异步）的概念：

- 所谓 Sync，是指操作一个接一个地执行，下一个操作必须等上一个操作完成后才能执行；
- 而 Async 是指不同操作间可以相互交替执行，如果其中的某个操作被 block 了，程序并不会等待，而是会找出可执行的操作继续执行；

以方法调用为例，同步调用代表调用方要阻塞等待被调用方法中的逻辑执行完成。这种方式下，当被调用方法响应时间较长时，会造成调用方长久的阻塞，在高并发下会造成整体系统性能下降甚至发生雪崩。而异步调用恰恰相反，调用方不需要等待方法逻辑执行完成就可以返回执行其他的逻辑，在被调用方法执行完毕后再通过回调、事件通知等方式将结果反馈给调用方。

所以，在高并发的场景下，我们可以分析一下我们的请求调用链路，找到那些从业务场景需求下不需要马上返回结果的调用并将其异步化，从而提升我们单个请求的RT（Response Time），进而提升我们服务器的并发处理能力。

### **性能优化方法论**

要实现一个高并发下的高性能系统，架构设计是一方面，架构性能调优则是硬币的另一面。之前提到好的架构是逐渐演变而来的，这里的演变就包括了对性能的逐渐调适到位。

针对性能优化，我总结了几个核心原则：

1. **目标导向**：在做性能优化的时候要明确目标，在目标明确的情况下，持续不断地寻找性能瓶颈，制定优化方案，直到达到目标为止；
2. **数据驱动**：通过数据帮助发现性能瓶颈，对性能问题和优化目标进行量化，并对性能提升程度进行度量；
3. **二八原则**：用 20% 的精力解决 80% 的性能问题，即抓住主要矛盾，优先优化主要的性能瓶颈点；

针对不同的应用类型，我们的优化方法是不同的，总的来说，有两种典型的应用类型，即CPU密集型与IO密集型

- CPU密集型：顾名思义就是需要持续依赖 CPU 资源来执行的操作，比如各种逻辑计算、解析、判断等等。在这种情况下，我们的优化方向是
  - 从代码层面考虑优化，比如选用更高效的算法或者减少运算次数；
  - 尽可能地通过并发利用多核 CPU 资源，并且避免让 CPU 做无效的切换；
  - 对于可以预先计算且时效性要求不高的场景，可以考虑将计算结果预先计算好并缓存；
  - 发现这类问题的主要方式，是通过一些 Profile 工具来找到消耗 CPU 时间最多的方法或者模块，比如 Linux 的 perf、eBPF 等；
- IO 密集型：比如磁盘 IO 或者网络 IO，这个过程操作系统会挂起任务线程，让出 CPU 资源，我们熟知的系统大部分都属于 IO 密集型，比如数据库系统、缓存系统、Web 系统等。这类系统的性能瓶颈可能出在系统内部，也可能是依赖的其他系统。所以此时我们需要
  - 适当地增加任务线程数量，来提高吞吐量，由于业务中的阻塞请求比较多，所以可以将配置的线程数提高到可用 CPU 核数的两倍以上，但也不能无限增加线程的数量，毕竟资源有限；
  - 使用更高效的IO，比如同步非阻塞IO、异步非阻塞IO等，使用少量的线程来处理大量的请求（IO多路复用）；
  - 优化方案会随着问题的不同而不同。比方说，如果是数据库访问慢，那么就要看是不是有锁表的情况、是不是有全表扫描、索引加的是否合适、是否有 JOIN 操作、需不需要加缓存，等等；如果是网络的问题，就要看网络的参数是否有优化的空间，抓包来看是否有大量的超时重传，网卡是否有大量丢包等；

好的架构不是设计出来的，而是持续演进出来的，在架构设计和演进的过程中，我们要遵循KISS原则，能用简单的方法解决问题的尽量先使用简单的方法，在确实需要对架构进行调整优化的时候，我们可以使用Scale-out、分而治之、空间换时间、异步等方法来实现我们的优化目标。

另外，在进行系统性能优化的时候，我们可以系统性地从如下这些方面，来分别考虑优化的思路：

1. **代码**：

   有一些性能问题，完全是由于代码写的不合理，通过直接修改一下代码就能解决问题的，比如for循环次数过多、作了很多无谓的条件判断、相同逻辑重复多次等。可以通过分布式链路跟踪工具拿到各个环节的性能数据，有针对性地优化。或者是在多线程并发情况下没有合理使用锁导致的并发性能差；

2. **数据库**：
    - 2.1 SQL调优：最常见的方式是，由自带的慢查询日志或者开源的慢查询系统定位到具体的出问题的SQL，然后使用explain、profile等工具来逐步调优，最后经过测试达到效果后上线；
    - 2.2 架构调优：读写分离、多从库负载均衡、垂直和水平分库分表；
    - 2.3 数据库连接池：应用为了实现数据库连接的高效获取、对数据库连接的限流等目的，通常会采用连接池类的方案，即每一个应用节点都管理了一个到各个数据库的连接池，**随着业务访问量或者数据量的增长，原有的连接池参数可能不能很好地满足需求**，这个时候就需要结合当前使用连接池的原理、具体的连接池监控数据和当前的业务量作一个综合的判断，通过反复的几次调试得到最终的调优参数；

3. **缓存**：

   如果数据读多写少且更新不频繁，数据库扛不住更高的并发时候，可以考虑在数据库前面加一层缓存来扛更高的并发；

4. **异步**：

   针对某些客户端的请求，在服务端可能需要针对这些请求做一些额外附加的事情，这些事情其实用户并不关心或者用户不需要立即拿到这些事情的处理结果，这种情况就比较适合用异步的方式处理这些事情；
    - 4.1 开辟新线程或使用线程池；
    - 4.2 引入 Blockingqueue： 如果异步线程处理的任务涉及的数据量非常巨大，那么可以引入阻塞队列BlockingQueue作进一步的优化。具体做法是让一批异步线程不断地往阻塞队列里扔数据，然后额外起一个处理线程，循环批量从队列里拿预设大小的一批数据，来进行批处理（比如发一个批量的远程服务请求），这样进一步提高了性能
    - 4.3 引入 MQ

5. **JVM 调优**：
    - 5.1 如果发现高峰期CPU使用率与Load值偏大，这个时候可以观察一些JVM的thread count以及gc count（可能主要是young gc count），如果这两个值都比以往偏大（也可以和一个历史经验值作对比），基本上可以定位是young gc频率过高导致，这个时候可以通过适当增大young区大小或者占比的方式来解决；
    - 5.2 如果发现关键接口响应时间很慢，可以结合gc time以及gc log中的stop the world的时间，看一下整个应用的stop the world的时间是不是比较多。如果是，可能需要减少总的gc time，具体可以从减小gc的次数和减小单次gc的时间这两个维度来考虑，一般来说，这两个因素是一对互斥因素，我们需要根据实际的监控数据来调整相应的参数（比如新生代与老生代比值、eden与survivor比值、MTT值、触发cms回收的old区比率阈值等）来达到一个最优值；
    - 5.3 如果发生full gc或者old cms gc非常频繁，通常这种情况会诱发STW的时间相应加长，从而也会导致接口响应时间变慢。这种情况，大概率是出现了“内存泄露”，Java里的内存泄露指的是一些应该释放的对象没有被释放掉（还有引用拉着它）。那么这些对象是如何产生的呢？为啥不会释放呢？对应的代码是不是出问题了？问题的关键是搞明白这个，找到相应的代码，然后对症下药。所以问题的关键是转化成寻找这些对象。怎么找？综合使用jmap和MAT，基本就能定位到具体的代码；

6. **度量系统**：

   严格来说，度量系统不属于性能优化的范畴，但是这方面和性能优化息息相关，可以说为性能优化提供一个强有力的数据参考和支撑。没有度量系统，基本上就没有办法定位到系统的问题，也没有办法有效衡量优化后的效果。很多人不重视这方面，但我认为它是系统稳定性和性能保障的基石。度量系统基本包括：确定度量指标、数据收集、数据传输、数据存储、数据分析和数据展现这几个环节
