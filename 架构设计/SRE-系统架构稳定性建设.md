### **SRE出现的背景**

很多不同类型、不同规模的企业 IT 团队，为了提升用户价值的交付效率，都在积极采用微服务、容器，以及其他的分布式技术和产品，而且也在积极引入像 DevOps 这样的先进理念

这些公司选择了正确的架构演进方向和交付理念，效率自然是提升了一大截，这时候你会发现，效率提升了，但挑战紧跟着也来了：在引入了这么多先进的技术和理念之后，这种复杂架构的系统稳定性很难得到保障，怎么办？这个问题其实不难回答，答案就是SRE

### **什么是SRE**

Google SRE 就是目前稳定性领域的最佳实践。也可以说，SRE 已经成为稳定性的代名词。其实，SRE 要做的事情并不神秘，我们每天做的监控告警、运维自动化、故障处理和复盘等工作，就是 SRE 的一部分

很多人想当然地认为，SRE 就是一个岗位，是一个角色，而且是无所不能的角色

其实，**SRE 是一套体系化的方法**，我们也只有用全局视角才能更透彻地理解它。从职能分工上，SRE 体系的建设绝不是单个岗位或单个部门就能独立完成的，必然要求有高效的跨团队组织协作才可以。不要想着设定一个 SRE 岗位，就能把稳定性的事情全部解决掉，这明显不现实。你应该从体系的角度出发，设置不同的职能岗位，同时还要有让不同角色有效协作的机制。SRE 是一个体系化工程，它需要协同多个部门、多项技术。

![SRE保障性规划图](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/resources/SRE%E4%BF%9D%E9%9A%9C%E6%80%A7%E8%A7%84%E5%88%92%E5%9B%BE.png)

### **SRE的目的**

就是提升稳定性。但是怎样才算提升了稳定性呢？

从业界稳定性通用的衡量标准看，有两个非常关键的指标：
- MTBF，Mean Time Between Failure，平均故障时间间隔；
- MTTR，Mean Time To Repair， 故障平均修复时间；

 从上面的SRE 稳定性保障规划图，你会发现我们把整个软件运行周期按照这两个指标分成了两段。通俗地说，**MTBF 指示了系统正常运行的阶段，而 MTTR 则意味着系统故障状态的阶段**。

如果想提升稳定性，就会有两个方向：
- **提升 MTBF**：也就是减少故障发生次数，提升故障发生间隔时长；
- **降低 MTTR**：故障不可避免，那就提升故障处理效率，减少故障影响时长；

从 SRE 稳定性保障规划图中，可以看出 MTTR 可以细分为 4 个指标：MTTI、MTTK、MTTF 和 MTTV

![MTTR四个细分指标](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/resources/MTTR%E5%9B%9B%E4%B8%AA%E7%BB%86%E5%88%86%E6%8C%87%E6%A0%87.png)

我们做的任何一件事情、开发的任何一套系统、引入的任何一个理念和方法论，有且只有一个目标，那就是“提升 MTBF，降低 MTTR”，也就是把故障发生时间的间隔变长，将故障影响的时间减少：
- 比如，在 Pre-MTBF 阶段（无故障阶段），我们要做好架构设计，提供限流、降级、熔断这些 Design-for-Failure 的服务治理手段，以具备故障快速隔离的条件；还可以考虑引入混沌工程这样的故障模拟机制，在线上模拟故障，提前发现问题；
- 在 Post-MTBF 阶段，也就是上一故障刚结束，开启新的 MTBF 阶段，我们应该要做故障复盘，总结经验，找到不足，落地改进措施等；
- 在 MTTI 阶段，我们就需要依赖监控系统帮我们及时发现问题，对于复杂度较高和体量非常大的系统，要依赖 AIOps 的能力，提升告警准确率，做出精准的响应；
- 同时 AIOps 能力在大规模分布式系统中，在 MTTK 阶段也非常关键，因为我们在这个阶段需要确认根因，至少是根因的范围；

### **系统可用性**

我们先来讨论一下系统可用性这个概念，因为系统可用性和我们建设 SRE 的目标强相关

目前业界有两种衡量系统可用性的方式，一个是时间维度，一个是请求维度
- **时间维度**：Availability = Uptime / (Uptime + Downtime)
- **请求维度**：Availability = Successful request / Total request

1. **时间维度**：

   时长维度，是从故障角度出发对系统稳定性进行评估，在真实的使用场景中，怎么样才算是可用时长，什么情况下又是不可用时长，这个是怎么定义的呢？

   这里就涉及到一个测量方法和判定方法的问题，包含三个要素：
   - 一个是衡量指标；
   - 第二个是衡量目标，达到什么目标是正常，达不到就是异常；
   - 但是单次测量不能说明问题，我们可以多次测量，所以第三个是影响时长，比如持续超过 12 小时

   对应到系统上，我们也会用一系列的标准和判定逻辑来说明系统是否正常。比如，系统请求状态码为非 5xx 的比例，也就是请求成功率低于 95%，已经连续超过 10 分钟，这时就要算作故障，那么 10 分钟就要纳入 Downtime（宕机时间），如果达不到这个标准，就不算作故障，只是算作一般或偶然的异常问题，这里同样有三个要素：
   - 衡量指标：系统请求状态码；
   - 衡量目标：非 5xx 占比，也就是成功率低于 95%；
   - 影响时长：持续 10 分钟

   因此，**只有当问题达到一定影响程度才会算作故障，这时才会计算不可用时长**，也就是上面公式中的 Downtime。同时，我们还要求一个周期内，允许的 Downtime，或者说是系统的“生病时间”是有限的，用这个有限时间来约束系统稳定性

   下面是我们常见的按时长维度统计的可用性对照表

   ![availability-downtime](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/resources/availability-downtime.png)

   用时间维度统计可用性，最显著的问题就是，稳定性只与故障发生挂钩

   这样做会带来哪些问题？**比如有一个系统，因为网络抖动，有短暂的几秒、十几秒，或者几分钟异常，但是后来系统自己恢复了，业务并没有中断，这时我们按照时长维度来判断，这肯定不会算作系统故障**。但是如果这种短暂的影响频度非常高，一天来个 5、6 次，持续一两周，我们应该可以判定系统运行状况也是不正常的，可能不是故障，但肯定是不稳定了

   所以这种用时长维度来衡量系统稳定性的方式，其**主要缺点就是粒度不够精细**。这些小的异常问题和它们的影响，如果从更长的周期来看，也是有一定参考价值的

   这就需要第二种衡量方式了，也就是从请求维度来衡量系统可用性

2. **请求维度**：

   请求维度，是从成功请求占比的角度出发，对系统的稳定性进行评估

   假定我们的系统一天内有 100,000 次请求，我们期望的成功率至少是 95%，如果有 5001 次请求失败了，也就是成功率低于 95% 了，我们就认为系统运行状态是不正常的。
   
   请求维度的系统可用性同样包含三个关键要素：
   - 第一个衡量指标：请求成功率；
   - 第二个衡量目标：成功率达到 95% 才算系统运行正常；
   - 第三个是统计周期：比如一天、一周、一个月等等，我们是在一个统计周期内计算整体状况，而不是看单次的

### **设定系统稳定性目标要考虑的 3 个因素**

1. **成本因素**

   从理论上来说，肯定是 9 越多稳定性越好，但是相应付出的成本和代价也会更高。比如**为了更高的可用性，要有更多的冗余资源投入，甚至要做主备、双活甚至是多活**。如果一家公司的业务量和影响力都发展到一定程度，那这个成本不管多高都是必须要付出的。但是，肯定不是所有的公司都需要付出这么高的成本，而是要先考虑 ROI（回报率）。这时候就要看企业自身对成本压力的承担情况了

2. **业务容忍度**

   稳定性怎么设定，很大程度上还要取决于业务上的容忍度。
   
   对于核心业务或核心应用，比如电商的交易和支付系统，我们当然是希望成功率越高越好，一般对系统稳定性要求是“3个9”或“4个9”。因为这些系统一旦出问题，就会直接影响整个网站和公司的收益，这些都是钱，所以对稳定性要求必然就会提高。
   
   但是，对于非核心业务或应用，比如商品评论，商品评分等，或许“2个9”也能容忍。因为短时间的评论看不到，并不会对业务收入和用户体验造成太大的影响

3. **系统当前的稳定性状况**

   结合系统的实际情况，定一个合理的标准比定一个更高的标准会更重要

   这个合理的值应该怎么来定呢？建议是从系统现状入手，比如，如果系统可用性是低于 99% 的，那首先第一步是不是可以做到 99%，然后再争取做到 99.5%，再到 99.9%，一步一步朝着更高的标准迈进。同时，这样做也会更容易落地，因为你如果定一个太高的目标，又始终达不成，反而会打击到团队的自信心和积极性

### **总结一下**

关于系统可用性，业界有两种计算方式，一种是时长维度，另一种是请求维度，这两种方式各有优劣。在 SRE 的实践中，会更多采用请求维度的统计方式，因为 SRE 关注的稳定性是系统的整体运行状态，而不仅仅只关注故障状态下的稳定性，在系统运行过程中的任何异常，都会被纳入稳定性的评估范畴中

### **SRE的切入点：选择SLI，设定SLO**

- SLI，Service Level Indicator，服务等级指标，其实就是我们选择哪些指标来衡量我们的稳定性;
- SLO，Service Level Objective，服务等级目标，指的就是我们设定的稳定性目标，比如“几个9”这样的目标;

落地 SRE 的第一步其实就是“选择合适的 SLI，设定对应的 SLO”,SLI 就是我们要监控的指标，SLO 就是这个指标对应的目标

对于我们的系统，我们应该选择哪些指标来监控系统的稳定性？指标选好后，对应地怎么定它的目标呢？

系统中常见的监控指标有如下这些：

![system-monitor-indicator](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/resources/system-monitor-indicator.png)

以衡量系统的稳定性为例，我们可以通过问自己两个问题来选择指标：
- 第一个问题：我要衡量谁的稳定性？ 也就是先找到稳定性的主体；
- 主体确定后，我们继续问第二问题：这个指标能够标识这个实例是否稳定吗？

一般来说，这两个问题解决了，SLI 指标也就确认了

给指标分层非常关键，如上图那样分层后，再看稳定性主体是属于哪一层的，就可以在这一层里选择适合的指标。但是，你要注意，即便都是应用层的，针对具体的主体，这一层的指标也不是每一个都适合

选择 SLI 指标的两大原则：
1. 原则一：选择能够标识一个主体是否稳定的指标，如果不是这个主体本身的指标，或者不能标识主体稳定性的，就要排除在外；
2. 原则二：针对电商这类有用户界面的业务系统，优先选择与用户体验强相关或用户可以明显感知的指标；

### **选取SLI指标的方法：VALET**

VALET 是 5 个单词的首字母，分别是 Volume、Availability、Latency、Error 和 Ticket。这 5 个单词就是**我们选择 SLI 指标的 5 个维度**

- **Volume - 容量**

  **Volume（容量）是指服务承诺的最大容量是多少**。比如，一个应用集群的 QPS、TPS、会话数以及连接数等等，如果我们对日常设定一个目标，就是日常的容量 SLO，对双 11 这样的大促设定一个目标，就是大促 SLO。对于数据平台，我们要看它的吞吐能力，比如每小时能处理的记录数或任务数

- **Availablity - 可用性**

  **Availablity（可用性）代表服务是否正常**。比如，我们前面介绍到的请求调用的非 5xx 状态码成功率，就可以归于可用性。对于数据平台，我们就看任务的执行成功情况，这个也可以根据不同的任务执行状态码来归类

- **Latency - 时延**

  Latency（时延）是说响应是否足够快。这是一个会直接影响用户访问体验的指标。对于任务类的作业，我们会看每个任务是否在规定时间内完成了

  通常对于时延这个指标，我们不会直接做所有请求时延的平均，因为整个时延的分布也符合正态分布，所以通常会以类似“90% 请求的时延 <= 80ms，或者 95% 请求的时延 <=120ms ”这样的方式来设定时延 SLO，熟悉数理统计的同学应该知道，这个 90% 或 95% 我们称之为置信区间

  因为不排除很多请求从业务逻辑层面是不成功的，这时业务逻辑的处理时长就会非常短（可能 10ms），或者出现 404 这样的状态码（可能就 1ms）。从可用性来讲，这些请求也算成功，但是这样的请求会拉低整个均值
  
  同时，也会出现另一种极端情况，就是某几次请求因为各种原因，导致时延高了，到了 500ms，但是因为次数所占比例较低，数据被平均掉了，单纯从平均值来看是没有异常的。但是从实际情况看，有少部分用户的体验其实已经非常糟糕了。所以，为了识别出这种情况，我们就要设定不同的置信区间来找出这样的用户占比，有针对性地解决。

- **Errors - 错误率**

  错误率有多少？这里除了 5xx 之外，我们还可以把 4xx 列进来，因为前面我们的服务可用性不错，但是从业务和体验角度，4xx 太多，用户也是不能接受的

  或者可以增加一些自定义的状态码，**看哪些状态是对业务有损的**，比如某些热门商品总是缺货，用户登录验证码总是输入错误，这些**虽不是系统错误，但从业务角度来看，对用户的体验影响还是比较大的**

- **Tickets - 人工介入**

  是否需要人工介入？**如果一项工作或任务需要人工介入，那说明一定是低效或有问题的**。举一个我们常见的场景，数据任务跑失败了，但是无法自动恢复，这时就要人工介入恢复；或者超时了，也需要人工介入，来中断任务、重启拉起来跑等等

  Tickets 的 SLO 可以想象成它的中文含义：门票。一个周期内，门票数量是固定的，比如每月 20 张，每次人工介入，就消耗一张，如果消耗完了，还需要人工介入，那就是不达标了

### **如何通过 SLO 计算可用性？**

推荐的计算方式：

- SLO1：99.95% 状态码成功率
- SLO2：90% Latency <= 80ms
- SLO3：99% Latency <= 200ms

直接用公式表示：Availability = SLO1 & SLO2 & SLO3

只有当这个三个 SLO 同时达标时，整个系统的稳定性才算达标，有一个不达标就不算达标，这样就可以很好地将 SLO 设定的合理性与最终可用性结合了起来。所以，通常在 SRE 实践中，我们通常会采用这种设定方式

### **Error Budget**

SLO 目标定好了，很具体，但实施起来不直观，所以我们可以反过来看，制定出一个允许犯错的次数标准，我们直接监控这些错误，错误预算的警示效果比看成功率这种统计数据更直观，感官冲击力更强

错误预算的计算方式一点都不复杂，简单讲就是通过 SLO 反向推导出来的，如下图所示：

![error-budget](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/resources/error-budget.png)

错误预算的计算很简单，起到的警示效果又更强烈，所以在 SLO 落地实践时，我们通常就把 SLO 转化为错误预算，以此来推进稳定性目标达成

实际场景下应用Error Budget的几种方式
1.  **稳定性燃尽图**

    当我们制定好错误预算后，就代表了要严格遵守它。如果在一个周期内，比如 4 个自然周，错误预算被消耗完了，尽管整个过程中没有出现达到故障标准的问题，这个周期的稳定性要求其实也是不达标的

    所以我们需要把错误预算尽可能直观地表现出来，随时可以看到它的消耗情况。当你和团队成员能够时刻看到还有多少犯错的机会时，对生产系统的敬畏心理也会大大增强。而且当错误预算消耗到一定比例，如 80% 或 90% 时，就要开始预警，控制各种变更，或者投入精力去解决影响稳定性的问题

2. **故障定级**

   第二种是把错误预算应用在故障定级中。我们判定一个问题是不是故障，或者评估问题影响程度到底有多大，除了看影响时长外，还有一个更具操作性的方法，那就是按照该问题消耗的错误预算比例来评判

   ![error-budget-for-failure-level](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/resources/error-budget-for-failure-level.png)

   可以看到，通过错误预算来定义故障等级就可以做到量化，而一旦可以被量化，就意味着可以标准化，有了标准，我们就可以进而推进达成共识

3. **稳定性共识机制**

    第三种是用错误预算来确定稳定性共识机制，也即在我们系统稳定性保障过程中，我们也会根据剩余预算的情况，来制定相应的行动措施，来避免我们的稳定性目标，也就是 SLO 达不成，常见的措施有：
    - 剩余预算充足或未消耗完之前，对问题的发生要有容忍度；
    - 剩余预算消耗过快或即将消耗完之前，SRE 有权中止和拒绝任何线上变更；

4. **基于错误预算的告警**

    第四种是把错误预算应用在告警中

    日常工作中，作为一线的工程师，你肯定要接收大量的告警短信，但是这些告警里面很大一部分都是没有实际意义的。为什么这么说呢？因为它们没有行动指导意义，比如 CPU 使用率 80%、成功率低于 95%、时延超过 80ms 等等，这样的告警只是告诉我们有问题、有异常，但是否需要高优先级马上处理，还是说可以先放一放、过一会再处理呢？你可能并没有办法判断

    这样的告警，接收的次数多了，就会变成“狼来了”，你自己变得警惕性不高，当故障真的发生时，你也没法快速响应

    基于错误预算来做告警，也就是说我们只关注对稳定性造成影响的告警，比如我们前面提到的，**当单次问题消耗的错误预算达到 20% 或 30% 等某一阈值时，就意味着问题非常严重了**，这种告警信息一旦收到，就要马上做出响应。这样告警数量不多，既达到了收敛效果，又非常精准

### **一个落地SLO的案例**
#### **案例背景**

一般来说，电商系统一定有一个或几个核心服务，比如要给用户提供商品选择、搜索和购买的服务等。但我们知道，大部分用户并不是上来就购买，而是会有一个访问的过程，他们会先登录，再搜索，然后访问一个或多个商品详情介绍，决定是放到购物车候选，还是选择物流地址后直接下单，最后支付购买，这条从登录到购买的链路，我们一般称之为系统的核心链路（Critical Path）

至于电商系统的其它页面或能力，比如网站政策、新手指导、开店指南等等，这些对用户购买服务不会造成太大影响的，相对于核心链路来说，它的重要性就相对低一些

我们要给电商系统设定 SLO，大的原则就是先设定核心链路的 SLO，然后根据核心链路进行 SLO 的分解

#### **找到核心链路、确定核心应用与强弱依赖关系**

我们可以先通过全链路跟踪这样的技术手段找出所有相关应用，也就是呈现出调用关系的拓扑图，这样的关系拓扑图一般会很复杂，我们需要在此基础上进行精简，也就是区分哪些是核心应用，哪些是非核心应用。这是要根据业务场景和特点来决定的，基本上需要对每个应用逐个进行分析和讨论。这个过程可能要投入大量的人工来才能完成

比如刚才讲的电商场景里，用户访问商品详情的时候，会同时展现商品评价信息，但是这些信息不展现，对于用户选择商品也不会造成非常大影响，特别是在双十一大促这样的场景中，用户在这个时刻的目的很明确，就是购买商品，并不是看评价。所以类似商品评价的应用是可以降级的，或者短时间不提供服务。那这种不影响核心业务的应用就可以归为非核心应用。

相反的，像商品 SKU 或 优惠券这样的应用，直接决定了用户最终的购买金额，这种应用在任何时刻都要保持高可用。那这种必须是高可用的应用就是核心应用，这样梳理完后，我们大概可以得到一个类似下图的简化拓扑关系

![simple-topo](https://github.com/xiaoyuge/Tech-Notes/blob/main/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/resources/simple-topo.png)

这张图就呈现出一条典型的电商交易的关键路径，其中绿色部分为核心应用，蓝色部分为非核心应用

这个时候，我们又要进行关键的一步了，就是确认强弱依赖

核心应用之间的依赖关系，我们称之为强依赖，而其它应用之间的依赖关系，我们称之为弱依赖，这里就包含两种关系，一种是核心应用与非核心应用之间的依赖，另一种是非核心应用之间的依赖

针对核心和非核心应用，以及强弱依赖关系，我们在设定 SLO 时的要求也是不同的，具体来说，可以采取下面 4 个原则
1. 核心应用的 SLO 要更严格，非核心应用可以放宽。 这么做，就是为了确保 SRE 的精力能够更多地关注在核心业务上;
2. **强依赖之间的核心应用SLO要一致**: 比如下单的 Buy 应用要依赖 Coupon 这个促销应用，我们要求下单成功率的 SLO 要 99.95%，如果 Coupon 只有 99.9%，那很显然，下单成功率是达不成目标的，所以我们就会要求 Coupon 的成功率 SLO 也要达到 99.95% ;
3. 弱依赖中，核心应用对非核心的依赖，要有降级、熔断和限流等服务治理手段。 这样做是为了避免由非核心应用的异常而导致核心应用 SLO 不达标;
4. Error Budget 策略，核心应用的错误预算要共享，就是如果某个核心应用错误预算消耗完，SLO 没有达成，那整条链路，原则上是要全部暂停操作的;

#### **如何验证核心链路的 SLO**

梳理出系统的核心链路并设定好 SLO 后，我们需要一些手段来进行验证。这里介绍两种手段，一种是容量压测，另一种就是 Chaos Engineering，也就是混沌工程

1. **容量压测**

容量压测的主要作用，就是看 SLO 中的 Volume，也就是容量目标是否可以达成。对于一般的业务系统，我们都会用 QPS 和 TPS 来表示系统容量，得到了容量这个指标，你就可以在平时观察应用或系统运行的容量水位情况。比如，我们设定容量的 SLO 是 5000 QPS，如果日常达到 4500，也就是 SLO 的 90%，我们认为**这个水位状态下，就要启动扩容，提前应对更大的访问流量**

容量压测的另一个作用，就是看在极端的容量场景下，**验证我们前面说到的限流降级策略是否可以生效**

以 Detail（商品详情页）和 Comment（商品评论）这两个应用之间的弱依赖关系为例。从弱依赖的原则上讲，如果 Comment 出现被调用次数过多超时或失败，是不能影响 Detail 这个核心应用的，这时，我们就要看这两个应用之间对应的降级策略是否生效，如果生效业务流程是不会阻塞的，如果没有生效，那这条链路的成功率就会马上降下来

还有一种场景，如果某个非核心应用调用 Detail 的次数突然激增，对于 Detail 来说，它**自身的限流保护机制要发挥作用，确保自己不会被外部流量随意打垮**

2. **Chaos Engineering - 混沌工程**

混沌工程可以帮助我们做到**在线上模拟真实的故障**，做线上应急演练，提前发现隐患

容量压测是模拟线上真实的用户访问行为的，但是压测过程中，如果我们模拟极端场景，可能也会造成异常发生，但这时的异常是被动发生的。而**混沌工程是模拟故障发生场景，主动产生线上异常和故障**

比如对于机房故障，有些大厂会直接模拟断电这样的场景，看机房是否可以切换到双活或备用机房；在网络层面，我们会模拟丢包或网卡流量打满；硬件和系统层面，可能故意把一块磁盘写满，或者把 CPU 跑满，甚至直接把一个服务器重启；应用层面，更多地会做一些故障注入，比如增加某个接口时延，直接返回错误异常，线程池跑满，甚至一个应用集群直接下线一半或更多机器等

混沌工程是一个非常复杂的系统化工程，因为要在线上制造故障，或多或少都要对线上业务造成影响，如果模拟故障造成的真实影响超过了预估影响，也要能够快速隔离，并快速恢复正常业务。即使是在稳定性体系已经非常完善的情况下，对于混沌工程的实施也要极为谨慎小心。对于一个模拟策略上线实施，一定是在一个隔离的环境中经过了大量反复验证，包括异常情况下的恢复预案实施，确保影响可控之后，在经过多方团队评审或验证，才最终在线上实施

所以，混沌工程是 SRE 稳定性体系建设的高级阶段，一定是 SRE 体系在服务治理、容量压测、链路跟踪、监控告警、运维自动化等相对基础和必需的部分非常完善的情况下才会考虑











