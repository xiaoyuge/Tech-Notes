## **Java IO**

先来了解一下TCP的send和recv
1) TCP socket的buffer
每个TCP socket在内核中都有一个发送缓冲区和一个接收缓冲区，TCP的全双工的工作模式以及TCP的流量(拥塞)控制便是依赖于这两个独立的buffer以及buffer的填充状态。
接收缓冲区把数据缓存入内核，应用进程一直没有调用recv()进行读取的话，此数据会一直缓存在相应socket的接收缓冲区内。再啰嗦一点，不管进程是否调用recv()读取socket，对端发来的数据都会经由内核接收并且缓存到socket的内核接收缓冲区之中。recv()所做的工作，就是把内核缓冲区中的数据拷贝到应用层用户的buffer里面，并返回，仅此而已。
进程调用send()发送的数据的时候，最简单情况（也是一般情况），将数据拷贝进入socket的内核发送缓冲区之中，然后send便会在上层返回。换句话说，send（）返回之时，数据不一定会发送到对端去（和write写文件有点类似），send()仅仅是把应用层buffer的数据拷贝进socket的内核发送buffer中，发送是TCP的事情，和send其实没有太大关系。
接收缓冲区被TCP用来缓存网络上来的数据，一直保存到应用进程读走为止。对于TCP，如果应用进程一直没有读取，接收缓冲区满了之后，发生的动作是：收端通知发端，接收窗口关闭（win=0）。这个便是滑动窗口的实现。保证TCP套接口接收缓冲区不会溢出，从而保证了TCP是可靠传输。因为对方不允许发出超过所通告窗口大小的数据。 这就是TCP的流量控制，如果对方无视窗口大小而发出了超过窗口大小的数据，则接收方TCP将丢弃它；
2)接收窗口（滑动窗口）
TCP连接建立之时的收端的初始接受窗口大小是14600，接收窗口是TCP中的滑动窗口，TCP的收端用这个接受窗口win=14600通知发端，我目前的接收能力是14600字节。后续发送过程中，收端会不断的用ACK通知发端自己的接收窗口的大小状态。

通常情况下，操作系统的一次写操作分为两步：
1)将数据从用户空间拷贝到系统空间；
2)从系统空间往网卡写；
同理，读操作也分为两步：
1)将数据从网卡拷贝到系统空间；
2)将数据从系统空间拷贝到用户空间；

同步IOvs异步IO & 阻塞IOvs非阻塞IO
一次IO操作分为IO等待（就绪）和IO读写，阻塞&非阻塞对应IO等待（就绪），同步&异步对应IO读写:

阻塞：A发IO请求给B，等待B回复，期间不干其他事情；
非阻塞：A发io请求给B，不等待B回复直接返回，后续通过轮询或回调获取B的返回结果；

PS：个人感觉上述描述有问题，我的理解如下
阻塞：A发IO请求给B，B一直等待A的数据是否发送完成（即数据是否就绪），期间不干其他事情；
非阻塞：A发io请求给B，B不等待A的数据是否发送完成（即数据是否就绪）而直接返回，后续通过轮询或回调获取A发送的数据；

同步：io读写由用户进程完成；
异步：io读写由系统进程而非用户进程完成，系统进程完成之后用户进程再获取数据完成后续业务操作；

所有的系统IO都分为两个阶段：等待IO就绪和实际IO操作。举例来说，读函数，分为等待系统可读和真正的读；同理，写函数分为等待网卡可以写和真正的写。等待IO就绪的阻塞是不使用CPU的，是在“空等”；而实际的IO操作的阻塞是使用CPU的，真正在"干活"，而且这个过程非常快，属于memory copy，带宽通常在1GB/s级别以上，可以理解为基本不耗时。

以socket.read()为例子：
传统的BIO里面socket.read()，如果TCP RecvBuffer里没有数据，函数会一直阻塞，直到收到数据，返回读到的数据；
对于NIO，如果TCP RecvBuffer有数据，就把数据从网卡读到内存，并且返回给用户；反之则直接返回0，永远不会阻塞；
最新的AIO(Async I/O)里面会更进一步：不但等待就绪是非阻塞的，就连数据从网卡到内存的过程也是异步的；

Java对BIO、NIO、AIO的支持：
Java BIO ： 同步并阻塞IO（Blocking IO），服务器实现模式为一个连接一个线程，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，当然可以通过线程池机制改善。
使用线程池，可以让线程的创建和回收成本相对较低。在活动连接数不是特别高（小于单机1000）的情况下，这种模型是比较不错的，可以让每一个连接专注于自己的I/O并且编程模型简单，也不用过多考虑系统的过载、限流等问题。线程池本身就是一个天然的漏斗，可以缓冲一些系统处理不了的连接或请求。不过，这个模型最本质的问题在于，严重依赖于线程。但线程是很"贵"的资源，主要表现在：
1.  线程的创建和销毁成本很高，在Linux这样的操作系统中，线程本质上就是一个进程。创建和销毁都是重量级的系统函数。
2.  线程本身占用较大内存，像Java的线程栈，一般至少分配512K～1M的空间，如果系统中的线程数过千，恐怕整个JVM的内存都会被吃掉一半。
3.  线程的切换成本是很高的。操作系统发生线程切换的时候，需要保留线程的上下文，然后执行系统调用。如果线程数过高，可能执行线程切换的时间甚至会大于线程执行的时间，这时候带来的表现往往是系统load偏高、CPU sy使用率特别高（超过20%以上)，导致系统几乎陷入不可用的状态。
4.  容易造成锯齿状的系统负载。因为系统负载是用活动线程数或CPU核心数，一旦线程数量高但外部网络环境不是很稳定，就很容易造成大量请求的结果同时返回，激活大量阻塞线程从而使系统负载压力过大。
所以，当面对十万甚至百万级连接的时候，传统的BIO模型是无能为力的。

Java NIO ： 
同步非阻塞，服务器实现模式为一个IO请求（IO就绪）一个线程，即客户端发送的连接请求都会注册到IO多路复用器上（selector），多路复用器轮询到连接有I/O请求时才启动一个线程进行处理。 在NIO的处理方式中，当一个请求来的话，开启线程进行处理，可能会等待后端应用的资源(比如JDBC连接等)，这时这个线程就被阻塞了，当并发上来的话，还是会有BIO一样的问题
NIO的主要事件有几个：读就绪、写就绪、有新连接到来
首先需要注册当这几个事件到来的时候所对应的处理器。然后在合适的时机告诉事件选择器：我对这个事件感兴趣。对于写操作，就是写不出去的时候对写事件感兴趣；对于读操作，就是完成连接和系统没有办法承载新读入的数据的时；对于accept，一般是服务器刚启动的时候；而对于connect，一般是connect失败需要重连或者直接异步调用connect的时候
其次，用一个死循环选择就绪的事件，会执行系统调用（Linux 2.6之前是select、poll，2.6之后是epoll，Windows是IOCP），还会阻塞的等待新事件的到来。新事件到来的时候，会在selector上注册标记位，标示可读、可写或者有连接到来。
注意，select是阻塞的，无论是通过操作系统的通知（epoll）还是不停的轮询(select，poll)，这个函数是阻塞的。

Java AIO(NIO.2) ： 
异步非阻塞，服务器实现模式为一个有效请求一个线程，客户端的I/O请求都是由OS先完成了再通知服务器应用去启动线程进行处理

BIO、NIO、AIO适用场景分析:
BIO方式适用于连接数目比较小且固定的架构，这种方式对服务器资源要求比较高，并发局限于应用中，JDK1.4以前的唯一选择，但程序直观简单易理解。
NIO方式适用于连接数目多且连接比较短（轻操作）的架构，比如聊天服务器，并发局限于应用中，编程比较复杂，JDK1.4开始支持。
AIO方式使用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分调用OS参与并发操作，编程比较复杂，JDK7开始支持

Proactor与Reactor模式
一般情况下，I/O 复用机制需要事件分发器（event dispatcher），事件分发器的作用，即将那些读写事件源分发给各读写事件的处理者，开发人员在开始的时候需要在分发器那里注册感兴趣的事件，并提供相应的处理者（event handler)或者是回调函数；事件分发器在适当的时候，会将请求的事件分发给这些handler或者回调函数。

涉及到事件分发器的两种模式称为：Reactor和Proactor
Reactor模式是基于同步I/O的，在Reactor模式中，事件分发器等待某个事件或者可应用或可操作的状态发生（比如文件描述符可读写，或者是socket可读写），事件分发器就把这个事件传给事先注册的事件处理函数或者回调函数，由后者来做实际的读写操作；
Proactor模式是和异步I/O相关的，在Proactor模式中，事件处理器（或者代由事件分发器发起）直接发起一个异步读写操作（相当于请求），这时候实际的读写工作是由操作系统来完成的。发起时，需要提供的参数包括用于存放读到数据的缓存区、读的数据大小或用于存放外发数据的缓存区，以及这个读写操作（请求）完后的回调函数等信息。事件分发器得知了这个请求，它默默等待这个请求的完成，然后转发完成事件给相应的事件处理者或者回调；

在Reactor中实现读
•   注册读就绪事件和相应的事件处理器；
•   事件分发器等待事件；
•   事件到来，激活分发器，分发器调用事件对应的处理器；
•   事件处理器完成实际的读操作，处理读到的数据，注册新的事件，然后返还控制权；
在Proactor中实现读：
•   事件处理器发起异步读操作（注意：操作系统必须支持异步IO）。在这种情况下，事件处理器无视IO就绪事件，它关注的是IO完成事件；
•   事件分发器等待IO完成事件；
•   在分发器等待过程中，操作系统利用并行的内核线程执行实际的读操作，并将结果数据存入用户自定义缓冲区，最后通知事件分发器读操作完成；
•   事件分发器呼唤事件处理器；
•   事件处理器处理用户自定义缓冲区中的数据，然后启动一个新的异步操作，并将控制权返回事件分发器；

两个模式的相同点，都是对某个I/O事件的事件通知（即告诉某个模块，这个I/O操作可以进行或已经完成)。在结构上，两者也有相同点：事件分发器负责提交IO操作（异步)、查询设备是否可操作（同步)，然后当条件满足时，就回调handler；不同点在于，异步情况下（Proactor)，当回调handler时，表示I/O操作已经完成；同步情况下（Reactor)，回调handler时，表示I/O设备可以进行某个操作（can read 或 can write)。

总结一下NIO是怎么解决掉线程的瓶颈并处理海量连接的：
NIO由原来的阻塞读写（占用线程）变成了单线程轮询事件，找到可以进行读写的网络描述符进行读写。除了事件的轮询是阻塞的（没有可干的事情必须要阻塞），剩余的I/O操作都是纯CPU操作，没有必要开启多线程。并且由于线程的节约，连接数大的时候因为线程切换带来的问题也随之解决，进而为处理海量连接提供了可能。单线程处理I/O的效率确实非常高，没有线程切换，只是拼命的读、写、选择事件。但现在的服务器，一般都是多核处理器，如果能够利用多核心进行I/O，无疑对效率会有更大的提高。

仔细分析一下我们需要的线程，其实主要包括以下几种：
1）事件分发器：单线程选择就绪的事件；
2）I/O事件处理器：包括connect、read、write等，这种纯CPU操作，一般开启CPU核心个线程就可以；
3）业务线程：在处理完I/O后，业务一般还会有自己的业务逻辑，有的还会有其他的阻塞I/O，如DB操作，RPC等，只要有阻塞，就需要单独的线程；

Reactor 线程模型以及 tomcat BIO、NIO 的实现

1. 单线程 Reactor 线程模型
最开始 NIO 是基于单线程实现的，所有的 I/O 操作都是在一个 NIO 线程上完成。由于 NIO 是非阻塞 I/O，理论上一个线程可以完成所有的 I/O 操作。但 NIO 其实还不算真正地实现了非阻塞 I/O 操作，因为读写 I/O 操作时用户进程还是处于阻塞状态，这种方式在高负载、高并发的场景下会存在性能瓶颈，一个 NIO 线程如果同时处理上万连接的 I/O 操作，系统是无法支撑这种量级的请求的。


2. 多线程 Reactor 线程模型
为了解决这种单线程的 NIO 在高负载、高并发场景下的性能瓶颈，后来使用了线程池。在 Tomcat 和 Netty 中都使用了一个 Acceptor 线程来监听连接请求事件，当连接成功之后，会将建立的连接注册到多路复用器中，一旦监听到事件，将交给 Worker 线程池来负责处理。大多数情况下，这种线程模型可以满足性能要求，但如果连接的客户端再上一个量级，一个 Acceptor 线程可能会存在性能瓶颈。


3. 主从 Reactor 线程模型
现在主流通信框架中的 NIO 通信框架都是基于主从 Reactor 线程模型来实现的。在这个模型中，Acceptor 不再是一个单独的 NIO 线程，而是一个线程池。Acceptor 接收到客户端的 TCP 连接请求，建立连接之后，后续的 I/O 操作将交给 Worker I/O 线程。



基于线程模型的 Tomcat 参数调优
Tomcat 中，BIO、NIO 是基于主从 Reactor 线程模型实现的。
在 BIO 中，Tomcat 中的 Acceptor 只负责监听新的连接，一旦连接建立监听到 I/O 操作，将会交给 Worker 线程中，Worker 线程专门负责 I/O 读写操作。

在 NIO 中，Tomcat 新增了一个 Poller 线程池，Acceptor 监听到连接后，不是直接使用 Worker 中的线程处理请求，而是先将请求发送给了 Poller 缓冲队列。在 Poller 中，维护了一个 Selector 对象，当 Poller 从队列中取出连接后，注册到该 Selector 中；然后通过遍历 Selector，找出其中就绪的 I/O 操作，并使用 Worker 中的线程处理相应的请求。


你可以通过以下几个参数来设置 Acceptor 线程池和 Worker 线程池的配置项：
acceptorThreadCount：该参数代表 Acceptor 的线程数量，在请求客户端的数据量非常巨大的情况下，可以适当地调大该线程数量来提高处理请求连接的能力，默认值为 1。
maxThreads：专门处理 I/O 操作的 Worker 线程数量，默认是 200，可以根据实际的环境来调整该参数，但不一定越大越好。
acceptCount：Tomcat 的 Acceptor 线程是负责从 accept 队列中取出该 connection，然后交给工作线程去执行相关操作，这里的 acceptCount 指的是 accept 队列的大小。当 Http 关闭 keep alive，在并发量比较大时，可以适当地调大这个值。而在 Http 开启 keep alive 时，因为 Worker 线程数量有限，Worker 线程就可能因长时间被占用，而连接在 accept 队列中等待超时。如果 accept 队列过大，就容易浪费连接。
maxConnections：表示有多少个 socket 连接到 Tomcat 上。在 BIO 模式中，一个线程只能处理一个连接，一般maxConnections 与 maxThreads 的值大小相同；在 NIO 模式中，一个线程同时处理多个连接，maxConnections 应该设置得比 maxThreads 要大的多，默认是 10000。


Linux 提供了 I/O 复用函数 select/poll/epoll，进程将一个或多个读操作通过系统调用函数，阻塞在函数操作上。这样，系统内核就可以帮我们侦测多个读操作是否处于就绪状态

select() 函数：它的用途是，在超时时间内，监听用户感兴趣的文件描述符上的可读可写和异常事件的发生。Linux 操作系统的内核将所有外部设备都看做一个文件来操作，对一个文件的读写操作会调用内核提供的系统命令，返回一个文件描述符（fd）.select() 函数监视的文件描述符分 3 类，分别是 writefds（写文件描述符）、readfds（读文件描述符）以及 exceptfds（异常事件文件描述符）。调用后 select() 函数会阻塞，直到有描述符就绪或者超时，函数返回。当 select 函数返回后，可以通过函数 FD_ISSET 遍历 fdset，来找到就绪的描述符。fd_set 可以理解为一个集合，这个集合中存放的是文件描述符

poll() 函数：在每次调用 select() 函数之前，系统需要把一个 fd 从用户态拷贝到内核态，这样就给系统带来了一定的性能开销。再有单个进程监视的 fd 数量默认是 1024，我们可以通过修改宏定义甚至重新编译内核的方式打破这一限制。但由于 fd_set 是基于数组实现的，在新增和删除 fd 时，数量过大会导致效率降低。poll() 的机制与 select() 类似，二者在本质上差别不大。poll() 管理多个描述符也是通过轮询，根据描述符的状态进行处理，但 poll() 没有最大文件描述符数量的限制

poll() 和 select() 存在一个相同的缺点，那就是包含大量文件描述符的数组被整体复制到用户态和内核的地址空间之间，而无论这些文件描述符是否就绪，他们的开销都会随着文件描述符数量的增加而线性增大

epoll() 函数：select/poll 是顺序扫描 fd 是否就绪，而且支持的 fd 数量不宜过大，因此它的使用受到了一些制约
Linux 在 2.6 内核版本中提供了一个 epoll 调用，epoll 使用事件驱动的方式代替轮询扫描 fd。epoll 事先通过 epoll_ctl() 来注册一个文件描述符，将文件描述符存放到内核的一个事件表中，这个事件表是基于红黑树实现的，所以在大量 I/O 请求的场景下，插入和删除的性能比 select/poll 的数组 fd_set 要好，因此 epoll 的性能更胜一筹，而且不会受到 fd 数量的限制。一旦某个文件描述符就绪时，内核会采用类似 callback 的回调机制，迅速激活这个文件描述符，当进程调用 epoll_wait() 时便得到通知，之后进程将完成相关 I/O 操作

I/O 和线程模型
I/O 模型的本质就是为了缓解 CPU 和外设之间的速度差。当线程发起 I/O 请求时，比如读写网络数据，网卡数据还没准备好，这个线程就会被阻塞，让出 CPU，也就是说发生了线程切换。而线程切换是无用功，并且线程被阻塞后，它持有内存资源并没有释放，阻塞的线程越多，消耗的内存就越大，因此 I/O 模型的目标就是尽量减少线程阻塞。Tomcat 和 Jetty 都已经抛弃了传统的同步阻塞 I/O，采用了非阻塞 I/O 或者异步 I/O，目的是业务线程不需要阻塞在 I/O 等待上

除了 I/O 模型，线程模型也是影响性能和并发的关键点。
Tomcat 和 Jetty 的总体处理原则是：
1）连接请求由专门的 Acceptor 线程组处理；
2）I/O 事件侦测也由专门的 Selector 线程组来处理；
3）具体的协议解析和业务处理可能交给Worker线程池（Tomcat），或者交给 Selector 线程来处理（Jetty）。
将这些事情分开的好处是解耦，并且可以根据实际情况合理设置各部分的线程数。这里请你注意，线程数并不是越多越好，因为 CPU 核的个数有限，线程太多也处理不过来，会导致大量的线程上下文切换

单机最大长连接数？
linux系统中与文件描述符相关的参数有以下几个：
1.soft/hard nofile
2.file-max（/proc/sys/fs/file-max）
3.nr_open（/proc/sys/fs/nr_open）
这三个参数的作用都是限制一个进程可以打开的最大文件数
文件的打开主要分两步，即申请fd和创建文件结构两个过程，nofile和nr_open在第一个过程起作用，file-max在第二个过程起作用。nofile直接限制fd的申请，nr_open限制文件描述符表的扩展，间接限制了fd的申请，file-max限制文件的实际创建过程
nofile，nr_open，file-max这三个参数的区别如下:


linux系统单机支持的tcp连接数主要受三个方面的限制：
1.文件描述符的限制
因为每个tcp连接都对应一个socket对象，而每个socket对象本身就占用一个文件描述符，文件描述符的限制单机可以达到20+亿，如果不考虑其他限制，单机支持的tcp长连接数就是20+亿，这个值是非常可观的，它绝对可以满足世界上任何一个系统对长连接的需求，只要一台机器就可以哦

2.tcp本身的限制
谈到tcp本身的限制，就涉及到tcp四元组（远端IP，远端端口号，本地IP，本地端口号），它标识一个tcp连接。根据常识理解，IP地址限定了一台主机（准确的说是网卡），端口号则限定了这个IP上的tcp连接。对于两个tcp连接，四个参数中必然是有一个不同的，因此四元组的数目决定了tcp连接的个数。对于服务端程序来，一般来说，本地ip和本地端口号固定，因此它上面可接受的的连接数=2^32*65536=2^48(不考虑少量的特殊ip和特殊端口号)，这也是个海量数字，基本可以支持世界上任何系统。对于客户端程序来说，一般本地ip、远端ip、远端口号都是固定的，因此可以支持的长连接数最多只有65536个，所以作为客户端的tcp代理比较容易出现端口号耗尽问题

linux系统对ip没有限制，对端口号有限制，相关参数为ip_local_port_range:
[root@localhost xuwei]# cat /proc/sys/net/ipv4/ip_local_port_range
1024    65535
这两个值分别代表最小值和最大值，小于1024的端口号一般是预留给系统使用的，这不是强制的，你一定要把最小值改成小于1024也是可以的。
这个端口号范围参数ip_local_port_range对于服务端程序没太大意义，服务端监听端口号一般也就几个，对于客户端来说，比如一些tcp代理程序，或压测客户端，这些程序通常会建立很多连接，这个参数就显得很重要

3.系统内存限制
关于系统内存限制，主要是两方面，一是tcp元数据的大小，包含sock、inode、file等结构；二是tcp缓存占用空间，这又包含系统缓存和用户缓存，系统缓存是系统调用read/write使用的缓存，用户缓存是码农在写代码时设计的缓冲区，在异步服务端程序里面用于把读写和数据解析处理分离

我做过测试，写两个程序，服务端只接收连接，客户端只发起连接，不读写数据，客户端和服务端分别部署在两个虚拟机上，当建立50w个连接时，服务端消耗2g内存，大概每个socket占用4kb，这个4kb是内核申请的空间，并不增加用户进程的内存。至于这个4kb是由哪个部分占用的，我还么找到答案，sock、inode、file这些元数据结构加起来也就一两百字节，由于没有收发数据因此跟tcp读写缓存关系不大，而且系统默认的读写缓冲区大小均为80k+。

由于虚拟机内存有限，不能进行更多连接的测试。不过从理论上分析，在内存足够的情况下，单机应该能达到20+亿的连接，也就是文件描述符的上限。由此可见单机最大连接数主要受内存限制，至于每个连接占用多少内存，每个系统，每个应用可能都不太一样，有的应用的数据流量比较大，占用的用户缓存和系统缓存也相应的会比较大，还要以实际压测的结果为准。